As we saw in lecture, (conditional) likelihood-based inference for the AR(1) proceeds identically to iid multiple regression. Whether it's MLE or Bayes, nothing changes. In particular, imagine you place a conjugate normal-inverse-gamma prior on the model parameters:

$$
\begin{aligned}
\sigma^2
&\sim
\text{IG}(a_0,\, b_0)
\\
\boldsymbol{\beta}\,|\, \sigma^2
&\sim 
\text{N}_2(\mathbf{m}_0,\,\sigma^2\mathbf{H}^{-1}_0)
\\
y_t
\,|\, 
y_{t-1}
,\,
\boldsymbol{\beta},\,\sigma^2
&\sim \text{N}
\left(
\mathbf{x}_t^{\scriptscriptstyle\mathsf{T}}\boldsymbol{\beta},\,\sigma^2
\right), && \mathbf{x}_t=\begin{bmatrix}1 & y_{t-1}\end{bmatrix}^{\scriptscriptstyle\mathsf{T}}.
\end{aligned}
$$

Then you get a normal-inverse-gamma posterior:

$$
\begin{aligned}
\sigma^2\,|\, y_{0:T}
&\sim
\text{IG}(a_T,\, b_T)
\\
\boldsymbol{\beta}\,|\, \sigma^2,\, y_{0:T}
&\sim 
\text{N}_2(\mathbf{m}_T,\,\sigma^2\mathbf{H}^{-1}_T)
\\
\\
\mathbf{H}_T
&=
\mathbf{X}_T^{\scriptscriptstyle\mathsf{T}}\mathbf{X}_T+\mathbf{H}_0
\\
\mathbf{m}_T
&=
\mathbf{H}_T^{-1}(\mathbf{X}_T^{\scriptscriptstyle\mathsf{T}}\mathbf{y}_T+\mathbf{H}_0\mathbf{m}_0)
\\
a_T 
&= 
a_0 + T/2
\\
b_T
&=
b_0
+
(\mathbf{y}_T^{\scriptscriptstyle\mathsf{T}}\mathbf{y}_T+\mathbf{m}_0^{\scriptscriptstyle\mathsf{T}}\mathbf{H}_0\mathbf{m}_0-\mathbf{m}_T^{\scriptscriptstyle\mathsf{T}}\mathbf{H}_T\mathbf{m}_T)/2.
\end{aligned}
$$

Those formulas for updating the hyperparameters are great for *batch* inference, but inefficient if the data are streaming. So let's improve them. Imagine we have collected data $y_{0:t-1}$ and characterized the posterior up to that point:

$$
\begin{aligned}
\sigma^2\,|\, y_{0:t-1}
&\sim
\text{IG}(a_{t-1},\, b_{t-1})
\\
\boldsymbol{\beta}\,|\, \sigma^2,\, y_{0:t-1}
&\sim 
\text{N}_2(\mathbf{m}_{t-1},\,\sigma^2\mathbf{H}^{-1}_{t-1}).
\end{aligned}
$$

Then a single new observation $y_t$ arrives and you want to characterize the new posterior:

$$
\begin{aligned}
\sigma^2\,|\, y_{0:t}
&\sim
\text{IG}(a_{t},\, b_{t})
\\
\boldsymbol{\beta}\,|\, \sigma^2,\, y_{0:t}
&\sim 
\text{N}_2(\mathbf{m}_{t},\,\sigma^2\mathbf{H}^{-1}_{t}).
\end{aligned}
$$

a. Derive a recursion that takes the old hyperparameters and the new data and computes the new hyperparameters *without inverting any matrices*;
a. `c(sunspots)` is a monthly time series of mean relative sunspot numbers from 1749 to 1983. Write a `for` loop in `R` that uses your recursion to fit an AR(1) to these data one observation at a time. Verify that at the end of the loop, you get the same estimates that you would have gotten if you had applied the batch update to the entire data set all at once.