An important consideration in time series analysis is *sequential* or *online* inference. In this environment, our data are not available to us all at once in a batch. Instead, the observations are *streaming*; they are arriving one-after-another in real-time, and we want to come up with a scheme that allows us to recursively update our inferences as new information arrives. Let's explore this in the context of least squares regression. 

::: callout-note
## OLS review

Consider the usual setting where we observe iid pairs $y_i\in\mathbb{R}$ and $\mathbf{x}_i\in\mathbb{R}^p$ from the linear model:

$$
y_i= \mathbf{x}_i^{\scriptscriptstyle\mathsf{T}}\boldsymbol{\beta}+\varepsilon_i,\quad \varepsilon_i\overset{\text{iid}}{\sim}\text{N}(0,\,\sigma^2).
$$
Given $n$ observations, we can form the matrices

$$
\mathbf{y}_n
=
\underbrace{
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{bmatrix}
}
_{n\times 1}
\quad 
\mathbf{X}_n
=
\underbrace{
\begin{bmatrix}
\mathbf{x}_1^{\scriptscriptstyle\mathsf{T}} \\
\mathbf{x}_2^{\scriptscriptstyle\mathsf{T}} \\
\vdots \\
\mathbf{x}_n^{\scriptscriptstyle\mathsf{T}}
\end{bmatrix}
}_{n\times p}
\quad
\boldsymbol{\varepsilon}_n
=
\underbrace{
\begin{bmatrix}
\varepsilon_1 \\
\varepsilon_2 \\
\vdots \\
\varepsilon_n
\end{bmatrix}
}
_{n\times 1}
,
$$
and we can rewrite the model in matrix form:

$$
\mathbf{y}_n=\mathbf{X}_n\boldsymbol{\beta}+\boldsymbol{\varepsilon}_n,\quad \boldsymbol{\varepsilon}_n\sim\text{N}_n(\mathbf{0},\,\sigma^2\mathbf{I}_n).
$$
The ordinary least squares (OLS) estimator of the regression coefficients is $\hat{\boldsymbol{\beta}}_n=(\mathbf{X}_n^{\scriptscriptstyle\mathsf{T}}\mathbf{X}_n)^{-1}\mathbf{X}_n^{\scriptscriptstyle\mathsf{T}}\mathbf{y}_n.$
:::

Hopefully that was all review, and it describes how to do **batch** or **offline** inference for the linear model. We are assuming that all of our data are available in a single batch, and we only need to compute the estimate once. But in our class, we will often care about **streaming** or **online** inference: efficiently computing the *sequence* of estimates $\hat{\boldsymbol{\beta}}_1$, $\hat{\boldsymbol{\beta}}_2$, $\hat{\boldsymbol{\beta}}_3$, ... as the pairs $(\mathbf{x}_1,\,y_1)$, $(\mathbf{x}_2,\,y_2)$, $(\mathbf{x}_3,\,y_3)$, ... arrive one-after-another in real-time.

a. So, imagine we have observed $n-1$ pairs, and we have computed the estimate $\hat{\boldsymbol{\beta}}_{n-1}=(\mathbf{X}_{n-1}^{\scriptscriptstyle\mathsf{T}}\mathbf{X}_{n-1})^{-1}\mathbf{X}_{n-1}^{\scriptscriptstyle\mathsf{T}}\mathbf{y}_{n-1}.$ Then, a new observation $(\mathbf{x}_n,\,y_n)$ arrives. How can we efficiently update our estimate of $\boldsymbol{\beta}$ to incorporate this new information? In other words, how can we quickly compute $\hat{\boldsymbol{\beta}}_{n}$ given only $\hat{\boldsymbol{\beta}}_{n-1}$ and the new $(\mathbf{x}_n,\,y_n)$? Of course, we could always just add new rows to $\mathbf{X}_{n-1}$ and $\mathbf{y}_{n-1}$ and recompute the whole estimate from scratch, but that is super inefficient and we can do better. To that end, show that the new estimate is related to the old estimate and the new data by the following recursion:  $\hat{\boldsymbol{\beta}}_n=\hat{\boldsymbol{\beta}}_{n-1}+\mathbf{k}_n(y_n-\mathbf{x}_n^{\scriptscriptstyle\mathsf{T}}\hat{\boldsymbol{\beta}}_{n-1})$. What is $\mathbf{k}_n$?

b. Interpret the recursion you derived in part a. Isn't $(y_n-\mathbf{x}_n^{\scriptscriptstyle\mathsf{T}}\hat{\boldsymbol{\beta}}_{n-1})$ a residual? What is this $\mathbf{k}_n$ thing doing? What would it mean if we had $\hat{\boldsymbol{\beta}}_n=\hat{\boldsymbol{\beta}}_{n-1}$?

c. Write a `for` loop in `R` that uses your recursion to process a dataset one observation at a time, and verify that at the end of the loop, you get the same estimates that you would have gotten if you had just used `lm`. Do this for the `mtcars` dataset and the regression `mpg ~ wt`.

::: {.callout-tip collapse="true"}
## Hint

The **Shermanâ€“Morrison formula** is a great tool! For invertible $\mathbf{A}\in\mathbb{R}^{p\times p}$ and $\mathbf{v}\in\mathbb{R}^p$, we have 

$$
(\mathbf{A}+\mathbf{v}\mathbf{v}^{\scriptscriptstyle\mathsf{T}})^{-1}
=
\mathbf{A}^{-1}
-
\frac{
\mathbf{A}^{-1}
\mathbf{v}\mathbf{v}^{\scriptscriptstyle\mathsf{T}}
\mathbf{A}^{-1}
}{
1
+
\mathbf{v}^{\scriptscriptstyle\mathsf{T}}
\mathbf{A}^{-1}
\mathbf{v}
}
.
$$

:::
