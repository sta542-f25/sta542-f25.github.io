Slicing-and-dicing the **multivariate normal distribution** is a very important skill in time series analysis, so let's make sure we can do that.

a. Let $\mathbf{x}\sim\text{N}_n(\boldsymbol{\mu},\,\boldsymbol{\Sigma})$, and fix constants $\mathbf{A}\in\mathbb{R}^{m\times n}$ and $\mathbf{c}\in\mathbb{R}^m$. Show that 

$$
\mathbf{y}
=
\mathbf{A}
\mathbf{x}
+
\mathbf{c}
\sim
\text{N}_m
\left(
\mathbf{A}
\boldsymbol{\mu}
+
\mathbf{c}
,\,
\mathbf{A}
\boldsymbol{\Sigma}
\mathbf{A}^{\scriptscriptstyle\mathsf{T}}
\right)
.
$$

b. Consider the joint distribution $p(\mathbf{x},\,\mathbf{y})=p(\mathbf{y}\,|\,\mathbf{x})p(\mathbf{x})$ written hierarchically as $\mathbf{x}\sim\text{N}_n(\boldsymbol{\mu},\,\boldsymbol{\Sigma})$ and $\mathbf{y}\,|\,\mathbf{x}\sim\text{N}_m(\mathbf{A}\mathbf{x}+\mathbf{c},\,\mathbf{R})$. Use the result in part a to show that the joint distribution implied by this hierarchy is

$$
\begin{bmatrix}
\mathbf{x}
\\
\mathbf{y}
\end{bmatrix}
\sim
\text{N}_{n+m}
\left(
\begin{bmatrix}
\boldsymbol{\mu}
\\
\mathbf{A}\boldsymbol{\mu}+\mathbf{c}
\end{bmatrix}
,\,
\begin{bmatrix}
\boldsymbol{\Sigma}
&
\boldsymbol{\Sigma}\mathbf{A}^{\scriptscriptstyle\mathsf{T}}
\\
\mathbf{A}
\boldsymbol{\Sigma}
&
\mathbf{A}
\boldsymbol{\Sigma}
\mathbf{A}^{\scriptscriptstyle\mathsf{T}} + \mathbf{R}
\end{bmatrix}
\right)
.
$$

c. Use the result in part a to show that the linear combination of independent normals is normal. That is, if $x_i\sim\text{N}(\mu_i,\,\sigma^2_i)$ are independent and $a_i\in\mathbb{R}$ are constant, then prove that

$$
\sum\limits_{i=1}^na_ix_i\sim\text{N}\left(\sum\limits_{i=1}^na_i\mu_i,\, \sum\limits_{i=1}^na_i^2\sigma_i^2\right).
$$




::: {.callout-tip collapse="true"}
## Hint

JZ changed this hint on 8/30/2025. The original hint mentioned using the change-of-variables formula. That's a fine solution if $\mathbf{A}$ is invertible, but the result holds even if $\mathbf{A}$ is not invertible, and we will often use that version (e.g. part c here). If you used change-of-variables, don't redo it. We'll award full credit if you did it right. But otherwise, consider using the *moment-generating function* (*mgf*) of a random vector $\mathbf{x}$:

$$
M(\mathbf{t})=E\left(e^{\mathbf{t}^{\scriptscriptstyle\mathsf{T}}\mathbf{x}}\right).
$$

As in the univariate case, when it exists, the mgf uniquely characterizes the entire distribution of a random vector, just like the density, cdf, or characteristic function do.
:::
