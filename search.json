[
  {
    "objectID": "syllabus/syllabus.html",
    "href": "syllabus/syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Class meeting: MoWe 11:45 AM - 1:00 PM in Old Chem 201;\nWhat the course catalog says: This is an introductory course in time series analysis with a focus on applications. Two basic approaches, including time domain and frequency domain methods, will be covered. A modern time-frequency analysis approach to study nonstationary time series analysis will also be introduced. The main goal is to guide the students to appreciate the main issues involved in time series analysis and solve practical challenges, particularly those coming from the high-frequency and ultra-long biomedical time series. The primary audience for this course is graduate students in statistics;\nWhat JZ says: Modern data science environments often feature high volumes of dependent, streaming data. Central banks, hedge funds, e-commerce sites, energy grids, meteorological stations, disease count trackers, radar systems, and autonomous vehicles are all sequentially analyzing a stream of incoming time series data and using it to forecast the future in real-time. But these forecasts are often more than just point predictions; they take the form of interval and density predictions that guide decision-making by incorporating many sources of uncertainty. This is what our course is about.\nA time series is a set of measurements collected over time. We model these data as a sequence of dependent random variables, and a time series model is their joint probability distribution. In this course, we will study many of the main methods for specifying this distribution: autoregressive moving average (ARMA) models, vector autoregressions (VARs), dynamic linear models (DLMs), hidden Markov models (HMMs), etc. Throughout, we will focus on sequential inference (recursive estimation when the data are streaming) and probabilistic prediction (point, interval, and density forecasting). Classical and Bayesian approaches to inference will be discussed, but the latter will be emphasized.\nPrerequisite: STA 521; Corequisite: STA 532.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#course-description",
    "href": "syllabus/syllabus.html#course-description",
    "title": "Syllabus",
    "section": "",
    "text": "Class meeting: MoWe 11:45 AM - 1:00 PM in Old Chem 201;\nWhat the course catalog says: This is an introductory course in time series analysis with a focus on applications. Two basic approaches, including time domain and frequency domain methods, will be covered. A modern time-frequency analysis approach to study nonstationary time series analysis will also be introduced. The main goal is to guide the students to appreciate the main issues involved in time series analysis and solve practical challenges, particularly those coming from the high-frequency and ultra-long biomedical time series. The primary audience for this course is graduate students in statistics;\nWhat JZ says: Modern data science environments often feature high volumes of dependent, streaming data. Central banks, hedge funds, e-commerce sites, energy grids, meteorological stations, disease count trackers, radar systems, and autonomous vehicles are all sequentially analyzing a stream of incoming time series data and using it to forecast the future in real-time. But these forecasts are often more than just point predictions; they take the form of interval and density predictions that guide decision-making by incorporating many sources of uncertainty. This is what our course is about.\nA time series is a set of measurements collected over time. We model these data as a sequence of dependent random variables, and a time series model is their joint probability distribution. In this course, we will study many of the main methods for specifying this distribution: autoregressive moving average (ARMA) models, vector autoregressions (VARs), dynamic linear models (DLMs), hidden Markov models (HMMs), etc. Throughout, we will focus on sequential inference (recursive estimation when the data are streaming) and probabilistic prediction (point, interval, and density forecasting). Classical and Bayesian approaches to inference will be discussed, but the latter will be emphasized.\nPrerequisite: STA 521; Corequisite: STA 532.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#teaching-team",
    "href": "syllabus/syllabus.html#teaching-team",
    "title": "Syllabus",
    "section": "Teaching team",
    "text": "Teaching team\n\n\n\n\n\n\n\n\n\nMug\nName\nRole\nOffice Hours\n\n\n\n\n\nLi, Aihua\nTA\nWed 10:30 AM - 12:30 PM\nremote (Zoom link on Canvas)\n\n\n\nZito, John\nInstructor\nTue 1:00 PM - 3:00 PM\nor by appointment\nOld Chem 207",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#textbooks",
    "href": "syllabus/syllabus.html#textbooks",
    "title": "Syllabus",
    "section": "Textbooks",
    "text": "Textbooks\nThe course does not have a designated textbook, and you are not required to purchase anything, but here are several you should know about:\n\n[SS] Time Series Analysis and Its Applications, 5e by Robert Shumway and David Stoffer (free pdf);\n[Skä] Bayesian Filtering and Smoothing, 1e by Simo Särkkä (free pdf);\n[WH] Bayesian Forecasting and Dynamic Models, 2e by Mike West and Jeff Harrison (free pdf);\n[PPC] Dynamic Linear Models with R by Giovanni Petris, Sonia Petrone, and Patrizia Campagnoli (free pdf);\n[HA] Forecasting: Principles and Practice, 3e by Rob Hyndman and George Athanasopoulos (online edition);\n[PFW] Time Series: Modeling, Computation, and Inference, 2e by Raquel Prado, Marco Ferreira, and Mike West;\n[Ham] Time Series Analysis by James Hamilton;\n[DK] Time Series Analysis by State Space Methods, 2e by James Durbin and Siem Jan Koopman;\n[BD] Time Series: Theory and Methods by Peter Brockwell and Richard Davis.\n\nI will list readings in the PREPARE column of the course schedule.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#assignments-and-grading",
    "href": "syllabus/syllabus.html#assignments-and-grading",
    "title": "Syllabus",
    "section": "Assignments and grading",
    "text": "Assignments and grading\nThere are four course components, weighted as follows:\n\n\n\n\nCategory\nPercentage\n\n\n\n\nProblem Sets\n25%\n\n\nExam 1\n25%\n\n\nExam 2\n25%\n\n\nProject\n25%\n\n\n\n\nYour final letter grade will be determined based on the usual thresholds, which will never move upward but may move downward in your favor:\n\n\n\n\nLetter Grade\nFinal Course Grade\n\n\n\n\nA+\n&gt;= 97\n\n\nA\n93 - 96.99\n\n\nA-\n90 - 92.99\n\n\nB+\n87 - 89.99\n\n\nB\n83 - 86.99\n\n\nB-\n80 - 82.99\n\n\nC+\n77 - 79.99\n\n\nC\n73 - 76.99\n\n\nC-\n70 - 72.99\n\n\nD+\n67 - 69.99\n\n\nD\n63 - 66.99\n\n\nD-\n60 - 62.99\n\n\nF\n&lt; 60\n\n\n\n\nHere are some details about the course assignments:\n\nProblem sets: there will be six in total, with one due roughly every two weeks;\nExams: there will be two written, in-class exams. The only resource you are allowed is both sides of one 8.5” x 11” sheet of notes prepared by you. The exam dates are…\n\nWednesday October 8 11:45 AM - 12:00 PM in Old Chem 201;\nMonday November 24 11:45 AM - 12:00 PM in Old Chem 201;\n\nFinal project: to end the semester, each student will write a final report that applies or extends ideas from the course. This is due at 9AM on Wednesday December 10. Full details about the project will be announced after the first exam.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#communication",
    "href": "syllabus/syllabus.html#communication",
    "title": "Syllabus",
    "section": "Communication",
    "text": "Communication\nIf you wish to ask content-related questions in writing, please do not do so via e-mail. Instead, please use the course discussion forum Ed Discussion. That way all members of the teaching team can see your question, and all students can benefit from the ensuing discussion. You are also encouraged to answer one another’s questions.\nIf you have questions about personal matters that may not be appropriate for the public course forum (e.g. illness, accommodations, etc), then please e-mail the instructor directly (john.zito@duke.edu).\n\n\n\n\n\n\nNote\n\n\n\nYou can ask questions anonymously on Ed. The teaching team will still know your identity, but your peers will not.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#late-work-and-extensions",
    "href": "syllabus/syllabus.html#late-work-and-extensions",
    "title": "Syllabus",
    "section": "Late work and extensions",
    "text": "Late work and extensions\nNo late work will be accepted unless you request an extension in advance by e-mailing the instructor directly (john.zito@duke.edu). All reasonable requests will be entertained, but extensions will not be long.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#attendance",
    "href": "syllabus/syllabus.html#attendance",
    "title": "Syllabus",
    "section": "Attendance",
    "text": "Attendance\nLive your life. Attendance is not strictly required for any of the class meetings. The responsibility lies with us to make class meetings sufficiently engaging and informative that you choose to attend.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#collaboration",
    "href": "syllabus/syllabus.html#collaboration",
    "title": "Syllabus",
    "section": "Collaboration",
    "text": "Collaboration\nYou are enthusiastically encouraged to work together and help one another on labs and problem sets. I would much rather you consult your peers than some daft language model. What I ask is that you acknowledge your collaborators. So, at the end of each problem in your write-up, leave a comment like “Ursula, Ignatius and I worked together on this problem,” or “Ethel explained to me how to do part b.” You are not being judged based on your acknowledgements, and there is no penalty for getting “too much” help from others. If you omit acknowledgements, I will assume you did your work solo. If your classmates tell a different story, I will have questions. But otherwise, so long as you are thorough and honest, there will be no problems.\nHaving said all of that, you should not be crassly handing your solutions to others for them to brainlessly copy. This is plagiarism, and all involved will earn a zero on the assignment and be referred to the conduct office, both sharers and recipients alike. The write-up you submit must be your own work.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#use-of-outside-resources-including-ai",
    "href": "syllabus/syllabus.html#use-of-outside-resources-including-ai",
    "title": "Syllabus",
    "section": "Use of outside resources, including AI",
    "text": "Use of outside resources, including AI\nThere are at least two reasons you might seek outside resources:\n\n✅ Extra practice or alternative instruction: Go crazy. Knock yourself out. Have a ball. The internet includes many good (and horrible) resources for learning this material, so if you find something that really resonates with you, have at it;\n❌ Doing the problems for you: If you find a solution online, or ask a language model to generate one, and you copy it down and submit it as your own work, that is plagiarism. If we detect it, you will earn a zero for that part of your write-up.\n\n“Using ChatGPT to complete assignments is like bringing a forklift into the weight room; you will never improve your cognitive fitness that way.” Furthermore, 50% of your final course grade is determined by your performance on old school, no-tech exams. As such, if you outsource all of your thinking to an AI, you will probably fail both exams. To avoid this, I suggest you refrain from using language models to do the problems for you.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#regrade-requests",
    "href": "syllabus/syllabus.html#regrade-requests",
    "title": "Syllabus",
    "section": "Regrade requests",
    "text": "Regrade requests\nIf you receive a graded assignment back, and you believe that some part of it was graded incorrectly, you may dispute the grade by submitting a regrade request in Gradescope. JZ is the sole reviewer of these. Note the following:\n\nYou have one week after you receive a grade to submit a regrade request;\nYou should submit separate regrade requests for each question you wish to dispute, not a single catch-all request;\nRequests will be considered if there was an error in the grade calculation or if a correct answer was mistakenly marked as incorrect;\nRequests to dispute the number of points deducted for an incorrect response will not be considered.\n\n\n\n\n\n\n\nWarning\n\n\n\nA regrade request can result in your grade going up, staying the same, or going down if we determine that, in fact, the original grader was too lenient.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#duke-community-standard",
    "href": "syllabus/syllabus.html#duke-community-standard",
    "title": "Syllabus",
    "section": "Duke Community Standard",
    "text": "Duke Community Standard\nDuke University is a community dedicated to scholarship, leadership, and service and to the principles of honesty, fairness, respect, and accountability. Members of this community commit to reflect upon and uphold these principles in all academic and non-academic endeavors, and to protect and promote a culture of integrity.\nDuke University has high expectations for students’ scholarship and conduct. In accepting admission, students indicate their willingness to subscribe to and be governed by the rules and regulations of the university, which flow from the Duke Community Standard (DCS).\nRegardless of course delivery format, it is the responsibility of all students to understand and follow all Duke policies, including but not limited to the academic integrity policy (e.g., completing one’s own work, following proper citation of sources, adhering to guidance around group work projects, and more). Ignoring these requirements is a violation of the DCS.\nStudents can direct any questions or concerns regarding academic integrity to the Office of Student Conduct and Community Standards at conduct@duke.edu and can access the DCS guide at https://dukecommunitystandard.students.duke.edu/.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#the-simplest-non-trivial-time-series-model",
    "href": "slides/03-ar-1-forecasting.html#the-simplest-non-trivial-time-series-model",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "The simplest non-trivial time series model",
    "text": "The simplest non-trivial time series model\nThe autoregression of order 1, or AR(1):\n\n\\[\n\\begin{aligned}\ny_t\n&=\n\\beta_0\n+\n\\beta_1\ny_{t-1}\n+\n\\varepsilon_t,\n\\quad\n\\varepsilon_t\\iid\\text{N}(0\\com\\sigma^2)\n\\\\\ny_0\n&\\sim\n\\text{N}(\\mu_0\\com \\initvar),\n\\end{aligned}\n\\]\n\n\nThis implies a joint distribution governed by a finite set of static parameters \\(\\Btheta = \\begin{bmatrix}\\beta_0&\\beta_1&\\sigma^2\\end{bmatrix}^\\tr\\):\n\n\n\\[\n\\begin{aligned}\np(y_{0:T}\\given \\Btheta)\n&=\np(y_0)\n\\prod_{t=1}^T\np(y_t\\given y_{t-1}\\com\\Btheta).\n\\end{aligned}\n\\]\n\n\nViewed as a function of \\(\\Btheta\\), that’s a likelihood!"
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#the-ar1-joint-distribution",
    "href": "slides/03-ar-1-forecasting.html#the-ar1-joint-distribution",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "The AR(1) joint distribution",
    "text": "The AR(1) joint distribution\nJoint distribution:\n\\[\n\\begin{bmatrix}\ny_0 & y_1 & \\cdots & y_T\n\\end{bmatrix}^\\tr\n\\sim\\text{N}_{T+1}\\left(\\Bmu\\com \\BSigma\\right).\n\\]\n\nMoments:\n\\[\n\\begin{aligned}\nE(y_t)\n&=\n\\beta_0\\sum\\limits_{i=0}^{t-1}\\beta_1^i\n+\n\\beta_1^t\\mu_0\n\\\\\n\\var(y_t)\n&=\n\\sigma^2\n\\sum\\limits_{i=0}^{t-1}\\beta_1^{2i}+\n\\beta_1^{2t}\\initvar\n\\\\\n\\cov(y_t\\com y_s)\n&=\n\\begin{cases}\n\\beta_1^{s-t}\\var(y_t) & t\\leq s\\\\\n\\beta_1^{t-s}\\var(y_s) & s &lt; t.\n\\end{cases}\n.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#the-special-case-when-beta_11",
    "href": "slides/03-ar-1-forecasting.html#the-special-case-when-beta_11",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "The special case when \\(|\\beta_1|<1\\)\n",
    "text": "The special case when \\(|\\beta_1|&lt;1\\)\n\nThe mean and variance are time-invariant, and the covariance structure is shift-invariant:\n\n\\[\n\\begin{aligned}\nE(y_t)\n&=\n\\frac{\\beta_0}{1-\\beta_1}\n\\\\\n\\var(y_t)\n&=\n\\frac{\\sigma^2}{1-\\beta_1^2}\n\\\\\n\\gamma(h)\n&=\n\\cov(y_{t+h}\\com y_t)\n=\n\\beta_1^{h}\\frac{\\sigma^2}{1-\\beta_1^2}.\n\\end{aligned}\n\\]\n\n\nThe common marginal shared by all \\(y_t\\) is called the stationary distribution:\n\n\n\\[\ny_t\\sim\\text{N}\\left(\\frac{\\beta_0}{1-\\beta_1}\\com \\frac{\\sigma^2}{1-\\beta_1^2}\\right).\n\\]\n\n\nSo “did: dependent but identically distributed.”"
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#method-of-moments",
    "href": "slides/03-ar-1-forecasting.html#method-of-moments",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Method of moments",
    "text": "Method of moments\nThe Yule-Walker estimators are\n\\[\n\\begin{aligned}\n\\hat{\\beta}_1&=\\frac{\\hat{\\gamma}_T(1)}{\\hat{\\gamma}_T(0)}\n\\\\\n\\hat{\\beta}_0&=(1-\\hat{\\beta}_1)\\hat{\\mu}_T\\\\\n\\hat{\\sigma^2_T}&=(1-\\hat{\\beta}_1^2)\\hat{\\gamma}_T(0),\n\\end{aligned}\n\\]\nwhere \\(\\hat{\\mu}_T\\), \\(\\hat{\\gamma}_T(0)\\), and \\(\\hat{\\gamma}_T(1)\\) are simple sample averages.\n\nThese have good statistical properties, but only if the true process is stationary."
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#likelihood-based-inference",
    "href": "slides/03-ar-1-forecasting.html#likelihood-based-inference",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Likelihood-based inference",
    "text": "Likelihood-based inference\nIn practice it is easiest to work with the conditional likelihood:\n\\[\np(y_{1:T}\\given  y_0\\com \\Btheta)\n=\n\\prod_{t=1}^T\np(y_t\\given y_{t-1}\\com\\Btheta).\n\\]\n\nClassical route:\n\\[\n\\hat{\\Btheta}_T=\\argmax{\\Btheta}\\,p(y_{1:T}\\given y_0\\com \\Btheta).\n\\]\n\n\nBayesian route:\n\\[\np(\\Btheta\\given y_{0:T})\n=\n\\frac{p(y_{1:T}\\given y_0\\com \\Btheta)p(\\Btheta)}{p(y_{1:T}\\given y_0)}\n.\n\\]\n\n\nEither way, everything you know about inference for iid regression goes through unmodified."
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#maximum-likelihood",
    "href": "slides/03-ar-1-forecasting.html#maximum-likelihood",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Maximum likelihood",
    "text": "Maximum likelihood\nIt’s just ordinary least squares (OLS):\n\n\\[\n\\begin{aligned}\n\\By_T\n&=\n\\begin{bmatrix}y_1&y_2 & \\cdots & y_T\\end{bmatrix}^\\tr\n\\\\\n\\BX_T\n&=\n\\begin{bmatrix}\n1 & 1 & \\cdots & 1 \\\\\ny_0 & y_1 & \\cdots & y_{T-1}\n\\end{bmatrix}^\\tr\n\\\\\n\\Bbeta\n&=\n\\begin{bmatrix}\\beta_0&\\beta_1\\end{bmatrix}^\\tr\n\\\\\n\\\\\n\\hat{\\Bbeta}_T\n&=\n(\\BX_T^\\tr\\BX_T)^{-1}\\BX_T^\\tr\\By_T\n\\\\\n\\hat{\\sigma^2_T}\n&=\n||\\By_T-\\BX_T\\hat{\\Bbeta}_T||_2^2 / T.\n\\end{aligned}\n\\]\n\n\nThings can get weird, but we have asymptotic theory for this regardless whether or not things are stationary."
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#recursive-version",
    "href": "slides/03-ar-1-forecasting.html#recursive-version",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Recursive version",
    "text": "Recursive version\nIf the data are streaming, we have some bad ass rank-1 updates:\n\n\\[\n\\begin{aligned}\n\\hat{\\Bbeta}_t\n&=\n\\hat{\\Bbeta}_{t-1}\n+\n\\Bk_t(y_t-\\Bx_t^\\tr\\hat{\\Bbeta}_{t-1})\n\\\\\n\\Bk_t\n&=\n\\frac{\\BP_{t-1}\\Bx_t}\n{1+\\Bx_t^\\tr\\BP_{t-1}\\Bx_t}\n\\\\\n\\BP_t\n&=\n(\\BX_t^\\tr\\BX_t)^{-1}\n\\\\\n&=\n(\\BX_{t-1}^\\tr\\BX_{t-1}+\\Bx_t\\Bx_t^\\tr)^{-1}\n\\\\\n&=\n(\\BP_{t-1}+\\Bx_t\\Bx_t^\\tr)^{-1}\n\\\\\n&=\n\\BP_{t-1}\n-\n\\Bk_t\n\\Bx_t^\\tr\\BP_{t-1}\n.\n\\end{aligned}\n\\]\n\n\nThanks Sherman-Morrison!"
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#conjugate-bayes",
    "href": "slides/03-ar-1-forecasting.html#conjugate-bayes",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Conjugate Bayes",
    "text": "Conjugate Bayes\nTake a conjugate normal-inverse-gamma prior:\n\n\\[\n\\begin{aligned}\n\\sigma^2\n&\\sim\n\\text{IG}(a_0\\com b_0)\n\\\\\n\\Bbeta\\given \\sigma^2\n&\\sim\n\\text{N}_2(\\Bm_0\\com\\sigma^2\\BH^{-1}_0)\n\\\\\ny_t\n\\given\ny_{t-1}\n\\com\n\\Bbeta\\com\\sigma^2\n&\\sim \\text{N}\n\\left(\n\\Bx_t^\\tr\\Bbeta\\com\\sigma^2\n\\right), && \\Bx_t=\\begin{bmatrix}1 & y_{t-1}\\end{bmatrix}^\\tr.\n\\end{aligned}\n\\]\n\n\nThe posterior is available in closed-form:\n\\[\n\\begin{aligned}\n\\sigma^2\\given y_{0:T}\n&\\sim\n\\text{IG}(a_T\\com b_T)\n\\\\\n\\Bbeta\\given \\sigma^2\\com y_{0:T}\n&\\sim\n\\text{N}_2(\\Bm_T\\com\\sigma^2\\BH^{-1}_T)\n\\\\\n\\\\\n\\BH_T\n&=\n\\BX_T^\\tr\\BX_T+\\BH_0\n\\\\\n\\Bm_T\n&=\n\\BH_T^{-1}(\\BX_T^\\tr\\By_T+\\BH_0\\Bm_0)\n\\\\\na_T\n&=\na_0 + T/2\n\\\\\nb_T\n&=\nb_0\n+\n(\\By_T^\\tr\\By_T+\\Bm_0^\\tr\\BH_0\\Bm_0-\\Bm_T^\\tr\\BH_T\\Bm_T)/2.\n\\end{aligned}\n\\]\n\n\nMake it recursive on Problem Set 1!"
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#a-classic-example",
    "href": "slides/03-ar-1-forecasting.html#a-classic-example",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "A classic example",
    "text": "A classic example"
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#point-forecast",
    "href": "slides/03-ar-1-forecasting.html#point-forecast",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Point forecast",
    "text": "Point forecast\nYour single-number best guess at tomorrow’s observation:"
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#forecast-interval",
    "href": "slides/03-ar-1-forecasting.html#forecast-interval",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Forecast interval",
    "text": "Forecast interval\nA range of likely values for tomorrow’s observation:"
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#forecast-density",
    "href": "slides/03-ar-1-forecasting.html#forecast-density",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Forecast density",
    "text": "Forecast density\nFull distribution capturing uncertainty about tomorrow:"
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#and-then-tomorrow-finally-comes",
    "href": "slides/03-ar-1-forecasting.html#and-then-tomorrow-finally-comes",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "And then tomorrow finally comes",
    "text": "And then tomorrow finally comes\nSo…how’d we do?"
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#whats-the-point",
    "href": "slides/03-ar-1-forecasting.html#whats-the-point",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "What’s the point?",
    "text": "What’s the point?\n\nWe want intervals and densities to communicate uncertainty about the forecast;\n\nWhat sources of uncertainty?\n\nBasic data uncertainty;\nParameter estimation uncertainty;\nHyperparameter uncertainty;\nModel uncertainty;\nUncertainty introduced by missing data.\n\n\nIn the small world of the AR(1), mainly the first two for now."
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#forecast-distribution",
    "href": "slides/03-ar-1-forecasting.html#forecast-distribution",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Forecast distribution",
    "text": "Forecast distribution\nGiven the data we’ve seen, we want a distribution for the data we haven’t seen.\n\nIn other words:\n\\[\np(y_{t+1:t+H}\\given y_{0:t})\n\\]\n\n\nAssume for now the parameters are fixed and known (red flag!)."
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#clumsy-but-true",
    "href": "slides/03-ar-1-forecasting.html#clumsy-but-true",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Clumsy, but true",
    "text": "Clumsy, but true\n\nWe know\n\\[\n\\begin{bmatrix}\n\\mathbf{y}_t \\\\\n\\mathbf{y}_{t+H}\n\\end{bmatrix}\n\\sim\n\\text{N}_{1+t+H}\n\\left(\n\\begin{bmatrix}\n\\boldsymbol{\\mu}_t \\\\\n\\boldsymbol{\\mu}_{t+H}\n\\end{bmatrix}\n,\\,\n\\begin{bmatrix}\n\\boldsymbol{\\Sigma}_t & \\boldsymbol{\\Sigma}_{t,t+H}\\\\\n\\boldsymbol{\\Sigma}_{t,t+H}^{\\scriptscriptstyle\\mathsf{T}} & \\boldsymbol{\\Sigma}_{t+H}\n\\end{bmatrix}\n\\right),\n\\]\n\n\nand so strictly speaking:\n\\[\n\\mathbf{y}_{t+H} \\,|\\,\\mathbf{y}_t\n\\sim\n\\text{N}_H\n\\left(\n\\boldsymbol{\\mu}_{t+H} + \\boldsymbol{\\Sigma}_{t,t+H}^{\\scriptscriptstyle\\mathsf{T}}\\boldsymbol{\\Sigma}_t^{-1}(\\mathbf{y}_t-\\boldsymbol{\\mu}_t)\n,\\,\n\\boldsymbol{\\Sigma}_{t+H} - \\boldsymbol{\\Sigma}_{t,t+H}^{\\scriptscriptstyle\\mathsf{T}}\n\\boldsymbol{\\Sigma}_t^{-1}\n\\boldsymbol{\\Sigma}_{t,t+H}\n\\right).\n\\]\n\n\nBut let’s make this…useful.\n\n\n(Also, ignore all this ridiculous notation)"
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#lets-give-it-some-thought",
    "href": "slides/03-ar-1-forecasting.html#lets-give-it-some-thought",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Let’s give it some thought",
    "text": "Let’s give it some thought\n\nFirst, the AR(1) is first order Markov, so\n\n\n\\[\np(y_{t+1:t+H}\\given y_{0:t})=p(y_{t+1:t+H}\\given y_{t}).\n\\]\n\n\nThe future data behaves just like the past, with a fixed initial condition:\n\n\n\\[\n\\begin{aligned}\ny_{t+h}\n&=\n\\beta_0\n+\n\\beta_1\ny_{t+h-1}\n+\n\\varepsilon_{t+h},\n\\quad\n\\varepsilon_t\\iid\\text{N}(0\\com\\sigma^2)\n\\\\\ny_t\n&\\text{ fixed}.\n\\end{aligned}\n\\]\nSo the conditional distribution has the same structure as the unconditional joint that we have already studied. Just starts in a different spot."
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#forecast-distributions",
    "href": "slides/03-ar-1-forecasting.html#forecast-distributions",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Forecast distributions",
    "text": "Forecast distributions\n\\[\n\\begin{aligned}\ny_{t+h}\\given y_{0:t}\n&\\sim\\N\\left(\\mu_{t+h|t}\\com \\tau^2_{t+h|t}\\right)\n\\\\\n\\\\\n\\mu_{t+h|t}\n&=\n\\beta_0\\sum\\limits_{i=0}^{h-1}\\beta_1^i\n+\n\\beta_1^hy_t\n\\\\\n&=\n\\beta_0\n+\n\\beta_1\n\\mu_{t+h-1|t}\n&& \\mu_{t|t}=y_t\n\\\\\n\\tau^2_{t+h|t}\n&=\n\\sigma^2\n\\sum\\limits_{i=0}^{h-1}\\beta_1^{2i}\n\\\\\n&=\n\\sigma^2\n+\n\\beta_1^2\n\\tau^2_{t+h-1|t}\n&&\n\\tau^2_{t|t}=0.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#prediction-intervals",
    "href": "slides/03-ar-1-forecasting.html#prediction-intervals",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Prediction intervals",
    "text": "Prediction intervals\nFull distribution:\n\\[\ny_{t+h}\\given y_{0:t}\n\\sim\\N\\left(\\mu_{t+h|t}\\com \\tau^2_{t+h|t}\\right)\n\\]\nPrediction interval:\n\\[\n\\begin{aligned}\n\\mu_{t+h-1|t}\n&\\pm\nz_{1-\\frac{\\alpha}{2}}\n\\tau_{t+h|t}\n\\\\\n\\beta_0\\sum\\limits_{i=0}^{h-1}\\beta_1^i\n&\\pm\nz_{1-\\frac{\\alpha}{2}}\n\\sqrt{\\sigma^2\n\\sum\\limits_{i=0}^{h-1}\\beta_1^{2i}}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#summary",
    "href": "slides/03-ar-1-forecasting.html#summary",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Summary",
    "text": "Summary\nDistribution forecasts:\n\\[\n\\begin{aligned}\ny_{t+h}\\given y_{0:t}\n&\\sim\\N\\left(\\mu_{t+h|t}\\com \\tau^2_{t+h|t}\\right)\n\\\\\n\\\\\n\\mu_{t+h|t}\n&=\n\\beta_0\\sum\\limits_{i=0}^{h-1}\\beta_1^i\n+\n\\beta_1^hy_t\n\\\\\n&=\n\\beta_0\n+\n\\beta_1\n\\mu_{t+h-1|t}\n&& \\mu_{t|t}=y_t\n\\\\\n\\tau^2_{t+h|t}\n&=\n\\sigma^2\n\\sum\\limits_{i=0}^{h-1}\\beta_1^{2i}\n\\\\\n&=\n\\sigma^2\n+\n\\beta_1^2\n\\tau^2_{t+h-1|t}\n&&\n\\tau^2_{t|t}=0.\n\\end{aligned}\n\\]\nPrediction intervals:\n\\[\n\\mu_{t+h-1|t}\n\\pm\nz_{1-\\frac{\\alpha}{2}}\n\\tau_{t+h|t}\n.\n\\]"
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#pics-or-it-didnt-happen",
    "href": "slides/03-ar-1-forecasting.html#pics-or-it-didnt-happen",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Pics or it didn’t happen",
    "text": "Pics or it didn’t happen\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\nlibrary(shiny)\n\nsimulate_ar_1 &lt;- function(T, b0, b1, s, y0){\n  y &lt;- numeric(T)\n  y[1] &lt;- y0\n  for(t in 2:T){\n    y[t] &lt;- b0 + b1 * y[t - 1] + rnorm(1, 0, s)\n  }\n  return(y)\n}\n\nar_1_mean &lt;- function(h, b0, b1, yT){\n  if(h == 0){\n    return(yT)\n  } else {\n    return(b0 * sum(b1 ^ (0:(h-1))) + yT * (b1^h)) \n  }\n}\n\nar_1_var &lt;- function(h, b1, s){\n  if(h == 0){\n    return(0)\n  } else {\n    return((s^2) * sum(b1 ^ (2*(0:(h-1)))))\n  }\n}\n\nui &lt;- fluidPage(\n  titlePanel(\"Forecast distribution of a Gaussian AR(1)\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"b0\", \"β₀\", min = -5, max = 5, value = 0, step = 0.1),\n      sliderInput(\"b1\", \"β₁\", min = -2, max = 2, value = 0, step = 0.1),\n      sliderInput(\"s\", \"σ\", min = 0, max = 2, value = 1, step = 0.1)\n    ),\n    mainPanel(\n      plotOutput(\"distPlot\", height = \"600px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  \n  # fixed observed data\n  set.seed(123)\n  y_obs &lt;- simulate_ar_1(10, 0, 0, 1, 0)\n  \n  output$distPlot &lt;- renderPlot({\n    b0 &lt;- input$b0\n    b1 &lt;- input$b1\n    s &lt;- input$s\n    \n    T_obs &lt;- length(y_obs)\n    H &lt;- 20  # forecast horizon\n    range &lt;- 1:(T_obs + H)\n    \n    # plot window\n    plot(range, c(y_obs, rep(NA,H)), type=\"n\",\n         xlab = \"t\", ylab = expression(y[t]),\n         ylim = c(-20,20), bty=\"n\")\n    \n    # grey forecast region\n    rect(T_obs+0.5, -20, T_obs+H+0.5, 20, col=rgb(0.8,0.8,0.8,0.5), border=NA)\n    \n    # observed data\n    lines(1:T_obs, y_obs, col=\"black\", lwd=2)\n    \n    # forecast distribution intervals\n    alpha = c(0.01, seq(0.1,0.9,0.1))\n    middle &lt;- sapply(0:H, ar_1_mean, b0, b1, y_obs[T_obs])\n    sds &lt;- sqrt(sapply(0:H, ar_1_var, b1, s))\n    f_range &lt;- T_obs:(T_obs+H)\n    \n    for(a in alpha){\n      U = qnorm(1 - a/2, mean = middle, sd = sds)\n      L = qnorm(a/2, mean = middle, sd = sds)\n      polygon(c(f_range, rev(f_range)),\n              c(U, rev(L)),\n              col = rgb(1,0,0,0.15), border=NA)\n    }\n    \n    # add mean forecast line\n    lines(f_range, middle, col=\"red\", lwd=2, lty=2)\n  })\n}\n\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#but-wait",
    "href": "slides/03-ar-1-forecasting.html#but-wait",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "But wait!",
    "text": "But wait!\nThe parameters are not known. They have to be estimated.\n\n\\[\n\\mu_{t+h-1|t}\n\\pm\nz_{1-\\frac{\\alpha}{2}}\n\\tau_{t+h|t}\n.\n\\]\n\n\nCan we plug-in estimates and replace \\(z\\) with \\(t\\)?\n\n\nNot quite."
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#bootstrapping-for-dependent-data-with-iid-errors",
    "href": "slides/03-ar-1-forecasting.html#bootstrapping-for-dependent-data-with-iid-errors",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Bootstrapping for dependent data with iid errors",
    "text": "Bootstrapping for dependent data with iid errors\n\n\nData come from an AR(1) with mean zero iid errors (may not be normal!):\n\n\n\n\\[\ny_t\n=\n\\beta_0\n+\n\\beta_1\ny_{t-1}\n+\n\\varepsilon_t,\n\\quad\n\\varepsilon_t\n\\overset{\\text{iid}}{\\sim}F;\n\\]\n\n\n\nUse observed \\(y_{0:T}\\) to compute OLS estimate \\(\\hat{\\Bbeta}_T\\);\n\n\n\n\nEstimate residuals and center them:\n\n\\[\n\\hat{\\varepsilon}_t=y_t-\\hat{\\beta}_0-\\hat{\\beta}_1y_{t-1}\\quad \\to\\quad e_t=\\hat{\\varepsilon}_t-\\sum\\limits_{j=1}^T\\hat{\\varepsilon}_j/T.\n\\]\n\n\n\nConstruct alternative time series by resampling residuals:\n\n\\[\n\\begin{aligned}\n\\tilde{y}_0&=y_0\\\\\n\\tilde{y}_t&=\\hat{\\beta}_0+\\hat{\\beta}_1\\tilde{y}_{t-1}+\\tilde{e}_{t},&&\\tilde{e}_{t}\\overset{\\text{iid}}{\\sim} \\hat{F}_T.\n\\end{aligned}\n\\]\n\n\nRepeats the last step many times, and from then on it’s bootstrap like normal."
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#bootstrapping-the-residuals",
    "href": "slides/03-ar-1-forecasting.html#bootstrapping-the-residuals",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Bootstrapping the residuals",
    "text": "Bootstrapping the residuals\nGird your loins:\n\n\\[\n\\begin{matrix}\n\\text{1. Original data} &&& y_{0:n} && \\\\\n&&& \\downarrow && \\\\\n\\text{2. OLS} &&& \\hat{\\Btheta}_{T} && \\\\\n&&& \\downarrow && \\\\\n\\text{3. Estimate (centered) residuals} &&& e_{1:T} && \\\\\n&\\swarrow &\\swarrow& \\cdots &\\searrow&\\searrow \\\\\n\\text{4. Resample residuals}&\\tilde{e}_{1:T}^{(1)} &\\tilde{e}_{1:T}^{(2)}& \\cdots &\\tilde{e}_{1:T}^{(k-1)}&\\tilde{e}_{1:T}^{(k)} \\\\\n&\\downarrow &\\downarrow& \\cdots &\\downarrow&\\downarrow \\\\\n\\text{5. Bootstrap time series}&\\tilde{y}_{0:T}^{(1)} &\\tilde{y}_{0:T}^{(2)}& \\cdots &\\tilde{y}_{0:T}^{(k-1)}&\\tilde{y}_{0:T}^{(k)} \\\\\n&\\downarrow &\\downarrow& \\cdots &\\downarrow&\\downarrow \\\\\n\\text{6. Bootstrap estimates}&\\tilde{\\Btheta}_{T}^{(1)} &\\tilde{\\Btheta}_{T}^{(2)}& \\cdots &\\tilde{\\Btheta}_{T}^{(k-1)}&\\tilde{\\Btheta}_{T}^{(k)} \\\\\n&\\downarrow &\\downarrow& \\cdots &\\downarrow&\\downarrow \\\\\n\\text{7. Draw more residuals}&\\tilde{e}_{t+1:t+h}^{(1)} &\\tilde{e}_{t+1:t+h}^{(2)}& \\cdots &\\tilde{e}_{t+1:t+h}^{(k-1)}&\\tilde{e}_{t+1:t+h}^{(k)} \\\\\n&\\downarrow &\\downarrow& \\cdots &\\downarrow&\\downarrow \\\\\n\\text{8. Simulate forecasts}&\\tilde{y}_{t+1:t+h}^{(1)} &\\tilde{y}_{t+1:t+h}^{(2)}& \\cdots &\\tilde{y}_{t+1:t+h}^{(k-1)}&\\tilde{y}_{t+1:t+h}^{(k)} \\\\\n\\end{matrix}\n\\]\n\n\nIn other words, it’s awful."
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#probabilistic-forecasting-via-monte-carlo",
    "href": "slides/03-ar-1-forecasting.html#probabilistic-forecasting-via-monte-carlo",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Probabilistic forecasting via Monte Carlo",
    "text": "Probabilistic forecasting via Monte Carlo\nUse the simulations \\(\\tilde{y}_{t+h}^{(1:k)}=\\left\\{\\tilde{y}_{t+h}^{(1)}\\com \\tilde{y}_{t+h}^{(2)}\\com ...\\com \\tilde{y}_{t+h}^{(k)}\\right\\}\\) to construct whatever forecast object you want:\n\n\\[\n\\hat{y}_{t+h|t}=\\frac{1}{k}\\sum\\limits_{j=1}^k\\tilde{y}_{t+h}^{(j)}. \\quad (\\text{...or median})\n\\]\n\n\nForecast interval:\n\\[\n\\hat{I}_{t+h|t} = \\left[\\hat{Q}_{\\frac{\\alpha}{2}}\\left(\\tilde{y}_{t+h}^{(1:k)}\\right)\\com \\hat{Q}_{1-\\frac{\\alpha}{2}}\\left(\\tilde{y}_{t+h}^{(1:k)}\\right)\\right]. \\quad (...\\text{or hdi})\n\\]\n\n\nForecast distribution:\n\\[\n\\hat{f}_{t+h|t}\n=\n\\text{histogram}\\left(\\tilde{y}_{t+h}^{(1:k)}\\right). \\quad (...\\text{or kde})\n\\]"
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#compare-intervals",
    "href": "slides/03-ar-1-forecasting.html#compare-intervals",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Compare intervals",
    "text": "Compare intervals\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  titlePanel(\"AR(1) Forecast: Plug-in vs Residual Bootstrap\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"true_b0\", \"True β₀:\", min = -1, max = 1, value = 0.5, step = 0.1),\n      sliderInput(\"true_b1\", \"True β₁:\", min = -0.95, max = 0.95, value = 0.7, step = 0.05),\n      sliderInput(\"true_sigma\", \"True σ:\", min = 0.1, max = 2, value = 1, step = 0.1),\n      sliderInput(\"n_obs\", \"Sample size:\", min = 40, max = 500, value = 100, step = 10),\n      actionButton(\"rerun\", \"Re-run simulation\"),\n      checkboxInput(\"show_red\", \"Show plug-in fan (red)\", TRUE),\n      checkboxInput(\"show_blue\", \"Show bootstrap fan (blue)\", TRUE)\n    ),\n    \n    mainPanel(\n      plotOutput(\"fanPlot\", height = \"600px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  \n  simulate_ar_1 &lt;- function(T, b0, b1, s, y1 = 0){\n    y &lt;- numeric(T)\n    y[1] &lt;- y1\n    for(t in 2:T){\n      y[t] &lt;- b0 + b1 * y[t-1] + rnorm(1, 0, s)\n    }\n    return(y)\n  }\n  \n  ar_1_mean_h &lt;- function(h, b0, b1, yT){\n    if(h == 0) return(yT)\n    b0 * sum(b1^(0:(h-1))) + yT * (b1^h)\n  }\n  \n  ar_1_var_h &lt;- function(h, b1, sigma){\n    if(h == 0) return(0)\n    sigma^2 * sum(b1^(2*(0:(h-1))))\n  }\n  \n  sim_data &lt;- reactiveVal(NULL)\n  \n  observeEvent(input$rerun, {\n    set.seed(123)  # keep deterministic for reproducibility\n    n_obs &lt;- input$n_obs\n    H &lt;- 20\n    B &lt;- 7500\n    \n    y_obs &lt;- simulate_ar_1(n_obs, input$true_b0, input$true_b1, input$true_sigma, y1 = 0)\n    \n    # OLS fit\n    Y &lt;- y_obs[2:n_obs]\n    X &lt;- cbind(1, y_obs[1:(n_obs-1)])\n    ols_fit &lt;- lm(Y ~ X - 1)\n    coef_hat &lt;- coef(ols_fit)\n    b0_hat &lt;- coef_hat[1]\n    b1_hat &lt;- coef_hat[2]\n    resid_hat &lt;- resid(ols_fit)\n    sigma_hat &lt;- sqrt(sum(resid_hat^2) / (length(resid_hat) - 1))\n    \n    h_seq &lt;- 0:H\n    plug_mean &lt;- sapply(h_seq, ar_1_mean_h, b0 = b0_hat, b1 = b1_hat, yT = y_obs[n_obs])\n    plug_sd   &lt;- sqrt(sapply(h_seq, ar_1_var_h, b1 = b1_hat, sigma = sigma_hat))\n    \n    # bootstrap\n    bootstrap_forecasts &lt;- matrix(NA, nrow = B, ncol = H + 1)\n    resid_centered &lt;- resid_hat - mean(resid_hat)\n    \n    for(b in 1:B){\n      e_star &lt;- sample(resid_centered, size = n_obs - 1, replace = TRUE)\n      y_star &lt;- numeric(n_obs)\n      y_star[1] &lt;- y_obs[1]\n      for(t in 2:n_obs){\n        y_star[t] &lt;- b0_hat + b1_hat * y_star[t-1] + e_star[t-1]\n      }\n      Ys &lt;- y_star[2:n_obs]\n      Xs &lt;- cbind(1, y_star[1:(n_obs-1)])\n      fit_star &lt;- lm(Ys ~ Xs - 1)\n      coef_star &lt;- coef(fit_star)\n      b0_star &lt;- coef_star[1]\n      b1_star &lt;- coef_star[2]\n      resid_star &lt;- resid(fit_star)\n      resid_star_centered &lt;- resid_star - mean(resid_star)\n      \n      y_fut &lt;- numeric(H + 1)\n      y_fut[1] &lt;- y_obs[n_obs]\n      future_shocks &lt;- sample(resid_star_centered, size = H, replace = TRUE)\n      for(h in 1:H){\n        y_fut[h + 1] &lt;- b0_star + b1_star * y_fut[h] + future_shocks[h]\n      }\n      bootstrap_forecasts[b, ] &lt;- y_fut\n    }\n    \n    boot_mean &lt;- colMeans(bootstrap_forecasts)\n    \n    sim_data(list(\n      y_obs = y_obs,\n      plug_mean = plug_mean,\n      plug_sd = plug_sd,\n      bootstrap_forecasts = bootstrap_forecasts,\n      boot_mean = boot_mean,\n      H = H\n    ))\n  }, ignoreNULL = FALSE)\n  \n  output$fanPlot &lt;- renderPlot({\n    dat &lt;- sim_data()\n    if(is.null(dat)) return(NULL)\n    \n    y_obs &lt;- dat$y_obs\n    plug_mean &lt;- dat$plug_mean\n    plug_sd &lt;- dat$plug_sd\n    bootstrap_forecasts &lt;- dat$bootstrap_forecasts\n    boot_mean &lt;- dat$boot_mean\n    H &lt;- dat$H\n    \n    n_obs &lt;- length(y_obs)\n    \n    # Window: last 20 obs and 20 forecasts\n    obs_window &lt;- (n_obs-19):n_obs\n    f_range &lt;- (n_obs):(n_obs + H)\n    plot_range &lt;- c((n_obs-19):(n_obs+H))\n    \n    y_min &lt;- min(c(y_obs[obs_window], plug_mean - 4 * plug_sd, bootstrap_forecasts))\n    y_max &lt;- max(c(y_obs[obs_window], plug_mean + 4 * plug_sd, bootstrap_forecasts))\n    \n    plot(plot_range, rep(NA, length(plot_range)), type = \"n\",\n         xlab = \"t\", ylab = expression(y[t]),\n         ylim = c(y_min, y_max), bty = \"n\",\n         main = \"AR(1) forecast: plug-in (red) vs residual-bootstrap (blue)\")\n    \n    rect(n_obs + 0.5, y_min, n_obs + H + 0.5, y_max,\n         col = rgb(0.85,0.85,0.85,0.5), border = NA)\n    \n    lines(obs_window, y_obs[obs_window], col = \"black\", lwd = 2)\n    \n    # bootstrap fan\n    if(input$show_blue){\n      prob_levels &lt;- c(0.001, 0.005, 0.01, seq(0.02, 0.48, by = 0.02))\n      lower_probs &lt;- prob_levels\n      upper_probs &lt;- 1 - prob_levels\n      boot_fan_lower &lt;- apply(bootstrap_forecasts, 2, quantile, probs = lower_probs)\n      boot_fan_upper &lt;- apply(bootstrap_forecasts, 2, quantile, probs = upper_probs)\n      for(i in seq_len(nrow(boot_fan_lower))){\n        polygon(c(f_range, rev(f_range)),\n                c(boot_fan_upper[i, ], rev(boot_fan_lower[i, ])),\n                col = rgb(0,0,1,0.08), border = NA)\n      }\n      lines(f_range, apply(bootstrap_forecasts, 2, median),\n            col = rgb(0,0,1,0.8), lty = 2, lwd = 1.5)\n      lines(f_range, boot_mean, col = rgb(0,0,1,0.9), lty = 1, lwd = 1)\n    }\n    \n    # plug-in fan\n    if(input$show_red){\n      alpha_vec &lt;- c(0.01, seq(0.1,0.9,by=0.1))\n      for(a in rev(alpha_vec)){\n        U &lt;- qnorm(1 - a/2, mean = plug_mean, sd = plug_sd)\n        L &lt;- qnorm(a/2, mean = plug_mean, sd = plug_sd)\n        polygon(c(f_range, rev(f_range)),\n                c(U, rev(L)),\n                col = rgb(1, 0, 0, 0.15), border = NA)\n      }\n      lines(f_range, plug_mean, col = \"red\", lty = 2, lwd = 2)\n    }\n    \n    abline(v = n_obs + 0.5, lty = 3)\n  })\n}\n\nshinyApp(ui, server)"
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#thats-just-the-tip-of-the-iceberg",
    "href": "slides/03-ar-1-forecasting.html#thats-just-the-tip-of-the-iceberg",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "That’s just the tip of the iceberg",
    "text": "That’s just the tip of the iceberg\n\n\n\n\nIf you really want to know,\nLahiri is your man."
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#posterior-predictive-distribution",
    "href": "slides/03-ar-1-forecasting.html#posterior-predictive-distribution",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Posterior predictive distribution",
    "text": "Posterior predictive distribution\nAfter computing the posterior\n\\[\np(\\Btheta\\given y_{0:T})\n=\n\\frac{p(y_{1:T}\\given y_0\\com \\Btheta)p(\\Btheta)}{p(y_{1:T}\\given y_0)}\n,\n\\]\n\nyou base forecasts on the posterior predictive distribution:\n\\[\np(y_{T+1:T+H}\\,|\\,y_{1:T})\n=\n\\int\np(y_{T+1:T+H}\\,|\\,y_{1:T},\\,\\boldsymbol{\\theta})\np(\\boldsymbol{\\theta}\\,|\\,y_{1:T})\n\\,\\text{d}\\boldsymbol{\\theta}.\n\\]\n\n\nImmediately incorporates both data and parameter uncertainty by construction.\n\n\nThere’s…nothing else to say about this."
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#probabilistic-prediction-is-a-natural-byproduct-of-a-bayesian-approach",
    "href": "slides/03-ar-1-forecasting.html#probabilistic-prediction-is-a-natural-byproduct-of-a-bayesian-approach",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Probabilistic prediction is a natural byproduct of a Bayesian approach",
    "text": "Probabilistic prediction is a natural byproduct of a Bayesian approach\nIt falls out basically for free:\n\n\n\n\n\n\nBjørnstad, Jan (1990): “Predictive likelihood: a review,” Statistical Science\n\n\n“Prediction of the value of an unobserved or future random variable is a fundamental problem in statistics. From a Bayesian point of view, it is solved in a straightforward manner by finding the posterior predictive density of the unobserved random variable given the data. If one does not want to pay the Bayesian price of having to determine a prior, no unifying basis for prediction has existed until recently.”\n\n\n\n\nIt’s a price I’m willing to pay, dude."
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#in-general-just-simulate",
    "href": "slides/03-ar-1-forecasting.html#in-general-just-simulate",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "In general, just simulate",
    "text": "In general, just simulate\nSimulate the posterior somehow (iid, MCMC, HMC, SMC, whatever), and then for each draw, simulate the model forward:\n\\[\n\\begin{aligned}\n\\Btheta^{(j)} &\\sim p(\\boldsymbol{\\theta}\\,|\\,y_{0:T})\\\\\n\\tilde{y}^{(j)}_{T+1:T+H}\n&\\sim\np(y_{T+1:T+H}\\,|\\,y_{0:T},\\,\\Btheta^{(j)}), &&j = 1\\com 2\\com ...\\com k.\n\\end{aligned}\n\\]\nThe retained sample \\(\\tilde{y}^{(1:k)}_{T+1:T+H}\\) is drawn from the posterior predictive."
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#same-slide-as-before",
    "href": "slides/03-ar-1-forecasting.html#same-slide-as-before",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Same slide as before",
    "text": "Same slide as before\nUse the simulations \\(\\tilde{y}_{t+h}^{(1:k)}=\\left\\{\\tilde{y}_{t+h}^{(1)}\\com \\tilde{y}_{t+h}^{(2)}\\com ...\\com \\tilde{y}_{t+h}^{(k)}\\right\\}\\) to construct whatever forecast object you want:\n\n\\[\n\\hat{y}_{t+h|t}=\\frac{1}{k}\\sum\\limits_{j=1}^k\\tilde{y}_{t+h}^{(j)}. \\quad (\\text{...or median})\n\\]\n\n\nForecast interval:\n\\[\n\\hat{I}_{t+h|t} = \\left[\\hat{Q}_{\\frac{\\alpha}{2}}\\left(\\tilde{y}_{t+h}^{(1:k)}\\right)\\com \\hat{Q}_{1-\\frac{\\alpha}{2}}\\left(\\tilde{y}_{t+h}^{(1:k)}\\right)\\right]. \\quad (...\\text{or hdi})\n\\]\n\n\nForecast distribution:\n\\[\n\\hat{f}_{t+h|t}\n=\n\\text{histogram}\\left(\\tilde{y}_{t+h}^{(1:k)}\\right). \\quad (...\\text{or kde})\n\\]"
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#one-step-ahead-in-the-conjugate-case",
    "href": "slides/03-ar-1-forecasting.html#one-step-ahead-in-the-conjugate-case",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "One-step-ahead in the conjugate case",
    "text": "One-step-ahead in the conjugate case\nWe want\n\n\\[\np(y_{t+1}\\given y_{0:t})\n=\n\\int\n\\int\np(y_{t+1}\\given\\Bbeta\\com\\sigma^2\\com y_{0:t})p(\\Bbeta\\com\\sigma^2\\given y_{0:t})\\,\\dd\\Bbeta\\,\\dd\\sigma^2\n,\n\\]\n\n\nand we know that\n\\[\n\\begin{aligned}\n\\sigma^2\\given y_{0:t}\n&\\sim\n\\text{IG}(a_t\\com b_t)\n\\\\\n\\Bbeta\\given \\sigma^2\\com y_{0:t}\n&\\sim\n\\text{N}_2(\\Bm_t\\com\\sigma^2\\BH^{-1}_t)\n\\\\\ny_{t+1}\\given\\Bbeta\\com \\sigma^2\\com y_{0:t}\n&\\sim \\N(\\Bx_{t+1}^\\tr\\Bbeta\\com\\sigma^2).\n\\end{aligned}\n\\]\n\n\nThere is actually a closed-form solution."
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#marginalize-out-bbeta",
    "href": "slides/03-ar-1-forecasting.html#marginalize-out-bbeta",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Marginalize out \\(\\Bbeta\\)\n",
    "text": "Marginalize out \\(\\Bbeta\\)\n\nWe know that\n\\[\n\\begin{aligned}\n\\Bbeta\\given \\sigma^2\\com y_{0:t}\n&\\sim\n\\text{N}_2(\\Bm_t\\com\\sigma^2\\BH^{-1}_t)\n\\\\\ny_{t+1}\n&=\n\\Bx_{t+1}^\\tr\\Bbeta\n+\n\\varepsilon_{t+1}\n,\n&&\n\\varepsilon_{t+1}\\sim\\N(0\\com \\sigma^2).\n\\end{aligned}\n\\]\n\nBy affine transformation:\n\\[\n\\Bx_{t+1}^\\tr\\Bbeta\n\\given \\sigma^2\\com y_{0:t}\n\\sim\n\\N(\n\\Bx_{t+1}^\\tr\\Bm_t\n\\com\n\\sigma^2\\Bx_{t+1}^\\tr\\BH_t^{-1}\\Bx_{t+1}\n).\n\\]\n\n\nBy linear combination of independent normals:\n\\[\n\\begin{aligned}\ny_{t+1}\\given \\sigma^2\\com y_{0:t}\n&\\sim \\N\\left(\\Bx_{t+1}^\\tr\\Bm_t\\com\\sigma^2(1+\\Bx_{t+1}^\\tr\\BH_t^{-1}\\Bx_{t+1})\\right).\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#marginalize-out-sigma2",
    "href": "slides/03-ar-1-forecasting.html#marginalize-out-sigma2",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Marginalize out \\(\\sigma^2\\)\n",
    "text": "Marginalize out \\(\\sigma^2\\)\n\nWe know that\n\\[\n\\begin{aligned}\n\\sigma^2\\given y_{0:t}\n&\\sim\n\\text{IG}(a_t\\com b_t)\n\\\\\ny_{t+1}\\given \\sigma^2\\com y_{0:t}\n&\\sim \\N\\left(\\Bx_{t+1}^\\tr\\Bm_t\\com\\sigma^2(1+\\Bx_{t+1}^\\tr\\BH_t^{-1}\\Bx_{t+1})\\right).\n\\end{aligned}\n\\]\n\nMarginalizing \\(\\sigma^2\\) out of this hierarchy is essentially the definition of Student’s \\(t\\):\n\n\n\\[\n\\begin{aligned}\ny_{t+1}\\given y_{0:t}\n&\\sim\nt(\\nu_{t+1|t}\\com\\bar{y}_{t+1|t}\\com s_{t+1|t}^2)\n\\\\\n\\\\\n\\nu_{t+1|t}\n&=\n2a_t\n\\\\\n\\bar{y}_{t+1|t}\n&=\n\\Bx_{t+1}^\\tr\\Bm_t\n\\\\\ns_{t+1|t}^2\n&=\n\\frac{b_t}{a_t}\n(1+\\Bx_{t+1}^\\tr\\BH_t^{-1}\\Bx_{t+1})\n.\n\\end{aligned}\n\\]\n\n\nSo, Student’s \\(t\\) with location-scale."
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#one-step-ahead-probabilistic-prediction",
    "href": "slides/03-ar-1-forecasting.html#one-step-ahead-probabilistic-prediction",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "One-step-ahead probabilistic prediction",
    "text": "One-step-ahead probabilistic prediction\n\nOur density forecast is:\n\\[\ny_{t+1}\\given y_{0:t}\n\\sim\nt(\\nu_{t+1|t}\\com\\bar{y}_{t+1|t}\\com s_{t+1|t}^2).\n\\]\n\n\nThis gives a \\(100\\times(1-\\alpha)\\%\\) credible interval:\n\\[\n\\bar{y}_{t+1|t}\n\\pm\n\\underbrace{t_{\\nu_{t+1|t}}\n\\left(\n1-\\frac{\\alpha}{2}\n\\right)}_{\\text{quantile of }t(\\nu_{t+1|t}\\com 0\\com 1)}\ns_{t+1|t}\n.\n\\]\n\n\nAnd the moments are\n\\[\n\\begin{aligned}\nE(y_{t+1}\\given y_{0:t})\n&=\n\\bar{y}_{t+1|t}\n,\n&&\n\\nu_{t+1|t}&gt;1\n\\\\\n\\text{var}(y_{t+1}\\given y_{0:t})\n&=\n\\frac{\\nu_{t+1|t}}{\\nu_{t+1|t}-2}\ns_{t+1|t}^2\n,\n&&\n\\nu_{t+1|t}&gt;2.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#things-to-notice",
    "href": "slides/03-ar-1-forecasting.html#things-to-notice",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Things to notice",
    "text": "Things to notice\n\nGoal: generate probabilistic predictions incorporating many sources of uncertainty. In the small world of the AR(1), these are mainly uncertainty from parameter estimation and the inherent randomness of future observations.\n\n\n\n\nClassical approach: pretty unnatural. Can’t do anything analytically. Whole books have been written about how the bootstrapping ought to go;\n\n\n\n\n\nBayesian approach: totally natural. Everything is probabilistic already. Posterior predictive simulation is conceptually straightforward. We can even do the math in a special case.\n\n\n\nBut in fairness, the Bayes stuff leans hard into normality. The bootstrap stuff less so."
  },
  {
    "objectID": "slides/01-ar-1-structure.html#time-series",
    "href": "slides/01-ar-1-structure.html#time-series",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Time series",
    "text": "Time series\n\nA time series is a set of measurements collected over time;\nWe model these data as a sequence of dependent random variables:\n\n\\[\n\\By_{0:T} = \\{\\By_0,\\,\\By_1,\\,\\By_2,\\,...,\\,\\By_T\\}.\n\\]\n\nA time series model is “just” their joint probability distribution:\n\n\\[\np(\\By_{0:T}) = p(\\By_0)\\prod_{t=1}^Tp(\\By_t\\,|\\,\\By_{0:t-1}).\n\\]"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#course-themes",
    "href": "slides/01-ar-1-structure.html#course-themes",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Course themes",
    "text": "Course themes\nWe will focus on a small set of themes, but go deep on them:\n\nInference should be sequential. We want recursive estimation techniques to handle data that are streaming in real-time;\nPredictions should be probabilistic. We want point, interval, and density forecasts that incorporate many sources of uncertainty;\nIf you can manipulate joint distributions, you can do time series analysis. This is obscured in too many TS texts;\n\n\nAnd there is a secret fourth theme:\n\nA Bayesian approach is an excellent way of achieving the goals of sequential inference and probabilistic prediction."
  },
  {
    "objectID": "slides/01-ar-1-structure.html#the-simplest-non-trivial-time-series-model",
    "href": "slides/01-ar-1-structure.html#the-simplest-non-trivial-time-series-model",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "The simplest non-trivial time series model",
    "text": "The simplest non-trivial time series model\nThe autoregression of order 1, or AR(1):\n\n\\[\n\\begin{aligned}\ny_t\n&=\n\\beta_0\n+\n\\beta_1\ny_{t-1}\n+\n\\varepsilon_t,\n\\quad\n\\varepsilon_t\\iid\\text{N}(0\\com\\sigma^2)\n\\\\\ny_0\n&\\sim\n\\text{N}(\\mu_0\\com \\initvar),\n\\end{aligned}\n\\]\n\n\nThat’s the recursive form. It implies a joint distribution written marginal-conditional style:\n\n\n\\[\n\\begin{aligned}\np(y_{0:T})\n&=\np(y_0)\n\\prod_{t=1}^T\np(y_t\\given y_{0:t-1})\n\\\\\n&=\np(y_0)\n\\prod_{t=1}^T\np(y_t\\given y_{t-1}).\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#putting-the-regression-in-autoregression",
    "href": "slides/01-ar-1-structure.html#putting-the-regression-in-autoregression",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Putting the “regression” in autoregression",
    "text": "Putting the “regression” in autoregression\nIn some sense the AR(1) is “just” a simple linear regression\n\\[\ny_t\n=\n\\beta_0\n+\n\\beta_1\nx_t\n+\n\\varepsilon_t,\n\\quad\n\\varepsilon_t\\iid\\text{N}(0\\com\\sigma^2),\n\\]\nwhere we took the predictor to be \\(x_t=y_{t-1}\\).\nThis perspective obscures the dependence structure, but will be useful for likelihood-based inference next week."
  },
  {
    "objectID": "slides/01-ar-1-structure.html#the-game-plan-for-the-next-few-lectures",
    "href": "slides/01-ar-1-structure.html#the-game-plan-for-the-next-few-lectures",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "The game plan for the next few lectures",
    "text": "The game plan for the next few lectures\nDo “All of Time Series Analysis” for this simple model:\n\nwhat is the joint distribution and what is its structure (marginals, conditionals, moments);\nstationarity;\nclassical inference;\nBayesian inference;\n(emphasizing recursive estimation in both cases);\nprobabilistic forecasting from both inferential perspectives;\nevaluating probabilistic forecasts.\n\n\nThe rest of the course is in some sense theme and variations."
  },
  {
    "objectID": "slides/01-ar-1-structure.html#last-time-substitution-fest",
    "href": "slides/01-ar-1-structure.html#last-time-substitution-fest",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Last time: substitution fest",
    "text": "Last time: substitution fest\n\n\\(t=0\\):\n\\[\ny_0=y_0\n\\]\n\n\n\\(t=1\\):\n\\[\ny_1=\\beta_0+\\beta_1y_0+\\varepsilon_1\n\\]\n\n\n\\(t=2\\):\n\\[\n\\begin{aligned}\ny_2\n&=\n\\beta_0\n+\n\\beta_1\ny_1\n+\n\\varepsilon_2\n\\\\\n&=\n\\beta_0\n+\n\\beta_1\n(\\beta_0+\\beta_1y_0+\\varepsilon_1)\n+\n\\varepsilon_2\n\\\\\n&=\n\\beta_0+\\beta_0\\beta_1+\\beta_1^2y_0+\\beta_1\\varepsilon_1+\\varepsilon_2.\n\\end{aligned}\n\\]\n\n\n\\(t=3\\):\n\\[\n\\begin{aligned}\ny_3\n&=\n\\beta_0\n+\n\\beta_1\ny_2\n+\n\\varepsilon_3\n\\\\\n&=\n\\beta_0\n+\n\\beta_1\n(\\beta_0+\\beta_0\\beta_1+\\beta_1^2y_0+\\beta_1\\varepsilon_1+\\varepsilon_2)\n+\n\\varepsilon_3\n\\\\\n&=\n\\beta_0+\\beta_0\\beta_1+\\beta_0\\beta_1^2+\\beta_1^3y_0+\\beta_1^2\\varepsilon_1+\\beta_1\\varepsilon_2+\\varepsilon_3.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#get-organized-and-notice-the-pattern",
    "href": "slides/01-ar-1-structure.html#get-organized-and-notice-the-pattern",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Get organized and notice the pattern",
    "text": "Get organized and notice the pattern\n\\[\n\\begin{aligned}\ny_0\n&=\n&\ny_0\n\\\\\ny_1\n&=\n\\beta_0\n&+\\,\n\\beta_1y_0\n&+\\,\n{\\color{white}\\beta_{\\color{white}1}}\\varepsilon_1\n\\\\\ny_2\n&=\n\\beta_0+\\beta_0\\beta_1\n&+\\,\n\\beta_1^2y_0\n&+\\,\n\\beta_1\\varepsilon_1\n+\n{\\color{white}\\beta_{\\color{white}1}}\\varepsilon_2\n\\\\\ny_3\n&=\n\\beta_0+\\beta_0\\beta_1+\\beta_0\\beta_1^2\n&+\\,\n\\beta_1^3y_0\n&+\\,\n\\beta_1^2\\varepsilon_1\n+\n\\beta_1\\varepsilon_2\n+\n\\varepsilon_3\n\\\\\n&\\vdots\n\\\\\ny_t\n&=\n\\beta_0\\sum\\limits_{i=0}^{t-1}\\beta_1^i\n&+\\,\n\\beta_1^ty_0\n&+\\,\n\\sum\\limits_{i=0}^{t-1}\\beta_1^i\\varepsilon_{t-i}\n\\\\\n&\\vdots\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#the-matrix-equation",
    "href": "slides/01-ar-1-structure.html#the-matrix-equation",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "The matrix equation",
    "text": "The matrix equation\nWriting the linear system as a matrix equation, you get:\n\n\\[\n\\begin{aligned}\n\\underbrace{\n\\begin{bmatrix}\ny_0\n\\\\\ny_1\n\\\\\ny_2\n\\\\\ny_3\n\\\\\n\\vdots\n\\\\\ny_T\n\\end{bmatrix}\n}_{\\By}\n&=\n\\underbrace{\n\\beta_0\n\\begin{bmatrix}\n0\n\\\\\n1\n\\\\\n1+\\beta_1\n\\\\\n1+\\beta_1+\\beta_1^2\n\\\\\n\\vdots\n\\\\\n\\sum\\limits_{i=0}^{T-1}\\beta_1^i\n\\end{bmatrix}\n}_{\\Bc}\n+\n\\underbrace{\n\\begin{bmatrix}\n1 & 0 & 0 & 0 & \\cdots & 0 \\\\\n\\beta_1 & 1 & 0 & 0 & \\cdots & 0 \\\\\n\\beta_1^2 & \\beta_1 & 1 & 0 & \\cdots & 0 \\\\\n\\beta_1^3 & \\beta_1^2 & \\beta_1 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\beta_1^T & \\beta_1^{T-1} & \\beta_1^{T-2} & \\beta_1^{T-3} & \\cdots & 1 \\\\\n\\end{bmatrix}\n}_{\\BA}\n\\underbrace{\n\\begin{bmatrix}\ny_0\n\\\\\n\\varepsilon_1\n\\\\\n\\varepsilon_2\n\\\\\n\\varepsilon_3\n\\\\\n\\vdots\n\\\\\n\\varepsilon_T\n\\end{bmatrix}\n}_{\\Be}\n.\n\\end{aligned}\n\\]\n\n\nBy assumption,\n\\[\n\\Be\n\\sim\n\\text{N}_{T+1}\n\\left(\n\\Bm=\n\\begin{bmatrix}\n\\mu_0\n\\\\\n\\Bzero\n\\end{bmatrix}\n\\com\n\\BS\n=\n\\begin{bmatrix}\n\\initvar\n&\n\\Bzero^\\tr\\\\\n\\Bzero & \\sigma^2\\BI_T\n\\end{bmatrix}\n\\right)\n.\n\\]\n\n\nSo by linearity,\n\\[\n\\By\n\\sim\n\\text{N}_{T+1}\n\\left(\n\\Bmu=\\Bc+\\BA\\Bm\n\\com\n\\BSigma\n=\n\\BA\\BS\\BA^\\tr\n\\right)\n.\n\\]"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#whats-the-mean",
    "href": "slides/01-ar-1-structure.html#whats-the-mean",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "What’s the mean?",
    "text": "What’s the mean?\n\nRecall that\n\\[\nE\\left(\n\\sum\\limits_{i=1}^na_iX_i\n\\right)\n=\n\\sum\\limits_{i=1}^na_iE(X_i).\n\\]\n\n\nSo:\n\\[\n\\begin{aligned}\nE(y_t)\n&=\nE\n\\left(\n\\beta_0\\sum\\limits_{i=0}^{t-1}\\beta_1^i\n+\n\\beta_1^ty_0\n+\n\\sum\\limits_{i=0}^{t-1}\\beta_1^i\\varepsilon_{t-i}\n\\right)\n\\\\\n&=\n\\beta_0\\sum\\limits_{i=0}^{t-1}\\beta_1^i\n+\n\\beta_1^t\nE\n\\left(\ny_0\n\\right)\n+\n\\sum\\limits_{i=0}^{t-1}\\beta_1^i\nE\n\\left(\n\\varepsilon_{t-i}\n\\right)\n\\\\\n&=\n\\beta_0\\sum\\limits_{i=0}^{t-1}\\beta_1^i\n+\n\\beta_1^t\n\\mu_0.\n\\end{aligned}\n\\]\n\n\nWe only used \\(E(\\varepsilon_t)=0\\). Didn’t need independence or normality."
  },
  {
    "objectID": "slides/01-ar-1-structure.html#whats-the-variance",
    "href": "slides/01-ar-1-structure.html#whats-the-variance",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "What’s the variance?",
    "text": "What’s the variance?\n\nRecall that for independent random variables,\n\\[\n\\var\\left(\n\\sum\\limits_{i=1}^na_iX_i\n\\right)\n=\n\\sum\\limits_{i=1}^na_i^2\\var(X_i).\n\\]\n\n\nSo:\n\\[\n\\begin{aligned}\n\\var(y_t)\n&=\n\\var\n\\left(\n\\beta_0\\sum\\limits_{i=0}^{t-1}\\beta_1^i\n+\n\\beta_1^ty_0\n+\n\\sum\\limits_{i=0}^{t-1}\\beta_1^i\\varepsilon_{t-i}\n\\right)\n\\\\\n&=\n\\beta_1^{2t}\n\\var\n\\left(\ny_0\n\\right)\n+\n\\sum\\limits_{i=0}^{t-1}\\beta_1^{2i}\n\\var\n\\left(\n\\varepsilon_{t-i}\n\\right)\n\\\\\n&=\n\\beta_1^{2t}\\initvar\n+\n\\sigma^2\n\\sum\\limits_{i=0}^{t-1}\\beta_1^{2i}.\n\\end{aligned}\n\\]\n\n\nWe used time-invariance and independence of \\(\\varepsilon_t\\) but not normality."
  },
  {
    "objectID": "slides/01-ar-1-structure.html#whats-the-covariance",
    "href": "slides/01-ar-1-structure.html#whats-the-covariance",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "What’s the covariance?",
    "text": "What’s the covariance?\nHere you go:\n\\[\n\\cov(y_t\\com y_s)\n=\n\\begin{cases}\n\\beta_1^{s-t}\\var(y_t) & t\\leq s\\\\\n\\beta_1^{t-s}\\var(y_s) & s &lt; t.\n\\end{cases}\n\\]\nDerivation deferred to Problem Set 1."
  },
  {
    "objectID": "slides/01-ar-1-structure.html#summary",
    "href": "slides/01-ar-1-structure.html#summary",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Summary",
    "text": "Summary\n\nRecursive form:\n\\[\n\\begin{aligned}\ny_t\n&=\n\\beta_0\n+\n\\beta_1\ny_{t-1}\n+\n\\varepsilon_t,\n\\quad\n\\varepsilon_t\\iid\\text{N}(0\\com\\sigma^2)\n\\\\\ny_0\n&\\sim\n\\text{N}(\\mu_0\\com \\initvar).\n\\end{aligned}\n\\]\n\n\nJoint distribution:\n\\[\n\\begin{bmatrix}\ny_0 & y_1 & \\cdots & y_T\n\\end{bmatrix}^\\tr\n\\sim\\text{N}_{T+1}\\left(\\Bmu\\com \\BSigma\\right).\n\\]\n\n\nMoments:\n\\[\n\\begin{aligned}\nE(y_t)\n&=\n\\beta_0\\sum\\limits_{i=0}^{t-1}\\beta_1^i\n+\n\\beta_1^t\\mu_0\n\\\\\n\\var(y_t)\n&=\n\\sigma^2\n\\sum\\limits_{i=0}^{t-1}\\beta_1^{2i}+\n\\beta_1^{2t}\\initvar\n\\\\\n\\cov(y_t\\com y_s)\n&=\n\\begin{cases}\n\\beta_1^{s-t}\\var(y_t) & t\\leq s\\\\\n\\beta_1^{t-s}\\var(y_s) & s &lt; t.\n\\end{cases}\n.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#some-special-cases",
    "href": "slides/01-ar-1-structure.html#some-special-cases",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Some special cases",
    "text": "Some special cases\n\n\niid: set \\(\\beta_1=0\\) (and \\(\\mu_0=\\beta_0\\); \\(\\initvar=\\sigma^2\\)), and\n\n\n\\[\ny_t\\iid\\text{N}(\\beta_0\\com\\sigma^2).\n\\]\n\n\n\n\nrandom walk with drift: set \\(\\beta_1=1\\), and\n\n\n\n\\[\n\\begin{aligned}\nE(y_t)\n&=\n\\beta_0t+\\mu_0\n\\\\\n\\var(y_t)\n&=\n\\sigma^2t+\\initvar.\n\\end{aligned}\n\\]\n\n\n\n\nfunky: set \\(\\beta_1=-1\\), and\n\n\n\n\\[\n\\begin{aligned}\nE(y_t)\n&=\n\\beta_0\\frac{1 - (-1)^t}{2}+(-1)^t\\mu_0\n\\\\\n\\var(y_t)\n&=\n\\sigma^2t+\\initvar.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#what-does-this-look-like",
    "href": "slides/01-ar-1-structure.html#what-does-this-look-like",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "What does this look like?",
    "text": "What does this look like?\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\nlibrary(shiny)\n\nsimulate_ar_1 &lt;- function(T, b0, b1, s, m0, s0){\n  y &lt;- numeric(T)\n  y[1] &lt;- rnorm(1, m0, s0)\n  for(t in 2:T){\n    y[t] &lt;- b0 + b1 * y[t - 1] + rnorm(1, 0, s)\n  }\n  return(y)\n}\n\nar_1_mean &lt;- function(t, b0, b1, m0){\n  if(t == 0){\n    return(m0)\n  }else{\n    return(b0 * sum(b1 ^ (0:(t-1))) + m0 * (b1^t)) \n  }\n}\n\nar_1_var &lt;- function(t, b1, s, s0){\n  if(t == 0){\n    return(s0^2)\n  }else{\n    return((s0^2) * (b1^(2*t)) + (s^2) * sum(b1 ^ (2*(0:(t-1)))))\n  }\n}\n\nar_1_sd &lt;- function(t, b1, s, s0){\n  sqrt(ar_1_var(t, b1, s, s0))\n}\n\n# Define UI for application that draws a histogram\nui &lt;- fluidPage(\n  \n  # Application title\n  titlePanel(\"Marginal distributions and sample paths of a Gaussian AR(1)\"),\n  \n  # Sidebar with a slider input for number of bins \n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"b0\",\n                  \"β₀\",\n                  min = -5,\n                  max = 5,\n                  value = 0,\n                  step = 0.1),\n      sliderInput(\"b1\",\n                  \"β₁\",\n                  min = -2,\n                  max = 2,\n                  value = 0,\n                  step = 0.1),\n      sliderInput(\"s\",\n                  \"σ\",\n                  min = 0,\n                  max = 2,\n                  value = 1, \n                  step = 0.1),\n      sliderInput(\"m0\",\n                  \"μ₀\",\n                  min = -5,\n                  max = 5,\n                  value = 0,\n                  step = 0.1),\n      sliderInput(\"T\",\n                  \"T\",\n                  min = 20,\n                  max = 200,\n                  step = 20,\n                  value = 100),\n      actionButton(\"redo\", \"New sample path\"),\n    ),\n    \n    # Show a plot of the generated distribution\n    mainPanel(\n      plotOutput(\"distPlot\", height = \"600px\")\n    )\n  )\n)\n\n# Define server logic required to draw a histogram\nserver &lt;- function(input, output) {\n  \n  output$distPlot &lt;- renderPlot({\n    input$redo\n    b0 &lt;- input$b0\n    b1 &lt;- input$b1\n    redo &lt;- input$redo\n    T &lt;- input$T\n    s &lt;- input$s\n    m0 &lt;- input$m0\n    s0 = 1\n    \n    range = 0:T\n    alpha = c(0.01, seq(0.1, 0.9, by = 0.1))\n    \n    middle &lt;- sapply(range, ar_1_mean, b0, b1, m0)\n    sds &lt;- sapply(range, ar_1_sd, b1, s, s0)\n    \n    \n    plot(range, middle, type = \"l\",\n         xaxt = \"n\", \n         yaxt = \"n\",\n         xlab = \"t\",\n         ylab = expression(y[t]),\n         ylim = c(-20, 20), bty = \"n\",\n         col = \"white\")\n    \n    for(a in alpha){\n      \n      U = qnorm(1 - a / 2, mean = middle, sd = sds)\n      L = qnorm(a / 2, mean = middle, sd = sds)\n      \n      polygon(\n        c(range, rev(range)),\n        c(U, rev(L)),\n        col = rgb(1, 0, 0, 0.15),\n        border = NA\n      )\n    }\n    \n    inc = 20\n    axis(1, pos = 0, at = seq(0, max(range), by = inc), \n         labels = c(NA, seq(inc, max(range), by = inc)))\n    axis(2, pos = 0)\n    \n    lines(range, simulate_ar_1(max(range) + 1, b0, b1, s, m0, s0), col = \"black\", lwd = 2)\n  })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#assume-beta_1neq-1",
    "href": "slides/01-ar-1-structure.html#assume-beta_1neq-1",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Assume \\(\\beta_1\\neq 1\\)\n",
    "text": "Assume \\(\\beta_1\\neq 1\\)\n\nFinite geometric sum formula gives:\n\\[\n\\begin{aligned}\nE(y_t)\n&=\n\\beta_0\\frac{1-\\beta_1^t}{1-\\beta_1}\n+\n\\beta_1^t\\mu_0\n\\\\\n\\var(y_t)\n&=\n\\sigma^2\n\\frac{1-\\beta_1^{2t}}{1-\\beta_1^2}+\n\\beta_1^{2t}\\initvar.\n\\end{aligned}\n\\]\nWhat happens as \\(t\\to\\infty\\)?"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#stationarity",
    "href": "slides/01-ar-1-structure.html#stationarity",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Stationarity",
    "text": "Stationarity\nA joint distribution is (strictly) stationary if it is “shift invariant”:\n\\[\n\\{y_{t_1}\\com y_{t_2}\\com ...\\com y_{t_n}\\}\\overset{d}{=}\\{y_{t_1+h}\\com y_{t_2+h}\\com ...\\com y_{t_n+h}\\}.\n\\]\nThe Gaussian AR(1) with \\(|\\beta_1|&lt;1\\) has this property."
  },
  {
    "objectID": "slides/01-ar-1-structure.html#stationary-ar1",
    "href": "slides/01-ar-1-structure.html#stationary-ar1",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Stationary AR(1)",
    "text": "Stationary AR(1)\nIf \\(-1&lt;\\beta_1&lt;1\\), \\(\\mu_0=\\beta_0/(1-\\beta_1)\\), and \\(\\initvar=\\sigma^2/(1-\\beta_1^2)\\), then the AR(1) is strictly stationary with the following:\n\\[\n\\begin{aligned}\nE(y_t)\n&=\n\\frac{\\beta_0}{1-\\beta_1}\n\\\\\n\\var(y_t)\n&=\n\\frac{\\sigma^2}{1-\\beta_1^2}\n\\\\\n\\cov(y_t\\com y_s)\n&=\n\\beta_1^{|t-s|}\\var(y_t)\n=\n\\beta_1^{|t-s|}\\frac{\\sigma^2}{1-\\beta_1^2}.\n\\end{aligned}\n\\]\nThe common marginal shared by all \\(y_t\\) is called the stationary distribution:\n\\[\ny_t\\sim\\text{N}\\left(\\frac{\\beta_0}{1-\\beta_1}\\com \\frac{\\sigma^2}{1-\\beta_1^2}\\right).\n\\]\nSo “did: dependent but identically distributed.”"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#chat-about-stationarity",
    "href": "slides/01-ar-1-structure.html#chat-about-stationarity",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Chat about stationarity",
    "text": "Chat about stationarity\n\nHopefully your first thought upon encountering this concept is “real data won’t be stationary.” True!\n\nSo why care about this?\n\nCute from a pure math point of view;\nIf you simulate distributions with Markov chain Monte Carlo (MCMC), you bow down at the altar of stationarity;\nStationarity means “dependent but not too dependent.” You can redo classical statistical theory replacing “iid” with “stationary,” and you don’t necessarily have to change a whole lot;\n(A Bayesian won’t necessarily care about that last point.)"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#autocovariance-of-a-stationary-process",
    "href": "slides/01-ar-1-structure.html#autocovariance-of-a-stationary-process",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Autocovariance of a stationary process",
    "text": "Autocovariance of a stationary process\nFor a stationary process, the covariance kernel satisfies\n\\[\n\\cov(y_t\\com y_{s})=\\cov(y_{t+h}\\com y_{s+h})\\quad \\forall (t\\com s\\com h).\n\\]\nSo you can define something called the autocovariance function:\n\\[\n\\gamma(h)=\\cov(y_{t+h}\\com y_{t}).\n\\]\nFor the AR(1), this is\n\\[\n\\begin{aligned}\n\\gamma(0)&=\\sigma^2/(1-\\beta_1^2)\n\\\\\n\\gamma(h)&=\\beta_1^h\\gamma(0).\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#what-does-the-autocov-function-look-like",
    "href": "slides/01-ar-1-structure.html#what-does-the-autocov-function-look-like",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "What does the autocov function look like?",
    "text": "What does the autocov function look like?"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#what-does-the-autocov-function-look-like-1",
    "href": "slides/01-ar-1-structure.html#what-does-the-autocov-function-look-like-1",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "What does the autocov function look like?",
    "text": "What does the autocov function look like?"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#what-does-the-autocov-function-look-like-2",
    "href": "slides/01-ar-1-structure.html#what-does-the-autocov-function-look-like-2",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "What does the autocov function look like?",
    "text": "What does the autocov function look like?"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#what-does-the-autocov-function-look-like-3",
    "href": "slides/01-ar-1-structure.html#what-does-the-autocov-function-look-like-3",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "What does the autocov function look like?",
    "text": "What does the autocov function look like?"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#what-does-the-autocov-function-look-like-4",
    "href": "slides/01-ar-1-structure.html#what-does-the-autocov-function-look-like-4",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "What does the autocov function look like?",
    "text": "What does the autocov function look like?"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#what-does-the-autocov-function-look-like-5",
    "href": "slides/01-ar-1-structure.html#what-does-the-autocov-function-look-like-5",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "What does the autocov function look like?",
    "text": "What does the autocov function look like?"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#plug-in-estimators-for-the-moments",
    "href": "slides/01-ar-1-structure.html#plug-in-estimators-for-the-moments",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Plug-in estimators for the moments",
    "text": "Plug-in estimators for the moments\nIf your AR(1) is stationary, then given data, expected values like\n\n\\[\n\\begin{aligned}\n\\mu\n&=\nE(y_t)\\\\\n\\gamma(h)\n&=\nE[(y_{t+h}-\\mu)(y_t-\\mu)],\n\\end{aligned}\n\\]\n\n\ncan be estimated with simple sample averages\n\n\n\\[\n\\begin{aligned}\n\\hat{\\mu}_T\n&=\n\\frac{1}{T}\\sum\\limits_{t=1}^Ty_t\n\\\\\n\\hat{\\gamma}_T(h)\n&=\n\\frac{1}{T}\\sum\\limits_{t=1}^{T-h}(y_{t+h}-\\hat{\\mu}_T)(y_t-\\hat{\\mu}_T).\n\\end{aligned}\n\\]\n\n\nThe last is the sample autocovariance function."
  },
  {
    "objectID": "slides/01-ar-1-structure.html#recall-some-facts-about-the-stationary-ar1",
    "href": "slides/01-ar-1-structure.html#recall-some-facts-about-the-stationary-ar1",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Recall some facts about the stationary AR(1)",
    "text": "Recall some facts about the stationary AR(1)\nHere’s what we’ve got:\n\n\\[\n\\begin{aligned}\n\\gamma(1)&=\\beta_1\\gamma(0) &&\\implies\\beta_1=\\frac{\\gamma(1)}{\\gamma(0)}\\\\\n\\mu&=\\frac{\\beta_0}{1-\\beta_1}&&\\implies\\beta_0=(1-\\beta_1)\\mu\\\\\n\\gamma(0)&=\\frac{\\sigma^2}{1-\\beta_1^2}&&\\implies \\sigma^2=(1-\\beta_1^2)\\gamma(0).\n\\end{aligned}\n\\]\n\n\nSo…any ideas how to estimate these?"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#the-baby-yule-walker-equations",
    "href": "slides/01-ar-1-structure.html#the-baby-yule-walker-equations",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "The (baby) Yule-Walker equation(s)",
    "text": "The (baby) Yule-Walker equation(s)\nEverywhere you see a “population” expected value, plug in the sample version:\n\n\\[\n\\begin{aligned}\n\\hat{\\beta}_1&=\\frac{\\hat{\\gamma}_T(1)}{\\hat{\\gamma}_T(0)}\n\\\\\n\\hat{\\beta}_0&=(1-\\hat{\\beta}_1)\\hat{\\mu}_T\\\\\n\\hat{\\sigma^2_T}&=(1-\\hat{\\beta}_1^2)\\hat{\\gamma}_T(0).\n\\end{aligned}\n\\]\nThese are method of moments estimators for the AR(1) parameters."
  },
  {
    "objectID": "slides/01-ar-1-structure.html#asymptotics",
    "href": "slides/01-ar-1-structure.html#asymptotics",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Asymptotics",
    "text": "Asymptotics\n\n\n\n\n\n\nShumway and Stoffer (2025) Property 3.7\n\n\nFor a stationary AR(1), the Yuke-Walker estimator has:\n\\[\n\\sqrt{T}(\\hat{\\beta}_1-\\beta_1)\\cd\\N\\left(0\\com\\frac{\\sigma^2}{\\gamma(0)}\\right).\n\\] And note in this case that \\(\\sigma^2 / \\gamma(0)=1-\\beta_1^2\\). The result remains true if you plug in the estimator of the asymptotic variance.\n\n\n\n\nAsymptotically valid \\(100\\times(1-\\alpha)\\%\\) confidence interval:\n\\[\n\\hat{\\beta}_1\\pm z_{1-\\alpha/2}\\sqrt{\\frac{1-\\hat{\\beta}_1^2}{T}}.\n\\]"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#what-happens-for-beta_1-close-to-pm1",
    "href": "slides/01-ar-1-structure.html#what-happens-for-beta_1-close-to-pm1",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "What happens for \\(\\beta_1\\) close to \\(\\pm1\\)?",
    "text": "What happens for \\(\\beta_1\\) close to \\(\\pm1\\)?\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\n# app.R\nlibrary(shiny)\n\n# ---------- Configuration ----------\nSAMPLE_SIZES &lt;- seq(10, 200, by = 20)   # smaller default for speed\nNSIM &lt;- 500                             # number of simulated AR(1) paths per sample size\nY_LIMITS &lt;- c(-1, 1)\n# ------------------------------------\n\n# Yule-Walker AR(1) estimator\nyule_walker_ar1 &lt;- function(x) {\n  n &lt;- length(x)\n  x &lt;- x - mean(x)  # center\n  gamma0 &lt;- mean(x^2)\n  gamma1 &lt;- mean(x[-1] * x[-n])\n  gamma1 / gamma0\n}\n\n# Simulate one stationary AR(1) with variance 1\nsimulate_ar_1 &lt;- function(T, b0, b1, s, m0, s0){\n  y &lt;- numeric(T)\n  y[1] &lt;- rnorm(1, m0, s0)\n  for(t in 2:T){\n    y[t] &lt;- b0 + b1 * y[t - 1] + rnorm(1, 0, s)\n  }\n  return(y)\n}\n\nui &lt;- fluidPage(\n  titlePanel(\"Sampling distribution of Yule–Walker AR(1) estimator\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"b1\", \"True value of β₁:\",\n                  min = -0.99, max = 0.99, value = 0.5, step = 0.01)\n    ),\n    mainPanel(\n      plotOutput(\"boxPlot\", height = \"600px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  \n  output$boxPlot &lt;- renderPlot({\n    b0 &lt;- 0\n    b1 &lt;- input$b1\n    s &lt;- 1\n    m0 &lt;- b0 / (1 - b1)\n    s0 &lt;- s / sqrt(1 - b1^2)\n    sizes &lt;- SAMPLE_SIZES\n    nsim &lt;- NSIM\n    \n    # Collect estimates in a list, one element per sample size\n    est_list &lt;- vector(\"list\", length(sizes))\n    \n    for (i in seq_along(sizes)) {\n      n &lt;- sizes[i]\n      phi_hats &lt;- numeric(nsim)\n      for (s in 1:nsim) {\n        x &lt;- simulate_ar_1(n, b0, b1, s, m0, s0)#simulate_ar1(n, phi)\n        phi_hats[s] &lt;- yule_walker_ar1(x)\n      }\n      est_list[[i]] &lt;- phi_hats\n    }\n    \n    # Draw boxplots side by side\n    par()\n    boxplot(est_list,\n            names = sizes,\n            ylim = Y_LIMITS,\n            xlab = \"Sample size T\",\n            ylab = \"Estimate\",\n            main = paste(\"True value: \", b1),\n            col = \"lightgray\", pch = 19)\n    \n    abline(h = b1, col = \"red\", lty = 2, lwd = 2)\n  })\n}\n\nshinyApp(ui, server)"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#a-word-about-assumptions",
    "href": "slides/01-ar-1-structure.html#a-word-about-assumptions",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "A word about assumptions",
    "text": "A word about assumptions\n\n\nNormality was not essential, but stationarity absolutely was:\n\nthere is no such thing as \\(\\gamma(h)\\) without it;\nthe estimating equations for method-of-moments make no sense if the process isn’t stationary;\n\n\nBut interesting “real-world” data probably are not stationary, and so it would be nice to have estimation techniques that don’t completely break down without it.\n\n\n…what’s that?\n\n\n\n\n\nDo you hear something?\n\n\n\n\n\nPeekaboo"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STA 542 Introduction to Time Series Analysis",
    "section": "",
    "text": "Below is a prospective outline for the course. Due dates are firm, but topics may change with advanced notice.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWEEK\nDATE\nPREPARE\nTOPIC\nMATERIALS\nDUE\n\n\n\n1\nMon, Aug 25\n\n🧑‍🏫 Welcome!\nslides\n\n\n\n\nWed, Aug 27\n\n🧑‍🏫 AR(1) structure\nslides\n\n\n\n2\nMon, Sep 1\n\n❌ Labor Day - No Lecture\n\n\n\n\n\n\nWed, Sep 3\n\n🧑‍🏫 AR(1) inference\nslides\n\n\n\n\nFri, Sep 5\n\n\n\n\nPSET 0 @ 5PM\n\n\n3\nMon, Sep 8\n\n🧑‍🏫 AR(1) forecasting\nslides\n\n\n\n\nWed, Sep 10\nGneiting et al (2007 JRSSB)\n🧑‍🏫 Forecast evaluation\nslides\n\n\n\n4\nMon, Sep 15\n\nSS: Ch. 3\n🧑‍🏫 AR(p)\n\n\n\n\n\nWed, Sep 17\n\nSS: Ch. 3\n🧑‍🏫 AR(p)\n\n\n\n\n\nFri, Sep 19\n\n\n\n\n\n\n5\nMon, Sep 22\n\nSS: Ch. 3\n🧑‍🏫 MA\n\n\nPSET 1 @ 11:30AM\n\n\n\nWed, Sep 24\n\nSS: Ch. 3\n🧑‍🏫 MA\n\n\n\n\n6\nMon, Sep 29\n\nSS: Ch. 3\n🧑‍🏫 ARMA\n\n\n\n\n\nWed, Oct 1\n\nSS: Ch. 3\n🧑‍🏫 ARMA\n\n\n\n\n\nFri, Oct 3\n\n\n\nPSET 2 @ 5PM\n\n\n7\nMon, Oct 6\n\nSS: Ch. 3\n🧑‍🏫 ARMA\n\n\n\n\n\nWed, Oct 8\n\n📝 Exam 1\n\n\n\n\n\n8\nMon, Oct 13\n\n❌ Fall Break - No Lecture\n\n\n\n\n\n\nWed, Oct 15\n\nSS: Ch. 5.5\n🧑‍🏫 VAR\n\n\n\n\n9\nMon, Oct 20\nKarlsson (2013 chapter)\n🧑‍🏫 VAR\n\n\n\n\n\nWed, Oct 22\n\n🧑‍🏫 VAR\n\n\n\n\n\nFri, Oct 24\n\n\n\nPSET 3 @ 5PM\n\n\n10\nMon, Oct 27\n\nPPC: Ch. 2\n🧑‍🏫 DLMs\n\n\n\n\n\nWed, Oct 29\n\nSkä: Ch. 4\n🧑‍🏫 DLMs\n\n\n\n\n11\nMon, Nov 3\n\nSkä: Ch. 8\n🧑‍🏫 DLMs\n\n\n\n\n\nWed, Nov 5\n\n🧑‍🏫 DLMs\n\n\n\n\n\nFri, Nov 7\n\n\n\nPSET 4 @ 5PM\n\n\n12\nMon, Nov 10\nScott (2002 JASA)\n🧑‍🏫 HMMs\n\n\n\n\n\nWed, Nov 12\n\n🧑‍🏫 HMMs\n\n\n\n\n13\nMon, Nov 17\nDoucet, Johansen (2008 chapter)\n🧑‍🏫 Nonlinear models\n\n\n\n\n\nWed, Nov 19\n\n🧑‍🏫 Nonlinear models\n\n\n\n\n\nFri, Nov 21\n\n\n\nPSET 5 @ 5PM\n\n\n14\nMon, Nov 24\n\n📝 Exam 2\n\n\n\n\n\n\nWed, Nov 26\n\n❌ Thanksgiving - No Lecture\n\n\n\n\n\n16\nWed, Dec 10\n\n\n\nProject @ 9AM"
  },
  {
    "objectID": "problems/bank/review/mvnormal-1.html",
    "href": "problems/bank/review/mvnormal-1.html",
    "title": "STA 542 Fall 2025",
    "section": "",
    "text": "Slicing-and-dicing the multivariate normal distribution is a very important skill in time series analysis, so let’s make sure we can do that.\n\nLet \\(\\mathbf{x}\\sim\\text{N}_n(\\boldsymbol{\\mu},\\,\\boldsymbol{\\Sigma})\\), and fix constants \\(\\mathbf{A}\\in\\mathbb{R}^{m\\times n}\\) and \\(\\mathbf{c}\\in\\mathbb{R}^m\\). Show that\n\n\\[\n\\mathbf{y}\n=\n\\mathbf{A}\n\\mathbf{x}\n+\n\\mathbf{c}\n\\sim\n\\text{N}_m\n\\left(\n\\mathbf{A}\n\\boldsymbol{\\mu}\n+\n\\mathbf{c}\n,\\,\n\\mathbf{A}\n\\boldsymbol{\\Sigma}\n\\mathbf{A}^{\\scriptscriptstyle\\mathsf{T}}\n\\right)\n.\n\\]\n\nConsider the joint distribution \\(p(\\mathbf{x},\\,\\mathbf{y})=p(\\mathbf{y}\\,|\\,\\mathbf{x})p(\\mathbf{x})\\) written hierarchically as \\(\\mathbf{x}\\sim\\text{N}_n(\\boldsymbol{\\mu},\\,\\boldsymbol{\\Sigma})\\) and \\(\\mathbf{y}\\,|\\,\\mathbf{x}\\sim\\text{N}_m(\\mathbf{A}\\mathbf{x}+\\mathbf{c},\\,\\mathbf{R})\\). Use the result in part a to show that the joint distribution implied by this hierarchy is\n\n\\[\n\\begin{bmatrix}\n\\mathbf{x}\n\\\\\n\\mathbf{y}\n\\end{bmatrix}\n\\sim\n\\text{N}_{n+m}\n\\left(\n\\begin{bmatrix}\n\\boldsymbol{\\mu}\n\\\\\n\\mathbf{A}\\boldsymbol{\\mu}+\\mathbf{c}\n\\end{bmatrix}\n,\\,\n\\begin{bmatrix}\n\\boldsymbol{\\Sigma}\n&\n\\boldsymbol{\\Sigma}\\mathbf{A}^{\\scriptscriptstyle\\mathsf{T}}\n\\\\\n\\mathbf{A}\n\\boldsymbol{\\Sigma}\n&\n\\mathbf{A}\n\\boldsymbol{\\Sigma}\n\\mathbf{A}^{\\scriptscriptstyle\\mathsf{T}} + \\mathbf{R}\n\\end{bmatrix}\n\\right)\n.\n\\]\n\nUse the result in part a to show that the linear combination of independent normals is normal. That is, if \\(x_i\\sim\\text{N}(\\mu_i,\\,\\sigma^2_i)\\) are independent and \\(a_i\\in\\mathbb{R}\\) are constant, then prove that\n\n\\[\n\\sum\\limits_{i=1}^na_ix_i\\sim\\text{N}\\left(\\sum\\limits_{i=1}^na_i\\mu_i,\\, \\sum\\limits_{i=1}^na_i^2\\sigma_i^2\\right).\n\\]\n\n\n\n\n\n\nHint\n\n\n\n\n\nJZ changed this hint on 8/30/2025. The original hint mentioned using the change-of-variables formula. That’s a fine solution if \\(\\mathbf{A}\\) is invertible, but the result holds even if \\(\\mathbf{A}\\) is not invertible, and we will often use that version (e.g. part c here). If you used change-of-variables, don’t redo it. We’ll award full credit if you did it right. But otherwise, consider using the moment-generating function (mgf) of a random vector \\(\\mathbf{x}\\):\n\\[\nM(\\mathbf{t})=E\\left(e^{\\mathbf{t}^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{x}}\\right).\n\\]\nAs in the univariate case, when it exists, the mgf uniquely characterizes the entire distribution of a random vector, just like the density, cdf, or characteristic function do."
  },
  {
    "objectID": "problems/bank/review/online-least-squares.html",
    "href": "problems/bank/review/online-least-squares.html",
    "title": "STA 542 Fall 2025",
    "section": "",
    "text": "An important consideration in time series analysis is sequential or online inference. In this environment, our data are not available to us all at once in a batch. Instead, the observations are streaming; they are arriving one-after-another in real-time, and we want to come up with a scheme that allows us to recursively update our inferences as new information arrives. Let’s explore this in the context of least squares regression.\n\n\n\n\n\n\nOLS review\n\n\n\nConsider the usual setting where we observe iid pairs \\(y_i\\in\\mathbb{R}\\) and \\(\\mathbf{x}_i\\in\\mathbb{R}^p\\) from the linear model:\n\\[\ny_i= \\mathbf{x}_i^{\\scriptscriptstyle\\mathsf{T}}\\boldsymbol{\\beta}+\\varepsilon_i,\\quad \\varepsilon_i\\overset{\\text{iid}}{\\sim}\\text{N}(0,\\,\\sigma^2).\n\\] Given \\(n\\) observations, we can form the matrices\n\\[\n\\mathbf{y}_n\n=\n\\underbrace{\n\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix}\n}\n_{n\\times 1}\n\\quad\n\\mathbf{X}_n\n=\n\\underbrace{\n\\begin{bmatrix}\n\\mathbf{x}_1^{\\scriptscriptstyle\\mathsf{T}} \\\\\n\\mathbf{x}_2^{\\scriptscriptstyle\\mathsf{T}} \\\\\n\\vdots \\\\\n\\mathbf{x}_n^{\\scriptscriptstyle\\mathsf{T}}\n\\end{bmatrix}\n}_{n\\times p}\n\\quad\n\\boldsymbol{\\varepsilon}_n\n=\n\\underbrace{\n\\begin{bmatrix}\n\\varepsilon_1 \\\\\n\\varepsilon_2 \\\\\n\\vdots \\\\\n\\varepsilon_n\n\\end{bmatrix}\n}\n_{n\\times 1}\n,\n\\] and we can rewrite the model in matrix form:\n\\[\n\\mathbf{y}_n=\\mathbf{X}_n\\boldsymbol{\\beta}+\\boldsymbol{\\varepsilon}_n,\\quad \\boldsymbol{\\varepsilon}_n\\sim\\text{N}_n(\\mathbf{0},\\,\\sigma^2\\mathbf{I}_n).\n\\] The ordinary least squares (OLS) estimator of the regression coefficients is \\(\\hat{\\boldsymbol{\\beta}}_n=(\\mathbf{X}_n^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{X}_n)^{-1}\\mathbf{X}_n^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{y}_n.\\)\n\n\nHopefully that was all review, and it describes how to do batch or offline inference for the linear model. We are assuming that all of our data are available in a single batch, and we only need to compute the estimate once. But in our class, we will often care about streaming or online inference: efficiently computing the sequence of estimates \\(\\hat{\\boldsymbol{\\beta}}_1\\), \\(\\hat{\\boldsymbol{\\beta}}_2\\), \\(\\hat{\\boldsymbol{\\beta}}_3\\), … as the pairs \\((\\mathbf{x}_1,\\,y_1)\\), \\((\\mathbf{x}_2,\\,y_2)\\), \\((\\mathbf{x}_3,\\,y_3)\\), … arrive one-after-another in real-time.\n\nSo, imagine we have observed \\(n-1\\) pairs, and we have computed the estimate \\(\\hat{\\boldsymbol{\\beta}}_{n-1}=(\\mathbf{X}_{n-1}^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{X}_{n-1})^{-1}\\mathbf{X}_{n-1}^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{y}_{n-1}.\\) Then, a new observation \\((\\mathbf{x}_n,\\,y_n)\\) arrives. How can we efficiently update our estimate of \\(\\boldsymbol{\\beta}\\) to incorporate this new information? In other words, how can we quickly compute \\(\\hat{\\boldsymbol{\\beta}}_{n}\\) given only \\(\\hat{\\boldsymbol{\\beta}}_{n-1}\\) and the new \\((\\mathbf{x}_n,\\,y_n)\\)? Of course, we could always just add new rows to \\(\\mathbf{X}_{n-1}\\) and \\(\\mathbf{y}_{n-1}\\) and recompute the whole estimate from scratch, but that is super inefficient and we can do better. To that end, show that the new estimate is related to the old estimate and the new data by the following recursion: \\(\\hat{\\boldsymbol{\\beta}}_n=\\hat{\\boldsymbol{\\beta}}_{n-1}+\\mathbf{k}_n(y_n-\\mathbf{x}_n^{\\scriptscriptstyle\\mathsf{T}}\\hat{\\boldsymbol{\\beta}}_{n-1})\\). What is \\(\\mathbf{k}_n\\)?\nInterpret the recursion you derived in part a. Isn’t \\((y_n-\\mathbf{x}_n^{\\scriptscriptstyle\\mathsf{T}}\\hat{\\boldsymbol{\\beta}}_{n-1})\\) a residual? What is this \\(\\mathbf{k}_n\\) thing doing? What would it mean if we had \\(\\hat{\\boldsymbol{\\beta}}_n=\\hat{\\boldsymbol{\\beta}}_{n-1}\\)?\nWrite a for loop in R that uses your recursion to process a dataset one observation at a time, and verify that at the end of the loop, you get the same estimates that you would have gotten if you had just used lm. Do this for the mtcars dataset and the regression mpg ~ wt.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe Sherman–Morrison formula is a great tool! For invertible \\(\\mathbf{A}\\in\\mathbb{R}^{p\\times p}\\) and \\(\\mathbf{v}\\in\\mathbb{R}^p\\), we have\n\\[\n(\\mathbf{A}+\\mathbf{v}\\mathbf{v}^{\\scriptscriptstyle\\mathsf{T}})^{-1}\n=\n\\mathbf{A}^{-1}\n-\n\\frac{\n\\mathbf{A}^{-1}\n\\mathbf{v}\\mathbf{v}^{\\scriptscriptstyle\\mathsf{T}}\n\\mathbf{A}^{-1}\n}{\n1\n+\n\\mathbf{v}^{\\scriptscriptstyle\\mathsf{T}}\n\\mathbf{A}^{-1}\n\\mathbf{v}\n}\n.\n\\]"
  },
  {
    "objectID": "problems/bank/review/prediction-interval.html",
    "href": "problems/bank/review/prediction-interval.html",
    "title": "STA 542 Fall 2025",
    "section": "",
    "text": "Consider these data\n\\[\nX_1,\\,X_2,\\,...,\\,X_n\\overset{\\text{iid}}{\\sim}\\text{N}(\\theta,\\,1)\n\\]\nand the usual estimator \\(\\hat{\\theta}_n=\\sum\\limits_{i=1}^nX_i/n\\).\n\nDerive an exact \\(100\\times (1-\\alpha)\\%\\) classical confidence interval for the unknown mean \\(\\theta\\). That is, find a random interval \\((L_n,\\,U_n)\\) satisfying:\n\n\\[\nP\\left(L_n&lt;\\theta&lt;U_n\\right)=1-\\alpha.\n\\]\n\nDerive an exact \\(100\\times (1-\\alpha)\\%\\) classical prediction interval for a new observation \\(X_{n+1}\\). That is, find a random interval \\((L_n',\\,U_n')\\) satisfying\n\n\\[\nP\\left(L_n'&lt;X_{n+1}&lt;U_n'\\right)=1-\\alpha.\n\\]\n\nComment on the difference between the two intervals."
  },
  {
    "objectID": "problems/bank/review/mle.html",
    "href": "problems/bank/review/mle.html",
    "title": "STA 542 Fall 2025",
    "section": "",
    "text": "Here is a cdf indexed by a parameter \\(\\theta&gt;0\\):\n\\[\nF(x;\\,\\theta)\n=\n1 - \\frac{1}{(1+x)^\\theta},\\quad x\\geq 0.\n\\]\n\nDescribe an algorithm that will simulate (pseudo)random numbers from this distribution;\nLet \\(x_1,\\,x_2,\\,...,\\,x_n\\overset{\\text{iid}}{\\sim}F(x;\\,\\theta)\\) and derive the maximum likelihood estimator \\(\\hat{\\theta}_n^{(\\text{MLE})}\\) for the parameter \\(\\theta\\);\nDerive the exact sampling distribution of \\(\\hat{\\theta}_n^{(\\text{MLE})}\\);\nImplement the bootstrap to simulate the sampling distribution of \\(\\hat{\\theta}_n^{(\\text{MLE})}\\), and compare the bootstrap distribution to the exact one you derived in part c. In order to do this, you will probably need your algorithm from part a.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nHere is a schematic of how the bootstrap works. We start with some ground truth parameter value \\(\\theta_0&gt;0\\), and then implement this:\n\\[\n\\begin{matrix}\n\\text{0. True distribution} &&& F_{\\theta_0} && \\\\\n&&& \\downarrow && \\\\\n\\text{1. Original data} &&& x_{1:n} && \\\\\n&&& \\downarrow && \\\\\n\\text{2. Empirical distribution} &&& \\hat{F}_{n} && \\\\\n&\\swarrow &\\swarrow& \\cdots &\\searrow&\\searrow \\\\\n\\text{3. Synthetic data}&\\tilde{x}_{1:n}^{(1)} &\\tilde{x}_{1:n}^{(2)}& \\cdots &\\tilde{x}_{1:n}^{(k-1)}&\\tilde{x}_{1:n}^{(k)} \\\\\n&\\downarrow &\\downarrow& \\cdots &\\downarrow&\\downarrow \\\\\n\\text{4. Bootstrap estimates}&\\hat{\\theta}_n^{(1)} &\\hat{\\theta}_n^{(2)}& \\cdots &\\hat{\\theta}_n^{(k-1)}&\\hat{\\theta}_n^{(k)} \\\\\n\\end{matrix}\n\\]\nIf you implemented this correctly, and if the original sample size \\(n\\) is “large enough,” then a histogram of the \\(k\\) estimates \\(\\hat{\\theta}_n^{(1)}\\), \\(\\hat{\\theta}_n^{(2)}\\), …, \\(\\hat{\\theta}_n^{(k)}\\) ought to resemble the exact density you derived in part c."
  },
  {
    "objectID": "problems/bank/autoregression/covariance-kernel.html",
    "href": "problems/bank/autoregression/covariance-kernel.html",
    "title": "STA 542 Fall 2025",
    "section": "",
    "text": "Recall our good ol’ pal the Gaussian AR(1):\n\\[\n\\begin{aligned}\ny_t\n&=\n\\beta_0+\\beta_1y_{t-1}+\\varepsilon_t, && \\varepsilon_t\\overset{\\text{iid}}{\\sim}\\text{N}(0,\\,\\sigma^2)\n\\\\\ny_0\n&\\sim\n\\text{N}(\\mu_0,\\,v_0^2).\n\\end{aligned}\n\\]\nWithout assuming anything about \\(\\beta_1\\in\\mathbb{R}\\), derive the covariance kernel of the joint distribution:\n\\[\n\\gamma(s,\\, t) = \\text{cov}(y_s,\\,y_t),\\quad s,\\,t\\in\\mathbb{N}.\n\\]"
  },
  {
    "objectID": "problems/bank/autoregression/streaming-conjugate-bayes.html",
    "href": "problems/bank/autoregression/streaming-conjugate-bayes.html",
    "title": "STA 542 Fall 2025",
    "section": "",
    "text": "As we saw in lecture, (conditional) likelihood-based inference for the AR(1) proceeds identically to iid multiple regression. Whether it’s MLE or Bayes, nothing changes. In particular, imagine you place a conjugate normal-inverse-gamma prior on the model parameters:\n\\[\n\\begin{aligned}\n\\sigma^2\n&\\sim\n\\text{IG}(a_0,\\, b_0)\n\\\\\n\\boldsymbol{\\beta}\\,|\\, \\sigma^2\n&\\sim\n\\text{N}_2(\\mathbf{m}_0,\\,\\sigma^2\\mathbf{H}^{-1}_0)\n\\\\\ny_t\n\\,|\\,\ny_{t-1}\n,\\,\n\\boldsymbol{\\beta},\\,\\sigma^2\n&\\sim \\text{N}\n\\left(\n\\mathbf{x}_t^{\\scriptscriptstyle\\mathsf{T}}\\boldsymbol{\\beta},\\,\\sigma^2\n\\right), && \\mathbf{x}_t=\\begin{bmatrix}1 & y_{t-1}\\end{bmatrix}^{\\scriptscriptstyle\\mathsf{T}}.\n\\end{aligned}\n\\]\nThen you get a normal-inverse-gamma posterior:\n\\[\n\\begin{aligned}\n\\sigma^2\\,|\\, y_{0:T}\n&\\sim\n\\text{IG}(a_T,\\, b_T)\n\\\\\n\\boldsymbol{\\beta}\\,|\\, \\sigma^2,\\, y_{0:T}\n&\\sim\n\\text{N}_2(\\mathbf{m}_T,\\,\\sigma^2\\mathbf{H}^{-1}_T)\n\\\\\n\\\\\n\\mathbf{H}_T\n&=\n\\mathbf{X}_T^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{X}_T+\\mathbf{H}_0\n\\\\\n\\mathbf{m}_T\n&=\n\\mathbf{H}_T^{-1}(\\mathbf{X}_T^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{y}_T+\\mathbf{H}_0\\mathbf{m}_0)\n\\\\\na_T\n&=\na_0 + T/2\n\\\\\nb_T\n&=\nb_0\n+\n(\\mathbf{y}_T^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{y}_T+\\mathbf{m}_0^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{H}_0\\mathbf{m}_0-\\mathbf{m}_T^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{H}_T\\mathbf{m}_T)/2.\n\\end{aligned}\n\\]\nThose formulas for updating the hyperparameters are great for batch inference, but inefficient if the data are streaming. So let’s improve them. Imagine we have collected data \\(y_{0:t-1}\\) and characterized the posterior up to that point:\n\\[\n\\begin{aligned}\n\\sigma^2\\,|\\, y_{0:t-1}\n&\\sim\n\\text{IG}(a_{t-1},\\, b_{t-1})\n\\\\\n\\boldsymbol{\\beta}\\,|\\, \\sigma^2,\\, y_{0:t-1}\n&\\sim\n\\text{N}_2(\\mathbf{m}_{t-1},\\,\\sigma^2\\mathbf{H}^{-1}_{t-1}).\n\\end{aligned}\n\\]\nThen a single new observation \\(y_t\\) arrives and you want to characterize the new posterior:\n\\[\n\\begin{aligned}\n\\sigma^2\\,|\\, y_{0:t}\n&\\sim\n\\text{IG}(a_{t},\\, b_{t})\n\\\\\n\\boldsymbol{\\beta}\\,|\\, \\sigma^2,\\, y_{0:t}\n&\\sim\n\\text{N}_2(\\mathbf{m}_{t},\\,\\sigma^2\\mathbf{H}^{-1}_{t}).\n\\end{aligned}\n\\]\n\nDerive a recursion that takes the old hyperparameters and the new data and computes the new hyperparameters without inverting any matrices;\nc(sunspots) is a monthly time series of mean relative sunspot numbers from 1749 to 1983. Write a for loop in R that uses your recursion to fit an AR(1) to these data one observation at a time. Verify that at the end of the loop, you get the same estimates that you would have gotten if you had applied the batch update to the entire data set all at once."
  },
  {
    "objectID": "problems/bank/autoregression/dependent-lln.html",
    "href": "problems/bank/autoregression/dependent-lln.html",
    "title": "STA 542 Fall 2025",
    "section": "",
    "text": "Recall the Yule-Walker estimators for the parameters of a stationary AR(1). They are based on these estimating equations\n\\[\n\\begin{aligned}\n\\gamma(1)&=\\beta_1\\gamma(0) &&\\implies\\beta_1=\\frac{\\gamma(1)}{\\gamma(0)}\\\\\n\\mu&=\\frac{\\beta_0}{1-\\beta_1}&&\\implies\\beta_0=(1-\\beta_1)\\mu\\\\\n\\gamma(0)&=\\frac{\\sigma^2}{1-\\beta_1^2}&&\\implies \\sigma^2=(1-\\beta_1^2)\\gamma(0),\n\\end{aligned}\n\\]\nwhere \\(\\mu=E(y_t)\\) is the time-invariant expected value, \\(\\gamma(0)=\\text{var}(y_t)\\) is the time-invariant variance, and so on. The Yule-Walker approach is an example of method of moments, where we replace all of the “population” expected values with sample averages. In order for this method to work, the sample averages must actually be good estimates of their corresponding expected values. If the data are iid, then we have classical laws of large numbers that guarantee this. But in our case the data are not iid. So how do we know this is going to work?\n\n\n\n\n\n\nTheorem (law of large numbers for dependent data)\n\n\n\nLet \\(y_t\\) follow a process with time-invariant mean and shift-invariant covariance:\n\\[\n\\begin{aligned}\n\\mu\n&=\nE(y_t)\n&&\n\\forall\\,t\n\\\\\n\\gamma(h)\n&=\n\\text{cov}\n(y_{t+h},\\,y_t)\n&&\\forall\\,t.\n\\end{aligned}\n\\]\nFurthermore, assume that\n\\[\n\\sum\\limits_{h=0}^\\infty |\\gamma(h)|&lt;\\infty.\n\\]\nThen\n\\[\n\\bar{y}_T=\\frac{1}{T}\\sum\\limits_{t=1}^Ty_t\\overset{\\text{prob}}{\\to}\\mu.\n\\]\n\n\n\nProve the theorem;\nShow that the AR(1) with \\(|\\beta_1|&lt;1\\) satisfies the conditions."
  },
  {
    "objectID": "lecture-notes/ar-1.html",
    "href": "lecture-notes/ar-1.html",
    "title": "Everything You Always Wanted to Know About the AR(1) But Were Afraid to Ask",
    "section": "",
    "text": "A model is a probability distribution over a sequence..\nIn the spirit of Larry Wasserman’s All of Statistics and All of Nonparametric Statistics, this note introduces “All of Time Series Analysis” for the simplest time series model: the autoregression of order 1, or AR(1) for short. Topics include:\nOnce you’re comfortable with all that, everything else in the course is in some sense just a theme and variations on these main ideas."
  },
  {
    "objectID": "lecture-notes/ar-1.html#what-is-a-time-series-model",
    "href": "lecture-notes/ar-1.html#what-is-a-time-series-model",
    "title": "Everything You Always Wanted to Know About the AR(1) But Were Afraid to Ask",
    "section": "What is a time series model?",
    "text": "What is a time series model?\nIn some sense, all we are doing is manipulating joint distributions: computing their marginals, conditionals, means, and covariances. If you can do that, you can “do” time series. Of course, TA has its own special features\nBut dont lose te forest for the trees\nIf you can manipulate joint distributions, then you can do time series analysis is a generic sense. Of course, TS will pose their own special problems that require new techniques, but don’t lost the forest for the trees.\n\n\n\n\n\n\nA quick word on notation\n\n\n\nTwo things you have to get used to:\n\nWe will not use uppercase \\(Y_t\\) versus lowercase \\(y_t\\) to distinguish random variables versus fixed realizations. Everything will just be \\(y_t\\), and context will make clear if something functions as a random variable or a constant;\nThe symbol \\(p\\) will be aggressively abused and overloaded to represent any probability distribution, sometimes several in the same sentence. So \\(p(x)\\) is the density of the random variable \\(x\\), and \\(p(y,\\,z)\\) is the joint density of the random pair \\((y,\\,z)\\), and \\(p(\\boldsymbol{\\beta}\\,|\\,\\sigma^2)\\) is the conditional density of…you get the idea. CHANGE THIS TO REFER TO THE EQUATION\nAlso the \\(y_{i:j}\\) notation"
  },
  {
    "objectID": "lecture-notes/ar-1.html#the-simplest-time-series-model",
    "href": "lecture-notes/ar-1.html#the-simplest-time-series-model",
    "title": "Everything You Always Wanted to Know About the AR(1) But Were Afraid to Ask",
    "section": "The simplest time series model",
    "text": "The simplest time series model\nIn truth, the “simplest” time series model is the one where there is no dependence at all: \\(y_t\\overset{\\text{iid}}{\\sim}F\\). But in that case, why bother with STA 542? Things get interesting when there is dependence, and in particular the kind of dependence where you think the past can help to predict the future. The simplest model that captures this basic idea is the autoregression of order 1, or AR(1) for short:\n\\[\n\\begin{aligned}\ny_t&=\\beta_0+\\beta_1y_{t-1}+\\varepsilon_t,\\quad \\varepsilon_t\\overset{\\text{iid}}{\\sim}\\text{N}(0,\\,\\sigma^2)\\\\\ny_0&\\sim\\text{N}(\\mu_0,\\,\\gamma_0).\n\\end{aligned}\n\\]\nThis takes the form of a simple linear regression where \\(y_t\\) is the response and its first lagged value \\(y_{t-1}\\) is the predictor, hence the name\n\\(y_0\\) independent of the errors.\nI don’t like the notation for the initial condition variance."
  },
  {
    "objectID": "lecture-notes/ar-1.html#what-is-the-joint-distribution",
    "href": "lecture-notes/ar-1.html#what-is-the-joint-distribution",
    "title": "Everything You Always Wanted to Know About the AR(1) But Were Afraid to Ask",
    "section": "What is the joint distribution?",
    "text": "What is the joint distribution?\n\\(y_t\\,|\\,y_{t-1}\\sim\\text{N}(\\beta_0+\\beta_1y_{t-1},\\,\\sigma^2)\\)\n\\[\np(y_{0:T}) = p(y_0)\\prod_{t=1}^Tp(y_t\\,|\\,y_{t-1})\n\\]\nto understand the joint distribution\n\\[\n\\begin{aligned}\ny_0\n&=\ny_0\n\\\\\ny_1\n&=\n\\beta_0+\\beta_1y_0+\\varepsilon_1\n\\\\\ny_2\n&=\n\\beta_0+\\beta_1\n\\\\\ny_3\n&=\n\\\\\n&\\vdots\n\\\\\ny_t\n&=\n\\end{aligned}\n\\]\nsummarized\n\\[\n\\begin{bmatrix}\ny_0\\\\\ny_1\\\\\ny_2\\\\\ny_3\\\\\n\\vdots \\\\\ny_T\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nbloop\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n1 & 0 & 0 & 0 & \\cdots & 0\n\\end{bmatrix}\n\\begin{bmatrix}\ny_0\\\\\n\\varepsilon_1\\\\\n\\varepsilon_2\\\\\n\\varepsilon_3\\\\\n\\vdots \\\\\n\\varepsilon_T\n\\end{bmatrix}\n\\]\nSo \\(\\sim\\text{N}_{T+1}()\\) and \\(\\mathbf{y}\\) is a linear transformation of BLAH, so it is also normal, with mean and covariance. What are these?"
  },
  {
    "objectID": "lecture-notes/ar-1.html#stationarity",
    "href": "lecture-notes/ar-1.html#stationarity",
    "title": "Everything You Always Wanted to Know About the AR(1) But Were Afraid to Ask",
    "section": "Stationarity",
    "text": "Stationarity\nMost interesting time series are not stationary. This comes down to us as an historical artifact because certain kinds of (classical) statistical theory require it as an assumption in order to prove theorems, but it’s never true in practice.\nweird concept. dependent but not too dependent. necessary for stable prediction and inference, but do we really care? Probably false in practice, but so what. I just want a model that generates good and useful predictions.\nImportant historically and theoretically, and it simplifies the model tremendously."
  },
  {
    "objectID": "lecture-notes/ar-1.html#classical-inference",
    "href": "lecture-notes/ar-1.html#classical-inference",
    "title": "Everything You Always Wanted to Know About the AR(1) But Were Afraid to Ask",
    "section": "Classical inference",
    "text": "Classical inference\n\nMethod of moments\n\n\nMaximum likelihood estimaton"
  },
  {
    "objectID": "lecture-notes/ar-1.html#bayesian-inference",
    "href": "lecture-notes/ar-1.html#bayesian-inference",
    "title": "Everything You Always Wanted to Know About the AR(1) But Were Afraid to Ask",
    "section": "Bayesian inference",
    "text": "Bayesian inference\n\\[\n\\begin{aligned}\n\\sigma^2\n&\\sim\n\\text{IG}(a_0,\\,b_0)\n\\\\\n\\boldsymbol{\\beta}\n\\,|\\,\n\\sigma^2\n&\\sim\n\\text{N}_2(\\bar{\\boldsymbol{\\beta}}_0,\\,)\n\\\\\ny_t\\,|\\,y_{t-1},\\,\\boldsymbol{\\beta},\\,\\sigma^2\n&\\sim\n\\end{aligned}\n\\]\nSo we want this:\n\\[\np(\\boldsymbol{\\beta},\\,\\sigma^2\\,|\\,y_{0:T})\n=\n\\frac\n{p(y_{1:T}\\,|\\,\\boldsymbol{\\beta},\\,\\sigma^2,\\,y_0)p(\\boldsymbol{\\beta},\\,\\sigma^2)}\n{p(y_{1:T}\\,|\\,y_0)}\n\\]\nFor simplicity, we’re just going to condition on \\(y_0\\).\nAlso, we do not have to assume stationarity to do this.\nOur prior is conjugate, so we can compute the exact posterior.\n\n\n\n\n\n\nTip\n\n\n\nconjugate updates\n\n\nOther priors: normal - IG that isn’t conjugate, whatever else is in West’s book, the prior that enforces stationarity, either with accept-reject or the Heaps stuff, maybe that West Huerta thing idk."
  },
  {
    "objectID": "lecture-notes/ar-1.html#forecasting",
    "href": "lecture-notes/ar-1.html#forecasting",
    "title": "Everything You Always Wanted to Know About the AR(1) But Were Afraid to Ask",
    "section": "Forecasting",
    "text": "Forecasting\n\nClassical\ntypically ignores estimation uncertainty from coefficients.\nshow a picture comparing\n\n\n\n\n\n\nTip\n\n\n\nsieve bootstrap Buhlmann bernoulli\n\n\nthese draws are a discrete approximation.\npoint forecast: sample mean or median interval forecast: quantiles, HPD, whatever\n\n\nBayes\nProbabilistic prediction is automatic, and it’s easier (not easy, but easier) to augment your inferences with more sources of uncertainty: missing data, model uncertainty, etc."
  },
  {
    "objectID": "lecture-notes/ar-1.html#forecast-evaluation",
    "href": "lecture-notes/ar-1.html#forecast-evaluation",
    "title": "Everything You Always Wanted to Know About the AR(1) But Were Afraid to Ask",
    "section": "Forecast evaluation",
    "text": "Forecast evaluation\nmarginal distributions\njoint distributions\nconditional (forecast) distributions\nstationarity\ndependence structure\nmethod of moments (Yule-Walker)\nmaximum likelihood (unconditional and conditional)\nemphasize sequential recursions\nbayesian inference\nprobabilistic prediction\nparametric bootstrap\npoint prediction\ninterval prediction\ndensity prediction"
  },
  {
    "objectID": "problems/bank/autoregression/forecasting-fred.html",
    "href": "problems/bank/autoregression/forecasting-fred.html",
    "title": "STA 542 Fall 2025",
    "section": "",
    "text": "FRED is a fabulous source of economic time series data, and this problem encourages you to play around with it. Please do the following:\n\nSurf around FRED for a while until you find a time series that interests you, then use the fredr package to pull the data into R;\nUsing your online recursion from the previous problem, write a loop that sequentially fits a Bayesian AR(1) with conjugate prior to the time series you chose;\nIn each step of the loop, compute the PIT value comparing the one-step posterior predictive distribution \\(p(y_t\\,|\\,y_{0:t-1})\\) to the actual, realized value \\(y_t\\);\nAt the end of the loop, plot a histogram of the PITs. Are the forecasts well-calibrated? If not, what diagnostic information does the histogram convey?\n\n\n\n\n\n\n\nHint\n\n\n\nThe extraDistr package provides tools for the non-standard Student’s \\(t\\) distribution."
  },
  {
    "objectID": "problems/bank/autoregression/new-process.html",
    "href": "problems/bank/autoregression/new-process.html",
    "title": "STA 542 Fall 2025",
    "section": "",
    "text": "Consider this silly time series process:\n\\[\ny_t\n=\n\\alpha\n+\n\\varepsilon_t\n+\n\\theta_1\n\\varepsilon_{t-1}\n+\n\\theta_2\n\\varepsilon_{t-2}\n,\\quad \\varepsilon_t\\overset{\\text{iid}}{\\sim}\\text{N}(0,\\,\\sigma^2).\n\\]\nDerive the joint distribution of \\(\\mathbf{y}=\\begin{bmatrix}y_1&y_2&\\cdots&y_T\\end{bmatrix}^{\\scriptscriptstyle\\mathsf{T}}\\). In particular…\n\nWhat distribution family does the joint distribution belong to? Justify your answer; don’t just assert it.\nDerive the mean function:\n\n\\[\n\\mu(t)=E(y_t).\n\\]\n\nDerive the covariance kernel:\n\n\\[\n\\gamma(s,\\, t) = \\text{cov}(y_s,\\,y_t),\\quad s,\\,t\\in\\mathbb{N}.\n\\]\n\nIs \\(y_t\\) stationary? Why or why not?"
  },
  {
    "objectID": "problems/bank/autoregression/prior-predictive.html",
    "href": "problems/bank/autoregression/prior-predictive.html",
    "title": "STA 542 Fall 2025",
    "section": "",
    "text": "Models are wrong and parameters don’t exist. All that matters is observables.\n\nThat statement is the essence of the predictive view of inference. In its Bayesian incarnation, this view says that the only objects that truly matter are the predictive distributions:\n\\[\n\\begin{aligned}\np(\\mathbf{y}_{1:T})\n&=\n\\int\np(\\mathbf{y}_{1:T}\\,|\\,\\boldsymbol{\\theta})\np(\\boldsymbol{\\theta})\n\\,\\text{d}\\boldsymbol{\\theta}\n&&\n(\\text{prior predictive})\n\\\\\np(\\mathbf{y}_{T+1:T+H}\\,|\\,\\mathbf{y}_{1:T})\n&=\n\\int\np(\\mathbf{y}_{T+1:T+H}\\,|\\,\\mathbf{y}_{1:T},\\,\\boldsymbol{\\theta})\np(\\boldsymbol{\\theta}\\,|\\,\\mathbf{y}_{1:T})\n\\,\\text{d}\\boldsymbol{\\theta}.\n&&\n(\\text{posterior predictive})\n\\end{aligned}\n\\]\nModels, parameters, likelihoods, priors, even Bayes’ theorem – these are all just a means to the end of specifying and accessing predictive distributions for observable quantities. So, when you write down a model and a prior on the model’s parameters, you should tune the prior on parameters to capture your prior beliefs about how the data will look before you see them. In other words, the goal of prior elicitation should be accurately eliciting the prior predictive distribution, not the prior on parameters per se. Let’s take this seriously in a toy example.\nImagine you are a data scientist at Feta Platforms Inc, and one day your manager Zohn Jito knocks on your office door. Zohn Jito is an irritating fool who knows nothing about statistics, and he is always making ridiculous requests. On this day, he asks you to build a model for the log of daily feta sales, and he describes his beliefs about how the data should look:\n\nThe series should look smoother than white noise, but not as smooth as a random walk. Runs of consecutive positive values are expected, but they should not typically last more than 5 – 6 periods. The series should hover around zero, but occasionally wander away and then return.\n\nYou decide to build an AR(1) model with normal-inverse-gamma prior (maybe conjugate, maybe not?). Your tasks:\n\nchoose a prior and set the hyperparameters so that the prior predictive distribution is consistent with Zohn’s description;\nsimulate prior predictive sample paths of length 100 and visually check whether they look the way they should;\nif not, adjust your prior until they do."
  },
  {
    "objectID": "problems/bank/review/bayes.html",
    "href": "problems/bank/review/bayes.html",
    "title": "STA 542 Fall 2025",
    "section": "",
    "text": "Recall the parametric family from the previous part, and consider the Bayesian model\n\\[\n\\begin{aligned}\n\\theta &\\sim \\text{Gamma}(a_0,\\,b_0) \\\\\nx_i\\,|\\, \\theta &\\overset{\\text{iid}}{\\sim }F(x;\\,\\theta).\n\\end{aligned}\n\\]\n\nDerive the posterior distribution \\(p(\\theta\\,|\\,x_{1:n})\\);\nShow that the posterior mean \\(E(\\theta\\,|\\,x_{1:n})\\) can be written as a convex combination of the prior mean and the MLE;\nDerive the posterior predictive distribution \\(p(x_{n+1}\\,|\\,x_{1:n})\\)."
  },
  {
    "objectID": "problems/bank/review/students-t.html",
    "href": "problems/bank/review/students-t.html",
    "title": "STA 542 Fall 2025",
    "section": "",
    "text": "If \\(X\\sim t_\\nu\\) has Student’s \\(t\\)-distribution, then its density is\n\\[\nf_X(x)=\\frac{\\Gamma \\left(\\frac{\\nu + 1}{2}\\right)}{\\sqrt{\\pi\\nu}\\, \\Gamma\\left(\\frac{\\nu}{2}\\right)} \\left(1 + \\frac{x^2}{\\nu}\\right)^{-\\frac{\\nu+1}{2}},\\quad x\\in\\mathbb{R}.\n\\]\nIf you define a location-scale transformation \\(Y=\\mu+\\tau X\\) for constants \\(\\mu\\in\\mathbb{R}\\) and \\(\\tau&gt;0\\), then the new random variable \\(Y\\sim t(\\nu,\\,\\mu,\\,\\tau^2)\\) has a non-standard Student’s \\(t\\)-distribution. If we did this to the Cauchy for example (\\(\\nu=1\\)), then we get:\n\n\n\n\n\n\n\n\nBe careful not to immediately interpret \\(\\mu\\) and \\(\\tau^2\\) as mean and variance. Only if \\(\\nu&gt;1\\) is \\(E(Y)=\\mu\\), and \\(\\tau^2\\) is only proportional to the variance if \\(\\nu&gt;2\\).\nAnyway, consider a bivariate distribution \\(p(y,\\,\\sigma^2)\\) written hierarchically:\n\\[\n\\begin{aligned}\n\\sigma^2\n&\n\\sim\n\\text{IG}(a,\\,b)\n\\\\\ny\n\\,|\\,\n\\sigma^2\n&\n\\sim\n\\text{N}\n(m\n,\\,\n\\sigma^2v^2)\n.\n\\end{aligned}\n\\]\nDerive the marginal density of \\(y\\) and show that it has a non-standard Student’s \\(t\\)-distribution. What are the parameters?"
  },
  {
    "objectID": "problems/bank/review/mvnormal-2.html",
    "href": "problems/bank/review/mvnormal-2.html",
    "title": "STA 542 Fall 2025",
    "section": "",
    "text": "Recall that if \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are jointly distributed according to\n\\[\n\\begin{bmatrix}\n\\mathbf{x} \\\\\n\\mathbf{y}\n\\end{bmatrix}\n\\sim\n\\text{N}_{n+m}\n\\left(\n\\begin{bmatrix}\n\\boldsymbol{\\mu}_x \\\\\n\\boldsymbol{\\mu}_y\n\\end{bmatrix}\n,\\,\n\\begin{bmatrix}\n\\boldsymbol{\\Sigma}_x & \\boldsymbol{\\Sigma}_{xy}\\\\\n\\boldsymbol{\\Sigma}_{xy}^{\\scriptscriptstyle\\mathsf{T}} & \\boldsymbol{\\Sigma}_y\n\\end{bmatrix}\n\\right),\n\\]\nthen the conditional distribution is\n\\[\n\\mathbf{y} \\,|\\,\\mathbf{x}\n\\sim\n\\text{N}_m\n\\left(\n\\boldsymbol{\\mu}_y + \\boldsymbol{\\Sigma}_{xy}^{\\scriptscriptstyle\\mathsf{T}}\\boldsymbol{\\Sigma}_x^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_x)\n,\\,\n\\boldsymbol{\\Sigma}_y - \\boldsymbol{\\Sigma}_{xy}^{\\scriptscriptstyle\\mathsf{T}}\n\\boldsymbol{\\Sigma}_x^{-1}\n\\boldsymbol{\\Sigma}_{xy}\n\\right).\n\\]\nWe will use this fact often, but proving it is tedious. I’m not going to make you do it. This problem set is unreasonable enough already. Instead, I want you to read the proof very carefully. If you’re like me, there is some step in the derivation that will make you say “what just happened?” Drill down on that. Pick a section of the proof where you get confused, or where a technique is being used that you are unfamiliar with. Study that part carefully until you understand it, and then in your problem set write-up, explain it to me like I’m an idiot (shouldn’t be too difficult). Use pictures, numerical examples, specialize to the bivariate case, whatever you need to do.\n\n\n\n\n\n\nDon’t be a silly goose.\n\n\n\nWe are grading this part of the problem set for completion, but give it a good faith effort so that you actually, like, learn something."
  },
  {
    "objectID": "problems/pset-0.html",
    "href": "problems/pset-0.html",
    "title": "Problem Set 0",
    "section": "",
    "text": "In this class, I expect you to have a working knowledge of:\nBelow is a workout in all of the above.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "problems/pset-0.html#problem-1",
    "href": "problems/pset-0.html#problem-1",
    "title": "Problem Set 0",
    "section": "Problem 1",
    "text": "Problem 1\nSlicing-and-dicing the multivariate normal distribution is a very important skill in time series analysis, so let’s make sure we can do that.\n\nLet \\(\\mathbf{x}\\sim\\text{N}_n(\\boldsymbol{\\mu},\\,\\boldsymbol{\\Sigma})\\), and fix constants \\(\\mathbf{A}\\in\\mathbb{R}^{m\\times n}\\) and \\(\\mathbf{c}\\in\\mathbb{R}^m\\). Show that\n\n\\[\n\\mathbf{y}\n=\n\\mathbf{A}\n\\mathbf{x}\n+\n\\mathbf{c}\n\\sim\n\\text{N}_m\n\\left(\n\\mathbf{A}\n\\boldsymbol{\\mu}\n+\n\\mathbf{c}\n,\\,\n\\mathbf{A}\n\\boldsymbol{\\Sigma}\n\\mathbf{A}^{\\scriptscriptstyle\\mathsf{T}}\n\\right)\n.\n\\]\n\nConsider the joint distribution \\(p(\\mathbf{x},\\,\\mathbf{y})=p(\\mathbf{y}\\,|\\,\\mathbf{x})p(\\mathbf{x})\\) written hierarchically as \\(\\mathbf{x}\\sim\\text{N}_n(\\boldsymbol{\\mu},\\,\\boldsymbol{\\Sigma})\\) and \\(\\mathbf{y}\\,|\\,\\mathbf{x}\\sim\\text{N}_m(\\mathbf{A}\\mathbf{x}+\\mathbf{c},\\,\\mathbf{R})\\). Use the result in part a to show that the joint distribution implied by this hierarchy is\n\n\\[\n\\begin{bmatrix}\n\\mathbf{x}\n\\\\\n\\mathbf{y}\n\\end{bmatrix}\n\\sim\n\\text{N}_{n+m}\n\\left(\n\\begin{bmatrix}\n\\boldsymbol{\\mu}\n\\\\\n\\mathbf{A}\\boldsymbol{\\mu}+\\mathbf{c}\n\\end{bmatrix}\n,\\,\n\\begin{bmatrix}\n\\boldsymbol{\\Sigma}\n&\n\\boldsymbol{\\Sigma}\\mathbf{A}^{\\scriptscriptstyle\\mathsf{T}}\n\\\\\n\\mathbf{A}\n\\boldsymbol{\\Sigma}\n&\n\\mathbf{A}\n\\boldsymbol{\\Sigma}\n\\mathbf{A}^{\\scriptscriptstyle\\mathsf{T}} + \\mathbf{R}\n\\end{bmatrix}\n\\right)\n.\n\\]\n\nUse the result in part a to show that the linear combination of independent normals is normal. That is, if \\(x_i\\sim\\text{N}(\\mu_i,\\,\\sigma^2_i)\\) are independent and \\(a_i\\in\\mathbb{R}\\) are constant, then prove that\n\n\\[\n\\sum\\limits_{i=1}^na_ix_i\\sim\\text{N}\\left(\\sum\\limits_{i=1}^na_i\\mu_i,\\, \\sum\\limits_{i=1}^na_i^2\\sigma_i^2\\right).\n\\]\n\n\n\n\n\n\nHint\n\n\n\n\n\nJZ changed this hint on 8/30/2025. The original hint mentioned using the change-of-variables formula. That’s a fine solution if \\(\\mathbf{A}\\) is invertible, but the result holds even if \\(\\mathbf{A}\\) is not invertible, and we will often use that version (e.g. part c here). If you used change-of-variables, don’t redo it. We’ll award full credit if you did it right. But otherwise, consider using the moment-generating function (mgf) of a random vector \\(\\mathbf{x}\\):\n\\[\nM(\\mathbf{t})=E\\left(e^{\\mathbf{t}^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{x}}\\right).\n\\]\nAs in the univariate case, when it exists, the mgf uniquely characterizes the entire distribution of a random vector, just like the density, cdf, or characteristic function do.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "problems/pset-0.html#problem-2",
    "href": "problems/pset-0.html#problem-2",
    "title": "Problem Set 0",
    "section": "Problem 2",
    "text": "Problem 2\nRecall that if \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are jointly distributed according to\n\\[\n\\begin{bmatrix}\n\\mathbf{x} \\\\\n\\mathbf{y}\n\\end{bmatrix}\n\\sim\n\\text{N}_{n+m}\n\\left(\n\\begin{bmatrix}\n\\boldsymbol{\\mu}_x \\\\\n\\boldsymbol{\\mu}_y\n\\end{bmatrix}\n,\\,\n\\begin{bmatrix}\n\\boldsymbol{\\Sigma}_x & \\boldsymbol{\\Sigma}_{xy}\\\\\n\\boldsymbol{\\Sigma}_{xy}^{\\scriptscriptstyle\\mathsf{T}} & \\boldsymbol{\\Sigma}_y\n\\end{bmatrix}\n\\right),\n\\]\nthen the conditional distribution is\n\\[\n\\mathbf{y} \\,|\\,\\mathbf{x}\n\\sim\n\\text{N}_m\n\\left(\n\\boldsymbol{\\mu}_y + \\boldsymbol{\\Sigma}_{xy}^{\\scriptscriptstyle\\mathsf{T}}\\boldsymbol{\\Sigma}_x^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_x)\n,\\,\n\\boldsymbol{\\Sigma}_y - \\boldsymbol{\\Sigma}_{xy}^{\\scriptscriptstyle\\mathsf{T}}\n\\boldsymbol{\\Sigma}_x^{-1}\n\\boldsymbol{\\Sigma}_{xy}\n\\right).\n\\]\nWe will use this fact often, but proving it is tedious. I’m not going to make you do it. This problem set is unreasonable enough already. Instead, I want you to read the proof very carefully. If you’re like me, there is some step in the derivation that will make you say “what just happened?” Drill down on that. Pick a section of the proof where you get confused, or where a technique is being used that you are unfamiliar with. Study that part carefully until you understand it, and then in your problem set write-up, explain it to me like I’m an idiot (shouldn’t be too difficult). Use pictures, numerical examples, specialize to the bivariate case, whatever you need to do.\n\n\n\n\n\n\nDon’t be a silly goose.\n\n\n\nWe are grading this part of the problem set for completion, but give it a good faith effort so that you actually, like, learn something.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "problems/pset-0.html#problem-3",
    "href": "problems/pset-0.html#problem-3",
    "title": "Problem Set 0",
    "section": "Problem 3",
    "text": "Problem 3\nConsider these data\n\\[\nX_1,\\,X_2,\\,...,\\,X_n\\overset{\\text{iid}}{\\sim}\\text{N}(\\theta,\\,1)\n\\]\nand the usual estimator \\(\\hat{\\theta}_n=\\sum\\limits_{i=1}^nX_i/n\\).\n\nDerive an exact \\(100\\times (1-\\alpha)\\%\\) classical confidence interval for the unknown mean \\(\\theta\\). That is, find a random interval \\((L_n,\\,U_n)\\) satisfying:\n\n\\[\nP\\left(L_n&lt;\\theta&lt;U_n\\right)=1-\\alpha.\n\\]\n\nDerive an exact \\(100\\times (1-\\alpha)\\%\\) classical prediction interval for a new observation \\(X_{n+1}\\). That is, find a random interval \\((L_n',\\,U_n')\\) satisfying\n\n\\[\nP\\left(L_n'&lt;X_{n+1}&lt;U_n'\\right)=1-\\alpha.\n\\]\n\nComment on the difference between the two intervals.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "problems/pset-0.html#problem-4",
    "href": "problems/pset-0.html#problem-4",
    "title": "Problem Set 0",
    "section": "Problem 4",
    "text": "Problem 4\nHere is a cdf indexed by a parameter \\(\\theta&gt;0\\):\n\\[\nF(x;\\,\\theta)\n=\n1 - \\frac{1}{(1+x)^\\theta},\\quad x\\geq 0.\n\\]\n\nDescribe an algorithm that will simulate (pseudo)random numbers from this distribution;\nLet \\(x_1,\\,x_2,\\,...,\\,x_n\\overset{\\text{iid}}{\\sim}F(x;\\,\\theta)\\) and derive the maximum likelihood estimator \\(\\hat{\\theta}_n^{(\\text{MLE})}\\) for the parameter \\(\\theta\\);\nDerive the exact sampling distribution of \\(\\hat{\\theta}_n^{(\\text{MLE})}\\);\nImplement the bootstrap to simulate the sampling distribution of \\(\\hat{\\theta}_n^{(\\text{MLE})}\\), and compare the bootstrap distribution to the exact one you derived in part c. In order to do this, you will probably need your algorithm from part a.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nHere is a schematic of how the bootstrap works. We start with some ground truth parameter value \\(\\theta_0&gt;0\\), and then implement this:\n\\[\n\\begin{matrix}\n\\text{0. True distribution} &&& F_{\\theta_0} && \\\\\n&&& \\downarrow && \\\\\n\\text{1. Original data} &&& x_{1:n} && \\\\\n&&& \\downarrow && \\\\\n\\text{2. Empirical distribution} &&& \\hat{F}_{n} && \\\\\n&\\swarrow &\\swarrow& \\cdots &\\searrow&\\searrow \\\\\n\\text{3. Synthetic data}&\\tilde{x}_{1:n}^{(1)} &\\tilde{x}_{1:n}^{(2)}& \\cdots &\\tilde{x}_{1:n}^{(k-1)}&\\tilde{x}_{1:n}^{(k)} \\\\\n&\\downarrow &\\downarrow& \\cdots &\\downarrow&\\downarrow \\\\\n\\text{4. Bootstrap estimates}&\\hat{\\theta}_n^{(1)} &\\hat{\\theta}_n^{(2)}& \\cdots &\\hat{\\theta}_n^{(k-1)}&\\hat{\\theta}_n^{(k)} \\\\\n\\end{matrix}\n\\]\nIf you implemented this correctly, and if the original sample size \\(n\\) is “large enough,” then a histogram of the \\(k\\) estimates \\(\\hat{\\theta}_n^{(1)}\\), \\(\\hat{\\theta}_n^{(2)}\\), …, \\(\\hat{\\theta}_n^{(k)}\\) ought to resemble the exact density you derived in part c.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "problems/pset-0.html#problem-5",
    "href": "problems/pset-0.html#problem-5",
    "title": "Problem Set 0",
    "section": "Problem 5",
    "text": "Problem 5\nRecall the parametric family from the previous part, and consider the Bayesian model\n\\[\n\\begin{aligned}\n\\theta &\\sim \\text{Gamma}(a_0,\\,b_0) \\\\\nx_i\\,|\\, \\theta &\\overset{\\text{iid}}{\\sim }F(x;\\,\\theta).\n\\end{aligned}\n\\]\n\nDerive the posterior distribution \\(p(\\theta\\,|\\,x_{1:n})\\);\nShow that the posterior mean \\(E(\\theta\\,|\\,x_{1:n})\\) can be written as a convex combination of the prior mean and the MLE;\nDerive the posterior predictive distribution \\(p(x_{n+1}\\,|\\,x_{1:n})\\).",
    "crumbs": [
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "problems/pset-0.html#problem-6",
    "href": "problems/pset-0.html#problem-6",
    "title": "Problem Set 0",
    "section": "Problem 6",
    "text": "Problem 6\nAn important consideration in time series analysis is sequential or online inference. In this environment, our data are not available to us all at once in a batch. Instead, the observations are streaming; they are arriving one-after-another in real-time, and we want to come up with a scheme that allows us to recursively update our inferences as new information arrives. Let’s explore this in the context of least squares regression.\n\n\n\n\n\n\nOLS review\n\n\n\nConsider the usual setting where we observe iid pairs \\(y_i\\in\\mathbb{R}\\) and \\(\\mathbf{x}_i\\in\\mathbb{R}^p\\) from the linear model:\n\\[\ny_i= \\mathbf{x}_i^{\\scriptscriptstyle\\mathsf{T}}\\boldsymbol{\\beta}+\\varepsilon_i,\\quad \\varepsilon_i\\overset{\\text{iid}}{\\sim}\\text{N}(0,\\,\\sigma^2).\n\\] Given \\(n\\) observations, we can form the matrices\n\\[\n\\mathbf{y}_n\n=\n\\underbrace{\n\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix}\n}\n_{n\\times 1}\n\\quad\n\\mathbf{X}_n\n=\n\\underbrace{\n\\begin{bmatrix}\n\\mathbf{x}_1^{\\scriptscriptstyle\\mathsf{T}} \\\\\n\\mathbf{x}_2^{\\scriptscriptstyle\\mathsf{T}} \\\\\n\\vdots \\\\\n\\mathbf{x}_n^{\\scriptscriptstyle\\mathsf{T}}\n\\end{bmatrix}\n}_{n\\times p}\n\\quad\n\\boldsymbol{\\varepsilon}_n\n=\n\\underbrace{\n\\begin{bmatrix}\n\\varepsilon_1 \\\\\n\\varepsilon_2 \\\\\n\\vdots \\\\\n\\varepsilon_n\n\\end{bmatrix}\n}\n_{n\\times 1}\n,\n\\] and we can rewrite the model in matrix form:\n\\[\n\\mathbf{y}_n=\\mathbf{X}_n\\boldsymbol{\\beta}+\\boldsymbol{\\varepsilon}_n,\\quad \\boldsymbol{\\varepsilon}_n\\sim\\text{N}_n(\\mathbf{0},\\,\\sigma^2\\mathbf{I}_n).\n\\] The ordinary least squares (OLS) estimator of the regression coefficients is \\(\\hat{\\boldsymbol{\\beta}}_n=(\\mathbf{X}_n^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{X}_n)^{-1}\\mathbf{X}_n^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{y}_n.\\)\n\n\nHopefully that was all review, and it describes how to do batch or offline inference for the linear model. We are assuming that all of our data are available in a single batch, and we only need to compute the estimate once. But in our class, we will often care about streaming or online inference: efficiently computing the sequence of estimates \\(\\hat{\\boldsymbol{\\beta}}_1\\), \\(\\hat{\\boldsymbol{\\beta}}_2\\), \\(\\hat{\\boldsymbol{\\beta}}_3\\), … as the pairs \\((\\mathbf{x}_1,\\,y_1)\\), \\((\\mathbf{x}_2,\\,y_2)\\), \\((\\mathbf{x}_3,\\,y_3)\\), … arrive one-after-another in real-time.\n\nSo, imagine we have observed \\(n-1\\) pairs, and we have computed the estimate \\(\\hat{\\boldsymbol{\\beta}}_{n-1}=(\\mathbf{X}_{n-1}^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{X}_{n-1})^{-1}\\mathbf{X}_{n-1}^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{y}_{n-1}.\\) Then, a new observation \\((\\mathbf{x}_n,\\,y_n)\\) arrives. How can we efficiently update our estimate of \\(\\boldsymbol{\\beta}\\) to incorporate this new information? In other words, how can we quickly compute \\(\\hat{\\boldsymbol{\\beta}}_{n}\\) given only \\(\\hat{\\boldsymbol{\\beta}}_{n-1}\\) and the new \\((\\mathbf{x}_n,\\,y_n)\\)? Of course, we could always just add new rows to \\(\\mathbf{X}_{n-1}\\) and \\(\\mathbf{y}_{n-1}\\) and recompute the whole estimate from scratch, but that is super inefficient and we can do better. To that end, show that the new estimate is related to the old estimate and the new data by the following recursion: \\(\\hat{\\boldsymbol{\\beta}}_n=\\hat{\\boldsymbol{\\beta}}_{n-1}+\\mathbf{k}_n(y_n-\\mathbf{x}_n^{\\scriptscriptstyle\\mathsf{T}}\\hat{\\boldsymbol{\\beta}}_{n-1})\\). What is \\(\\mathbf{k}_n\\)?\nInterpret the recursion you derived in part a. Isn’t \\((y_n-\\mathbf{x}_n^{\\scriptscriptstyle\\mathsf{T}}\\hat{\\boldsymbol{\\beta}}_{n-1})\\) a residual? What is this \\(\\mathbf{k}_n\\) thing doing? What would it mean if we had \\(\\hat{\\boldsymbol{\\beta}}_n=\\hat{\\boldsymbol{\\beta}}_{n-1}\\)?\nWrite a for loop in R that uses your recursion to process a dataset one observation at a time, and verify that at the end of the loop, you get the same estimates that you would have gotten if you had just used lm. Do this for the mtcars dataset and the regression mpg ~ wt.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe Sherman–Morrison formula is a great tool! For invertible \\(\\mathbf{A}\\in\\mathbb{R}^{p\\times p}\\) and \\(\\mathbf{v}\\in\\mathbb{R}^p\\), we have\n\\[\n(\\mathbf{A}+\\mathbf{v}\\mathbf{v}^{\\scriptscriptstyle\\mathsf{T}})^{-1}\n=\n\\mathbf{A}^{-1}\n-\n\\frac{\n\\mathbf{A}^{-1}\n\\mathbf{v}\\mathbf{v}^{\\scriptscriptstyle\\mathsf{T}}\n\\mathbf{A}^{-1}\n}{\n1\n+\n\\mathbf{v}^{\\scriptscriptstyle\\mathsf{T}}\n\\mathbf{A}^{-1}\n\\mathbf{v}\n}\n.\n\\]",
    "crumbs": [
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "problems/pset-0.html#submission",
    "href": "problems/pset-0.html#submission",
    "title": "Problem Set 0",
    "section": "Submission",
    "text": "Submission\nYou are free to compose your solutions for this problem set however you wish (scan or photograph written work, handwriting capture on a tablet device, LaTeX, Quarto, whatever) as long as the final product is a single PDF file. You must upload this to Gradescope and mark the pages associated with each problem.\nDo not forget to include the following:\n\nFor each problem, please acknowledge your collaborators;\nIf a problem required you to code something, please include both the code and the output.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#point-forecast",
    "href": "slides/04-forecast-evaluation.html#point-forecast",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Point forecast",
    "text": "Point forecast\nYour single-number best guess at tomorrow’s observation:\n\n\n\nAttaching package: 'LaplacesDemon'\n\n\nThe following objects are masked from 'package:lubridate':\n\n    dst, interval\n\n\nThe following object is masked from 'package:purrr':\n\n    partial\n\n\nThe following objects are masked from 'package:extraDistr':\n\n    dbern, dcat, ddirichlet, dgpd, dgpois, dinvchisq, dinvgamma,\n    dlaplace, dpareto, pbern, plaplace, ppareto, qbern, qcat, qlaplace,\n    qpareto, rbern, rcat, rdirichlet, rgpd, rinvchisq, rinvgamma,\n    rlaplace, rpareto"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#forecast-interval",
    "href": "slides/04-forecast-evaluation.html#forecast-interval",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Forecast interval",
    "text": "Forecast interval\nA range of likely values for tomorrow’s observation:"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#forecast-density",
    "href": "slides/04-forecast-evaluation.html#forecast-density",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Forecast density",
    "text": "Forecast density\nFull distribution capturing uncertainty about tomorrow:"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#and-then-tomorrow-finally-comes",
    "href": "slides/04-forecast-evaluation.html#and-then-tomorrow-finally-comes",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "And then tomorrow finally comes",
    "text": "And then tomorrow finally comes\nSo…how’d we do?"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#whats-the-point",
    "href": "slides/04-forecast-evaluation.html#whats-the-point",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "What’s the point?",
    "text": "What’s the point?\n\nWe want intervals and densities to communicate uncertainty about the forecast;\n\nWhat sources of uncertainty?\n\nBasic data uncertainty;\nParameter estimation uncertainty;\nHyperparameter uncertainty;\nModel uncertainty;\nUncertainty introduced by missing data.\n\n\nIn the small world of the AR(1), mainly the first two for now."
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#how-do-you-get-full-predictive-distributions",
    "href": "slides/04-forecast-evaluation.html#how-do-you-get-full-predictive-distributions",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "How do you get full predictive distributions?",
    "text": "How do you get full predictive distributions?\n\nIn general, use simulation:\n\nClassical approach: bootstrapping;\nBayesian approach: posterior predictive simulation.\n\n\n\nEither way, you get Monte Carlo draws from a forecast distribution:\n\\[\n\\tilde{y}_{t+h}^{(1)}\\com \\tilde{y}_{t+h}^{(2)}\\com ...\\com \\tilde{y}_{t+h}^{(k)}\n\\sim \\hat{F}_{t+h|t}.\n\\]\n\n\nWhat do you do with them?"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#probabilistic-forecasting-via-monte-carlo",
    "href": "slides/04-forecast-evaluation.html#probabilistic-forecasting-via-monte-carlo",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Probabilistic forecasting via Monte Carlo",
    "text": "Probabilistic forecasting via Monte Carlo\nUse the simulations \\(\\tilde{y}_{t+h}^{(1:k)}=\\left\\{\\tilde{y}_{t+h}^{(1)}\\com \\tilde{y}_{t+h}^{(2)}\\com ...\\com \\tilde{y}_{t+h}^{(k)}\\right\\}\\) to construct whatever forecast object you want:\n\n\\[\n\\hat{y}_{t+h|t}=\\frac{1}{k}\\sum\\limits_{j=1}^k\\tilde{y}_{t+h}^{(j)}. \\quad (\\text{...or median})\n\\]\n\n\nForecast interval:\n\\[\n\\hat{I}_{t+h|t} = \\left[\\hat{Q}_{\\frac{\\alpha}{2}}\\left(\\tilde{y}_{t+h}^{(1:k)}\\right)\\com \\hat{Q}_{1-\\frac{\\alpha}{2}}\\left(\\tilde{y}_{t+h}^{(1:k)}\\right)\\right]. \\quad (...\\text{or hdi})\n\\]\n\n\nForecast distribution:\n\\[\n\\hat{f}_{t+h|t}\n=\n\\text{histogram}\\left(\\tilde{y}_{t+h}^{(1:k)}\\right). \\quad (...\\text{or kde})\n\\]"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#well-behaved-case-iid-normal",
    "href": "slides/04-forecast-evaluation.html#well-behaved-case-iid-normal",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Well-behaved case: iid normal",
    "text": "Well-behaved case: iid normal\n\nAssume\n\n\n\\[\ny_1\\com y_2\\com ...\\com y_n\\com y_{n+1}\\iid\\N(\\mu\\com\\sigma^2).\n\\]\n\n\nWe know\n\\[\n\\frac{\\bar{y}_{n}-y_{n+1}}{\\sigma\\sqrt{1+\\frac{1}{n}}}\\sim\\N(0\\com 1),\n\\]\n\n\nand so plugging in sample standard deviation \\(s_n\\) gives\n\\[\n\\frac{\\bar{y}_{n}-y_{n+1}}{s_n\\sqrt{1+\\frac{1}{n}}}\\sim t_{n-1}.\n\\]\n\n\nThe predictive distribution is non-standard Student’s \\(t\\):\n\\[\n\\hat{y}_{n+1}\\sim t\\left(n-1\\com \\bar{y}_n\\com s_n^2\\left(1+\\frac{1}{n}\\right)\\right).\n\\]"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#well-behaved-case-bayes-with-conjugate-prior",
    "href": "slides/04-forecast-evaluation.html#well-behaved-case-bayes-with-conjugate-prior",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Well-behaved case: Bayes with conjugate prior",
    "text": "Well-behaved case: Bayes with conjugate prior\nA conjugate normal-inverse-gamma prior begets a conjugate posterior:\n\\[\n\\begin{aligned}\n\\sigma^2\\given y_{0:t}\n&\\sim\n\\text{IG}(a_t\\com b_t)\n\\\\\n\\Bbeta\\given \\sigma^2\\com y_{0:t}\n&\\sim\n\\text{N}_2(\\Bm_t\\com\\sigma^2\\BH^{-1}_t)\n\\\\\ny_{t+1}\\given\\Bbeta\\com \\sigma^2\\com y_{0:t}\n&\\sim \\N(\\Bx_{t+1}^\\tr\\Bbeta\\com\\sigma^2).\n\\end{aligned}\n\\]\n\nThe one-step posterior predictive distribution is non-standard Student’s \\(t\\):\n\\[\n\\begin{aligned}\ny_{t+1}\\given y_{0:t}\n&\\sim\nt(\\nu_{t+1|t}\\com\\bar{y}_{t+1|t}\\com s_{t+1|t}^2)\n\\\\\n\\\\\n\\nu_{t+1|t}\n&=\n2a_t\n\\\\\n\\bar{y}_{t+1|t}\n&=\n\\Bx_{t+1}^\\tr\\Bm_t\n\\\\\ns_{t+1|t}^2\n&=\n\\frac{b_t}{a_t}\n(1+\\Bx_{t+1}^\\tr\\BH_t^{-1}\\Bx_{t+1})\n.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#how-do-you-evaluate-the-forecasts",
    "href": "slides/04-forecast-evaluation.html#how-do-you-evaluate-the-forecasts",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "How do you evaluate the forecasts?",
    "text": "How do you evaluate the forecasts?\nYou generate a sequence of one-step-ahead predictions:\n\\[\n\\begin{matrix}\n\\hat{y}_{1|0} & \\hat{y}_{2|1} & \\hat{y}_{3|2} & \\hat{y}_{4|3} & \\hat{y}_{5|4} & \\cdots&\\hat{y}_{t|t-1} & \\cdots\\\\\n\\hat{I}_{1|0} & \\hat{I}_{2|1} & \\hat{I}_{3|2} & \\hat{I}_{4|3} & \\hat{I}_{5|4} & \\cdots&\\hat{I}_{t|t-1} & \\cdots\\\\\n\\hat{f}_{1|0} & \\hat{f}_{2|1} & \\hat{f}_{3|2} & \\hat{f}_{4|3} & \\hat{f}_{5|4} & \\cdots&\\hat{f}_{t|t-1} & \\cdots\n\\end{matrix}\n\\]\n\nBut then the data you were trying to forecast eventually arrive:\n\\[\n\\begin{matrix}\ny_1 & y_2 & y_3 & y_4 & y_5 & \\cdots &y_t & \\cdots\n\\end{matrix}\n\\]\nHow do we score the forecasts and summarize?"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#todays-agenda",
    "href": "slides/04-forecast-evaluation.html#todays-agenda",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nWe will learn how to evaluate probabilistic predictions;\nWe will illustrate by comparing the performance of the two well-behaved methods:\n\nA. classical predictive distribution from iid normal model;\nB. posterior predictive distribution from Gaussian AR(1) with conjugate prior."
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#there-will-be-two-running-examples",
    "href": "slides/04-forecast-evaluation.html#there-will-be-two-running-examples",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "There will be two running examples",
    "text": "There will be two running examples\n\n\nSimulated data from AR(1)\n\nMethod A (iid normal) is wrong by construction;\nMethod B is right by construction;\n\n\n\nApple’s daily stock price from 2000 - this week\n\nboth methods are “wrong,” but is one strictly preferred?\n\n\n\n\nOur forecast metrics will tease all of that out."
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#dataset-1-simulated",
    "href": "slides/04-forecast-evaluation.html#dataset-1-simulated",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Dataset 1: simulated",
    "text": "Dataset 1: simulated"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#forecast-distributions-from-iid-normal-model",
    "href": "slides/04-forecast-evaluation.html#forecast-distributions-from-iid-normal-model",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Forecast distributions from iid normal model",
    "text": "Forecast distributions from iid normal model"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#forecast-distributions-from-bayesian-ar1",
    "href": "slides/04-forecast-evaluation.html#forecast-distributions-from-bayesian-ar1",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Forecast distributions from Bayesian AR(1)",
    "text": "Forecast distributions from Bayesian AR(1)"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#dataset-2-apple-stock-price",
    "href": "slides/04-forecast-evaluation.html#dataset-2-apple-stock-price",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Dataset 2: Apple stock price",
    "text": "Dataset 2: Apple stock price\n\nlibrary(quantmod)\ngetSymbols(\"AAPL\", from = \"2000-01-01\", to = \"2025-09-08\", src = \"yahoo\")\n\n[1] \"AAPL\""
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#forecast-distributions-from-iid-normal-model-1",
    "href": "slides/04-forecast-evaluation.html#forecast-distributions-from-iid-normal-model-1",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Forecast distributions from iid normal model",
    "text": "Forecast distributions from iid normal model"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#forecast-distributions-from-bayesian-ar1-1",
    "href": "slides/04-forecast-evaluation.html#forecast-distributions-from-bayesian-ar1-1",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Forecast distributions from Bayesian AR(1)",
    "text": "Forecast distributions from Bayesian AR(1)"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#any-ideas",
    "href": "slides/04-forecast-evaluation.html#any-ideas",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Any ideas?",
    "text": "Any ideas?"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#point-prediction",
    "href": "slides/04-forecast-evaluation.html#point-prediction",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Point prediction",
    "text": "Point prediction\nWe want the point prediction that minimizes expected loss:\n\n\\[\n\\hat{y}_{t+1|t}\n\\;=\\;\n\\argmin{\\hat{y}\\in\\mathbb{R}}\n\\; E\\big[\\, L\\big(y_{t+1},\\,\\hat{y}\\big) \\,\\big|\\, y_{0:t} \\big].\n\\]\n\n\nThe expectation is taken with respect to the “true” or “idealized” conditional distribution \\(p(y_{t+1}\\given y_{0:t})\\), which we don’t know.\n\n\nWe approximate it with whatever forecast distribution we’ve generated."
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#picking-a-loss-function",
    "href": "slides/04-forecast-evaluation.html#picking-a-loss-function",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Picking a loss function",
    "text": "Picking a loss function\n\nWe have nice results for some loss functions:\n\n\n\\[\n\\begin{array}{rcl}\nL(y_{t+1},\\hat{y}) = (y_{t+1} - \\hat{y})^2\n& \\implies &\n\\hat{y}_{t+1|t} = E[\\,y_{t+1}\\mid y_{0:t}\\,] \\\\[1.2em]\nL(y_{t+1},\\hat{y}) = |y_{t+1} - \\hat{y}|\n& \\implies &\n\\hat{y}_{t+1|t} = \\operatorname{median}(y_{t+1}\\mid y_{0:t}).\n\\end{array}\n\\]\n\n\nAnd there are many more where that came from."
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#in-practice",
    "href": "slides/04-forecast-evaluation.html#in-practice",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "In practice",
    "text": "In practice\nMetrics for scoring the average quality of the point predictions over time:\n\\[\n\\begin{aligned}\n\\text{MSFE}\n&=\n\\frac{1}{T}\n\\sum\\limits_{t=1}^T\n(y_t-\\hat{y}_{t|t-1})^2\n\\\\\n\\text{MAFE}\n&=\n\\frac{1}{T}\n\\sum\\limits_{t=1}^T\n|y_t-\\hat{y}_{t|t-1}|.\n\\end{aligned}\n\\]\nWe want these to be small.\n\n\n\n\n\n\n\nMake sure your loss function and your point prediction play nice\n\n\n\nIf you’re looking at MAFE, use forecast median;\nIf you’re looking at MSFE, use the forecast mean."
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#our-simulated-data",
    "href": "slides/04-forecast-evaluation.html#our-simulated-data",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Our simulated data",
    "text": "Our simulated data\nMSE of forecast mean:\n\nmean((y - pred_params_iid_sim[,\"location\"])^2, na.rm = TRUE)\n\n[1] 162.4966\n\nmean((y - pred_params_ar1_sim[,\"location\"])^2, na.rm = TRUE)\n\n[1] 4.162699\n\n\n\nMAE of forecast median (same as mean for these methods):\n\nmean(abs(y - pred_params_iid_sim[,\"location\"]), na.rm = TRUE)\n\n[1] 10.03766\n\nmean(abs(y - pred_params_ar1_sim[,\"location\"]), na.rm = TRUE)\n\n[1] 1.637622"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#our-real-data",
    "href": "slides/04-forecast-evaluation.html#our-real-data",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Our real data",
    "text": "Our real data\nMSE of forecast mean:\n\nmean((stonks - pred_params_iid_real[,\"location\"])^2, na.rm = TRUE)\n\n[1] 4346.895\n\nmean((stonks - pred_params_ar1_real[,\"location\"])^2, na.rm = TRUE)\n\n[1] 2.111325\n\n\n\nMAE of forecast median (same as mean for these methods):\n\nmean(abs(stonks - pred_params_iid_real[,\"location\"]), na.rm = TRUE)\n\n[1] 37.60877\n\nmean(abs(stonks - pred_params_ar1_real[,\"location\"]), na.rm = TRUE)\n\n[1] 0.612237"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#any-ideas-1",
    "href": "slides/04-forecast-evaluation.html#any-ideas-1",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Any ideas?",
    "text": "Any ideas?"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#interval-width-and-coverage",
    "href": "slides/04-forecast-evaluation.html#interval-width-and-coverage",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Interval width and coverage",
    "text": "Interval width and coverage\n\nYou want intervals that are small enough to be informative, but large enough to swallow the truth often, and there’s a trade-off.\n\n\\(\\hat{I}=(-\\infty\\com \\infty)\\) has perfect coverage but teaches you nothing;\nLook at average size and empirical coverage:\n\n\n\\[\n\\begin{aligned}\n\\overline{\\text{Size}}\n&= \\frac{1}{T} \\sum_{t=1}^{T} \\Big( \\hat{I}_{t\\mid t-1}^{\\text{upper}} - \\hat{I}_{t\\mid t-1}^{\\text{lower}} \\Big), \\\\[0.8em]\n\\overline{\\text{Coverage}}\n&= \\frac{1}{T} \\sum_{t=1}^{T} \\mathbf{1}\\Big\\{ y_t \\in \\hat{I}_{t\\mid t-1} \\Big\\}.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#interval-performance-on-simulated-data",
    "href": "slides/04-forecast-evaluation.html#interval-performance-on-simulated-data",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Interval performance on simulated data",
    "text": "Interval performance on simulated data\n\nSize of 90% intervals:\n\nmean(PI_iid_sim[,1], na.rm = TRUE)   \n\n[1] 37.61362\n\nmean(PI_ar1_sim[,1], na.rm = TRUE)   \n\n[1] 6.660308\n\n\n\n\nCoverage of 90% intervals:\n\nmean(PI_iid_sim[,2], na.rm = TRUE)   \n\n[1] 0.8537415\n\nmean(PI_ar1_sim[,2], na.rm = TRUE)   \n\n[1] 0.8991798"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#interval-performance-on-stock-price-data",
    "href": "slides/04-forecast-evaluation.html#interval-performance-on-stock-price-data",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Interval performance on stock price data",
    "text": "Interval performance on stock price data\n\nSize of 90% intervals:\n\nmean(PI_iid_real[,1], na.rm = TRUE)   \n\n[1] 41.01969\n\nmean(PI_ar1_real[,1], na.rm = TRUE)   \n\n[1] 1.03624\n\n\n\n\nCoverage of 90% intervals:\n\nmean(PI_iid_real[,2], na.rm = TRUE)   \n\n[1] 0.2190211\n\nmean(PI_ar1_real[,2], na.rm = TRUE)   \n\n[1] 0.7071395"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#interval-score",
    "href": "slides/04-forecast-evaluation.html#interval-score",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Interval score",
    "text": "Interval score\nAverage over time for an holistic metric of interval performance:\n\\[\n\\mathrm{IS}_\\alpha(l,u; y)\n=\n(u - l)\n+\n\\frac{2}{\\alpha}\\,(l - y)\\,\\mathbf{1}(y &lt; l)\n+\n\\frac{2}{\\alpha}\\,(y - u)\\,\\mathbf{1}(y &gt; u).\n\\]\nSynthesizes both size and coverage, but in practice, if you want to understand why the score was good or bad, you have to crack it open and look at the size and coverage components separately anyway."
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#any-ideas-2",
    "href": "slides/04-forecast-evaluation.html#any-ideas-2",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Any ideas?",
    "text": "Any ideas?"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#probability-integral-transform-pit",
    "href": "slides/04-forecast-evaluation.html#probability-integral-transform-pit",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Probability integral transform (PIT)",
    "text": "Probability integral transform (PIT)\nLet \\(G_t\\) be the “true” cdf that nature is drawing from to produce \\(y_t\\). By the probability integral transform, we know that:\n\n\\[\nG_1(y_1)\\com G_2(y_2)\\com ...\\com G_t(y_t)\\com ...\\sim\\text{Unif}(0\\com 1).\n\\]\n\n\nIt would be ideal if \\(\\hat{F}_{t|t-1}=G_t\\). We’re probably not so lucky, but if we’re close, then we should see:\n\n\n\\[\n\\hat{F}_{1|0}(y_1)\\com \\hat{F}_{2|1}(y_2)\\com ...\\com \\hat{F}_{t|t-1}(y_t)\\com ...\\sim\\text{Unif}(0\\com 1).\n\\]\n\n\nLet’s check!"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#iid-normal-method-on-simulated-data",
    "href": "slides/04-forecast-evaluation.html#iid-normal-method-on-simulated-data",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "iid normal method on simulated data",
    "text": "iid normal method on simulated data"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#bayesian-ar1-on-simulated-data",
    "href": "slides/04-forecast-evaluation.html#bayesian-ar1-on-simulated-data",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Bayesian AR(1) on simulated data",
    "text": "Bayesian AR(1) on simulated data"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#iid-normal-method-on-stock-price-data",
    "href": "slides/04-forecast-evaluation.html#iid-normal-method-on-stock-price-data",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "iid normal method on stock price data",
    "text": "iid normal method on stock price data\n\n\n\n\n\n\n\n\nWay too many surprises in the right tail (recall waterfall)."
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#bayesian-ar1-on-stock-price-data",
    "href": "slides/04-forecast-evaluation.html#bayesian-ar1-on-stock-price-data",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Bayesian AR(1) on stock price data",
    "text": "Bayesian AR(1) on stock price data"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#diagnosing-underover-dispersion",
    "href": "slides/04-forecast-evaluation.html#diagnosing-underover-dispersion",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Diagnosing under/over-dispersion",
    "text": "Diagnosing under/over-dispersion"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#summary-calibration",
    "href": "slides/04-forecast-evaluation.html#summary-calibration",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Summary: calibration",
    "text": "Summary: calibration\nIf your sequence of forecast distributions is well-calibrated, then the PITs should be approximately uniformly distributed:\n\\[\n\\hat{F}_{1|0}(y_1)\\com \\hat{F}_{2|1}(y_2)\\com ...\\com \\hat{F}_{t|t-1}(y_t)\\com ...\\sim\\text{Unif}(0\\com 1).\n\\]\n\nCheck it with a histogram, QQ-plot, goodness-of-fit test…\n\n\n\n\n\n\n\n\nThis is necessary but not sufficient!\n\n\nCalibration alone is not enough to distinguish good/better/best forecasts."
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#maximize-sharpness-subject-to-calibration",
    "href": "slides/04-forecast-evaluation.html#maximize-sharpness-subject-to-calibration",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Maximize sharpness subject to calibration",
    "text": "Maximize sharpness subject to calibration\n\nYou want a forecasting method to be calibrated;\nIf you have many methods to choose from, all of which appear calibrated, select the one that is the sharpest;\n\nSharpness refers to how concentrated the forecast distributions are. Among calibrated distributions, you want the one that is sharpest, most decisive, most concentrated;\nSharpness can be measured by your preferred measure of spread: variance, IQR, etc."
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#sharpness-for-the-simulated-data",
    "href": "slides/04-forecast-evaluation.html#sharpness-for-the-simulated-data",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Sharpness for the simulated data",
    "text": "Sharpness for the simulated data\nCompare the scale parameters of the predictive distributions:"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#sharpness-for-the-stock-price-data",
    "href": "slides/04-forecast-evaluation.html#sharpness-for-the-stock-price-data",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Sharpness for the stock price data",
    "text": "Sharpness for the stock price data"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#log-predictive-score",
    "href": "slides/04-forecast-evaluation.html#log-predictive-score",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Log predictive score",
    "text": "Log predictive score\nEvaluates if the forecast distribution placed high mass/density on the region where \\(y_t\\) actually showed up:\n\\[\n\\overline{\\text{LPS}} = \\frac{1}{T} \\sum_{t=1}^{T} \\ln \\hat{f}_{t|t-1}(y_t).\n\\]\n\nBigger is better;\nRewards both calibration and sharpness;\n\nProper scoring rule: encourages honest probabilistic predictions;\nLocal measure of quality. We will see global measures like the continuous ranked probability score (CRPS) later."
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#for-our-examples",
    "href": "slides/04-forecast-evaluation.html#for-our-examples",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "For our examples",
    "text": "For our examples\n\nSimulated data:\n\naverage_log_score(y, pred_params_iid_sim)\n\n[1] -3.965963\n\naverage_log_score(y, pred_params_ar1_sim)\n\n[1] -2.132817\n\n\n\n\nReal data:\n\naverage_log_score(stonks, pred_params_iid_real)\n\n[1] -5.609798\n\naverage_log_score(stonks, pred_params_ar1_real)\n\n[1] -1.79456"
  },
  {
    "objectID": "slides/00-welcome.html#teaching-team",
    "href": "slides/00-welcome.html#teaching-team",
    "title": "Welcome to STA 542!",
    "section": "Teaching team",
    "text": "Teaching team\n\n\n\n\n\n\n\n\nMug\nName\nRole\nOffice Hours\n\n\n\n\nLi, Aihua\nTA\nWed 10:30AM - 12:30PM\n\n\n\nZito, John\nInstructor\nTue 1PM - 3PM"
  },
  {
    "objectID": "slides/00-welcome.html#motivation",
    "href": "slides/00-welcome.html#motivation",
    "title": "Welcome to STA 542!",
    "section": "Motivation",
    "text": "Motivation\n\n\n\n\n\n\n\nControversial statement\n\n\nStatistics is about quantifying uncertainty to help make decisions.\n\n\n\n\n\nIn modern data science environments, stakeholders are sequentially analyzing high volumes of dependent data and using it to forecast the future in real-time.\n\n\nLet’s see that in action."
  },
  {
    "objectID": "slides/00-welcome.html#central-banking",
    "href": "slides/00-welcome.html#central-banking",
    "title": "Welcome to STA 542!",
    "section": "Central banking",
    "text": "Central banking"
  },
  {
    "objectID": "slides/00-welcome.html#e-commerce-and-web-traffic",
    "href": "slides/00-welcome.html#e-commerce-and-web-traffic",
    "title": "Welcome to STA 542!",
    "section": "E-commerce and web traffic",
    "text": "E-commerce and web traffic\namazon.com is never “offline.”"
  },
  {
    "objectID": "slides/00-welcome.html#extreme-weather-forecasting",
    "href": "slides/00-welcome.html#extreme-weather-forecasting",
    "title": "Welcome to STA 542!",
    "section": "Extreme weather forecasting",
    "text": "Extreme weather forecasting"
  },
  {
    "objectID": "slides/00-welcome.html#monitoring-disease-case-counts",
    "href": "slides/00-welcome.html#monitoring-disease-case-counts",
    "title": "Welcome to STA 542!",
    "section": "Monitoring disease case counts",
    "text": "Monitoring disease case counts"
  },
  {
    "objectID": "slides/00-welcome.html#monitoring-the-energy-grid",
    "href": "slides/00-welcome.html#monitoring-the-energy-grid",
    "title": "Welcome to STA 542!",
    "section": "Monitoring the energy grid",
    "text": "Monitoring the energy grid"
  },
  {
    "objectID": "slides/00-welcome.html#object-tracking",
    "href": "slides/00-welcome.html#object-tracking",
    "title": "Welcome to STA 542!",
    "section": "Object tracking",
    "text": "Object tracking\n\n\n\n\n\nAny ethical complaints about this one?"
  },
  {
    "objectID": "slides/00-welcome.html#autonomous-vehicle-navigation",
    "href": "slides/00-welcome.html#autonomous-vehicle-navigation",
    "title": "Welcome to STA 542!",
    "section": "Autonomous vehicle navigation",
    "text": "Autonomous vehicle navigation"
  },
  {
    "objectID": "slides/00-welcome.html#lets-take-a-second-look-at-this",
    "href": "slides/00-welcome.html#lets-take-a-second-look-at-this",
    "title": "Welcome to STA 542!",
    "section": "Let’s take a second look at this",
    "text": "Let’s take a second look at this\n\n\n\n\n\nSequential inference and probabilistic prediction!"
  },
  {
    "objectID": "slides/00-welcome.html#point-forecast",
    "href": "slides/00-welcome.html#point-forecast",
    "title": "Welcome to STA 542!",
    "section": "Point forecast",
    "text": "Point forecast\nYour single-number best guess at tomorrow’s observation:"
  },
  {
    "objectID": "slides/00-welcome.html#forecast-interval",
    "href": "slides/00-welcome.html#forecast-interval",
    "title": "Welcome to STA 542!",
    "section": "Forecast interval",
    "text": "Forecast interval\nA range of likely values for tomorrow’s observation:"
  },
  {
    "objectID": "slides/00-welcome.html#forecast-density",
    "href": "slides/00-welcome.html#forecast-density",
    "title": "Welcome to STA 542!",
    "section": "Forecast density",
    "text": "Forecast density\nFull distribution capturing uncertainty about tomorrow:"
  },
  {
    "objectID": "slides/00-welcome.html#and-then-tomorrow-finally-comes",
    "href": "slides/00-welcome.html#and-then-tomorrow-finally-comes",
    "title": "Welcome to STA 542!",
    "section": "And then tomorrow finally comes",
    "text": "And then tomorrow finally comes\nSo…how’d we do?"
  },
  {
    "objectID": "slides/00-welcome.html#the-main-themes-of-the-course",
    "href": "slides/00-welcome.html#the-main-themes-of-the-course",
    "title": "Welcome to STA 542!",
    "section": "The main themes of the course",
    "text": "The main themes of the course\nWe will focus on the following:\n\nInference should be sequential. We want recursive estimation techniques to handle data that are streaming in real-time;\nPredictions should be probabilistic. We want point, interval, and density forecasts that incorporate many sources of uncertainty;\nIf you can manipulate joint distributions, you can do time series analysis. Too many TS textbooks quickly lose sight of this;\n\n\nAnd there is a secret fourth theme:\n\nA Bayesian approach is an excellent way of achieving the goals of sequential inference and probabilistic prediction."
  },
  {
    "objectID": "slides/00-welcome.html#topics-may-include",
    "href": "slides/00-welcome.html#topics-may-include",
    "title": "Welcome to STA 542!",
    "section": "Topics may include",
    "text": "Topics may include\n\nARMA models;\nVector autoregressions (VARs);\nDynamic linear models (DLMs);\nHidden Markov Models (HMMs);\nProbabilistic forecast evaluation;\nNonlinear non-Gaussian state-space models;\nSequential Monte Carlo (AKA particle filtering);\nForecast combination."
  },
  {
    "objectID": "slides/00-welcome.html#bookmark-the-course-page",
    "href": "slides/00-welcome.html#bookmark-the-course-page",
    "title": "Welcome to STA 542!",
    "section": "Bookmark the course page!",
    "text": "Bookmark the course page!\n\n\n\n\n\nhttps://sta542-f25.github.io/"
  },
  {
    "objectID": "slides/00-welcome.html#final-grade-breakdown",
    "href": "slides/00-welcome.html#final-grade-breakdown",
    "title": "Welcome to STA 542!",
    "section": "Final grade breakdown",
    "text": "Final grade breakdown\nYour final course grade will be calculated as follows:\n\n\nCategory\nPercentage\n\n\n\nProblem Sets\n25%\n\n\nExam 1\n25%\n\n\nExam 2\n25%\n\n\nFinal Project\n25%\n\n\n\n\n\n\n\n\n\nNote\n\n\nThe final letter grade will be based on the usual thresholds, and there might be a curve."
  },
  {
    "objectID": "slides/00-welcome.html#course-components",
    "href": "slides/00-welcome.html#course-components",
    "title": "Welcome to STA 542!",
    "section": "Course components",
    "text": "Course components\n\nProblem sets: 6 in total; one due about every 2 weeks;\n\nExams: in-class, with only an 8.5” x 11” note sheet:\n\nWednesday October 8 11:45 AM - 12:00 PM;\nMonday November 24 11:45 AM - 12:00 PM;\n\n\nFinal project: no clue, honestly. Talk to me in October."
  },
  {
    "objectID": "slides/00-welcome.html#late-policy",
    "href": "slides/00-welcome.html#late-policy",
    "title": "Welcome to STA 542!",
    "section": "Late policy",
    "text": "Late policy\nNo late work will be accepted unless you request an extension in advance by e-mailing JZ. All reasonable requests will be entertained, but extensions will not be long."
  },
  {
    "objectID": "slides/00-welcome.html#attendance",
    "href": "slides/00-welcome.html#attendance",
    "title": "Welcome to STA 542!",
    "section": "Attendance",
    "text": "Attendance\nNot required. Live your life."
  },
  {
    "objectID": "slides/00-welcome.html#communication",
    "href": "slides/00-welcome.html#communication",
    "title": "Welcome to STA 542!",
    "section": "Communication",
    "text": "Communication\nIf you wish to ask questions in writing…\n\nPost on Ed: about general course policies and content;\nEmail JZ directly: personal matters.\n\nYou should not really be emailing Aihua directly for any reason."
  },
  {
    "objectID": "slides/00-welcome.html#collaboration",
    "href": "slides/00-welcome.html#collaboration",
    "title": "Welcome to STA 542!",
    "section": "Collaboration",
    "text": "Collaboration\nYou are enthusiastically encouraged to work together on the problem sets. You will learn a lot from each other! Two policies:\n\n✅ Acknowledge your collaborators: “Aloysius, Cybill, and I worked together on this problem;”\n❌ Do not outright share or copy solutions. All submitted work must be your own.\n\nViolation of the second policy is plagiarism. Sharers and recipients alike are referred to the conduct office and receive zeros."
  },
  {
    "objectID": "slides/00-welcome.html#use-of-outside-resources-including-ai",
    "href": "slides/00-welcome.html#use-of-outside-resources-including-ai",
    "title": "Welcome to STA 542!",
    "section": "Use of outside resources, including AI",
    "text": "Use of outside resources, including AI\n\nUsing ChatGPT to complete assignments is like bringing a forklift into the weight room; you will never improve your cognitive fitness that way.\n\n\nIf you find a problem solution online (or prompt an LLM to generate one) and submit it as your own work, that will obviously be considered plagiarism;\nOtherwise, all outside resources are fair game for you to study and get extra practice;\nIf you outsource all of your thinking to a language model, you will probably fail both exams. Good luck!"
  },
  {
    "objectID": "slides/00-welcome.html#what-background-do-you-need",
    "href": "slides/00-welcome.html#what-background-do-you-need",
    "title": "Welcome to STA 542!",
    "section": "What background do you need?",
    "text": "What background do you need?\nI assume you have a working knowledge of…\n\nmatrix algebra;\nOLS regression;\nprobability and math stat at the level of Casella & Berger;\nBayesian statistics at the level of STA 602;\nThe R programming language.\n\n\nProblem Set 0 gives you a workout in all of the above."
  },
  {
    "objectID": "slides/00-welcome.html#time-series",
    "href": "slides/00-welcome.html#time-series",
    "title": "Welcome to STA 542!",
    "section": "Time series",
    "text": "Time series\n\nA time series is a set of measurements collected over time;\nWe model these data as a sequence of dependent random variables:\n\n\\[\n\\mathbf{y}_{0:T} = \\{\\mathbf{y}_0,\\,\\mathbf{y}_1,\\,\\mathbf{y}_2,\\,...,\\,\\mathbf{y}_T\\}.\n\\]\n\nA time series model is “just” their joint probability distribution:\n\n\\[\np(\\mathbf{y}_{0:T}) = p(\\mathbf{y}_0)\\prod_{t=1}^Tp(\\mathbf{y}_t\\,|\\,\\mathbf{y}_{0:t-1}).\n\\]\n\n\n\n\n\n\n\nStay grounded.\n\n\nLike much wisdom, that last bullet is simultaneously vacuous and profound. It tells you everything and it tells you nothing all at once. But don’t let this basic fact get lost in the sea of details."
  },
  {
    "objectID": "slides/00-welcome.html#notation-to-get-used-to",
    "href": "slides/00-welcome.html#notation-to-get-used-to",
    "title": "Welcome to STA 542!",
    "section": "Notation to get used to",
    "text": "Notation to get used to\n\nI will not use uppercase \\(Y_t\\) versus lowercase \\(y_t\\) to distinguish random variables and fixed realizations. It’s all just \\(y_t\\);\nA vector \\(\\mathbf{y}\\in\\RR^n\\) is always an \\(n\\times 1\\) column. The corresponding row vector is \\(\\By^\\tr\\);\nFor integers \\(i&lt;j\\), you will see this shorthand all the time:\n\n\\[\ny_{i:j}\n=\n\\{y_i\\com y_{i+1}\\com y_{i+2}\\com...\\com y_{j-2}\\com y_{j-1}\\com y_{j}\\}\n.\n\\]\n\nThe symbol “\\(p\\)” will be aggressively abused and overloaded to represent any probability distribution, sometimes several in the same line:\n\n\\[\np(\\mathbf{y}_{0:T}) = p(\\mathbf{y}_0)\\prod_{t=1}^Tp(\\mathbf{y}_t\\,|\\,\\mathbf{y}_{0:t-1}).\n\\]"
  },
  {
    "objectID": "slides/00-welcome.html#definition",
    "href": "slides/00-welcome.html#definition",
    "title": "Welcome to STA 542!",
    "section": "Definition",
    "text": "Definition\nA random vector \\(\\mathbf{x}=\\begin{bmatrix}x_1&x_2&\\cdots&x_n\\end{bmatrix}^{\\scriptscriptstyle\\mathsf{T}}\\in\\mathbb{R}^n\\) has the multivariate normal distribution with mean vector \\(\\boldsymbol{\\mu}\\in\\mathbb{R}^n\\) and covariance matrix \\(\\boldsymbol{\\Sigma}\\in\\mathbb{R}^{n\\times n}\\) if its density is\n\n\\[\np(\\mathbf{x})\n=\n\\frac\n{\n\\exp\n\\left(\n-\\frac{1}{2}\n(\\mathbf{x} - \\boldsymbol{\\mu})^{\\scriptscriptstyle\\mathsf{T}}\n\\boldsymbol{\\Sigma}^{-1}\n(\\mathbf{x}-\\boldsymbol{\\mu})\n\\right)\n}\n{\n(2\\pi)^{\\frac{n}{2}}\n|\\boldsymbol{\\Sigma}|^{1/2}\n}\n,\n\\quad\n\\mathbf{x}\n\\in\n\\mathbb{R}^n.\n\\]\n\n\nWe denote this \\(\\mathbf{x}\\sim\\text{N}_n(\\boldsymbol{\\mu},\\,\\boldsymbol{\\Sigma})\\).\n\n\n\n\n\n\n\n\nPlenty of linear algebra coming your way!\n\n\n\nTranspose \\((\\mathbf{x} - \\boldsymbol{\\mu})^{\\scriptscriptstyle\\mathsf{T}}\\);\nInverse \\(\\boldsymbol{\\Sigma}^{-1}\\);\nMatrix multiplication \\((\\mathbf{x} - \\boldsymbol{\\mu})^{\\scriptscriptstyle\\mathsf{T}}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\);\nDeterminant \\(|\\boldsymbol{\\Sigma}|\\);\n\n\\(\\boldsymbol{\\Sigma}\\) is a symmetric and positive definite matrix."
  },
  {
    "objectID": "slides/00-welcome.html#univariate",
    "href": "slides/00-welcome.html#univariate",
    "title": "Welcome to STA 542!",
    "section": "Univariate",
    "text": "Univariate\nIf \\(n=1\\), then we meet an old friend:\n\n\n\n\\[\np(x)=\\frac{\\exp\\left(-\\frac{1}{2}\\frac{(x-\\mu)^2}{\\sigma^2}\\right)}{\\sqrt{2\\pi\\sigma^2}}.\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nSo \\(\\mathbf{x} = [x]\\sim\\text{N}_1([\\mu],\\,[\\sigma^2])\\) is just \\(x\\sim\\text{N}(\\mu,\\,\\sigma^2)\\)."
  },
  {
    "objectID": "slides/00-welcome.html#bivariate-elliptical-contours",
    "href": "slides/00-welcome.html#bivariate-elliptical-contours",
    "title": "Welcome to STA 542!",
    "section": "Bivariate: elliptical contours!",
    "text": "Bivariate: elliptical contours!"
  },
  {
    "objectID": "slides/00-welcome.html#moments",
    "href": "slides/00-welcome.html#moments",
    "title": "Welcome to STA 542!",
    "section": "Moments",
    "text": "Moments\nFirst:\n\\[\n\\boldsymbol{\\mu}\n=\nE(\\mathbf{x})\n=\n\\begin{bmatrix}\n\\mu_1\n&\n\\mu_2\n&\n\\cdots\n&\n\\mu_n\n\\end{bmatrix}^{\\scriptscriptstyle\\mathsf{T}}\n=\n\\begin{bmatrix}\nE(x_1)\n&\nE(x_2)\n&\n\\cdots\n&\nE(x_n)\n\\end{bmatrix}^{\\scriptscriptstyle\\mathsf{T}}\n.\n\\]\n\nSecond:\n\\[\n\\begin{aligned}\n\\boldsymbol{\\Sigma}\n=\n\\text{cov}(\\mathbf{x})\n&=\n\\begin{bmatrix}\n\\sigma^2_1 & \\sigma_{1,2} & \\cdots & \\sigma_{1,n}\\\\\n\\sigma_{1,2} & \\sigma_{2}^2 & \\cdots & \\sigma_{2,n}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n\\sigma_{1,n} & \\sigma_{2,n} & \\cdots & \\sigma_{n}^2\\\\\n\\end{bmatrix}\n\\\\\n&=\n\\begin{bmatrix}\n\\text{var}(x_1) & \\text{cov}(x_1,\\,x_2) & \\cdots & \\text{cov}(x_1,\\,x_n)\\\\\n\\text{cov}(x_2,\\,x_1) & \\text{var}(x_2) & \\cdots & \\text{cov}(x_2,\\,x_n)\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n\\text{cov}(x_n,\\,x_1) & \\text{cov}(x_n,\\,x_2) & \\cdots & \\text{var}(x_n)\\\\\n\\end{bmatrix}\n.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/00-welcome.html#oh-my-aching-eigen",
    "href": "slides/00-welcome.html#oh-my-aching-eigen",
    "title": "Welcome to STA 542!",
    "section": "Oh my aching eigen",
    "text": "Oh my aching eigen\n\n\n\n\n\n\n\nEigenvectors and eigenvalues\n\n\nThere are \\(n\\) orthogonal vectors \\(\\mathbf{v}_i\\in\\mathbb{R}^n\\) and values \\(\\lambda_i&gt;0\\) satisfying:\n\\[\n\\boldsymbol{\\Sigma}\\mathbf{v}_i=\\lambda_i\\mathbf{v}_i.\n\\] Positive definite means all the eigenvalues are real and strictly postive.\n\n\n\n\n\n\n\n\n\n\n\nEigendecomposition (AKA spectral decomposition)\n\n\nA useful way to rewrite \\(\\boldsymbol{\\Sigma}\\): \\[\n\\begin{aligned}\n\\boldsymbol{\\Sigma}\n&=\n\\begin{bmatrix}\n\\mathbf{v}_1 & \\mathbf{v}_2 & \\cdots & \\mathbf{v}_n\n\\end{bmatrix}\n\\begin{bmatrix}\n\\lambda_1 & & &\\mathbf{0}\\\\\n& \\lambda_2 & & \\\\\n  & & \\ddots& \\\\\n  \\mathbf{0}  & & & \\lambda_n\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{v}_1^{\\scriptscriptstyle\\mathsf{T}} \\\\ \\mathbf{v}_2^{\\scriptscriptstyle\\mathsf{T}} \\\\ \\vdots \\\\ \\mathbf{v}_n^{\\scriptscriptstyle\\mathsf{T}}\n\\end{bmatrix}\n=\n\\mathbf{Q}\\boldsymbol{\\Lambda}\\mathbf{Q}^{\\scriptscriptstyle\\mathsf{T}}.\n\\end{aligned}\n\\]\nWe often set \\(||\\mathbf{v}_i||_2=1\\), and so the \\(n\\times n\\) matrix \\(\\mathbf{Q}\\) is orthogonal: \\(\\mathbf{Q}\\mathbf{Q}^{\\scriptscriptstyle\\mathsf{T}}=\\mathbf{Q}^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{Q}=\\mathbf{I}_n\\)."
  },
  {
    "objectID": "slides/00-welcome.html#who-cares",
    "href": "slides/00-welcome.html#who-cares",
    "title": "Welcome to STA 542!",
    "section": "Who cares?",
    "text": "Who cares?\nThe eigenvectors of \\(\\boldsymbol{\\Sigma}\\) point along the axes of the elliptical density contours. These are the directions of the principal components!"
  },
  {
    "objectID": "slides/00-welcome.html#review-joint-distributions",
    "href": "slides/00-welcome.html#review-joint-distributions",
    "title": "Welcome to STA 542!",
    "section": "Review: joint distributions",
    "text": "Review: joint distributions\n\nThe marginal distribution:\n\\[\np(\\mathbf{y}) = \\int p(\\mathbf{x},\\,\\mathbf{y})\\,\\text{d}\\mathbf{x}.\n\\]\n\n\nThe conditional distribution:\n\\[\np(\\mathbf{y}\\,|\\,\\mathbf{x})\n=\n\\frac{p(\\mathbf{x},\\,\\mathbf{y})}{p(\\mathbf{x})}\n=\n\\frac{p(\\mathbf{x}\\,|\\,\\mathbf{y})p(\\mathbf{y})}{p(\\mathbf{x})}\n.\n\\]"
  },
  {
    "objectID": "slides/00-welcome.html#marginals-and-conditionals",
    "href": "slides/00-welcome.html#marginals-and-conditionals",
    "title": "Welcome to STA 542!",
    "section": "Marginals and conditionals",
    "text": "Marginals and conditionals\nIf you apply those formulas to this\n\\[\n\\begin{bmatrix}\n\\mathbf{x} \\\\\n\\mathbf{y}\n\\end{bmatrix}\n\\sim\n\\text{N}_{n+m}\n\\left(\n\\begin{bmatrix}\n\\boldsymbol{\\mu}_x \\\\\n\\boldsymbol{\\mu}_y\n\\end{bmatrix}\n,\\,\n\\begin{bmatrix}\n\\boldsymbol{\\Sigma}_x & \\boldsymbol{\\Sigma}_{xy}\\\\\n\\boldsymbol{\\Sigma}_{xy}^{\\scriptscriptstyle\\mathsf{T}} & \\boldsymbol{\\Sigma}_y\n\\end{bmatrix}\n\\right),\n\\]\n\nthen you get this:\n\\[\n\\begin{aligned}\n\\mathbf{x} &\\sim  \\text{N}_n(\\boldsymbol{\\mu}_x,\\,\\boldsymbol{\\Sigma}_x)\\\\\n\\mathbf{y} &\\sim  \\text{N}_m(\\boldsymbol{\\mu}_y,\\,\\boldsymbol{\\Sigma}_y)\\\\\n\\mathbf{x} \\,|\\,\\mathbf{y}\n&\\sim\n\\text{N}_n\n\\left(\n\\boldsymbol{\\mu}_x + \\boldsymbol{\\Sigma}_{xy}\\boldsymbol{\\Sigma}_y^{-1}(\\mathbf{y}-\\boldsymbol{\\mu}_y)\n,\\,\n\\boldsymbol{\\Sigma}_x - \\boldsymbol{\\Sigma}_{xy}\n\\boldsymbol{\\Sigma}_y^{-1}\n\\boldsymbol{\\Sigma}_{xy}^{\\scriptscriptstyle\\mathsf{T}}\n\\right)\n\\\\\n\\mathbf{y} \\,|\\,\\mathbf{x}\n&\\sim\n\\text{N}_m\n\\left(\n\\boldsymbol{\\mu}_y + \\boldsymbol{\\Sigma}_{xy}^{\\scriptscriptstyle\\mathsf{T}}\\boldsymbol{\\Sigma}_x^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_x)\n,\\,\n\\boldsymbol{\\Sigma}_y - \\boldsymbol{\\Sigma}_{xy}^{\\scriptscriptstyle\\mathsf{T}}\n\\boldsymbol{\\Sigma}_x^{-1}\n\\boldsymbol{\\Sigma}_{xy}\n\\right).\n\\end{aligned}\n\\]\n\n\nSo the individual components are all normal: \\(x_i\\sim\\text{N}(\\mu_i,\\,\\sigma^2_i)\\)."
  },
  {
    "objectID": "slides/00-welcome.html#those-ugly-conditional-formulas-are-not-new",
    "href": "slides/00-welcome.html#those-ugly-conditional-formulas-are-not-new",
    "title": "Welcome to STA 542!",
    "section": "Those ugly conditional formulas are not new",
    "text": "Those ugly conditional formulas are not new\nConsider the bivariate case:\n\\[\n\\begin{bmatrix}\nx \\\\\ny\n\\end{bmatrix}\n\\sim\\text{N}_2\n\\left(\n\\begin{bmatrix}\n\\mu_x \\\\\n\\mu_y\n\\end{bmatrix}\n,\\,\n\\begin{bmatrix}\n\\sigma_x^2\n&\n\\rho\\sigma_x\\sigma_y\n\\\\\n\\rho\\sigma_x\\sigma_y\n&\n\\sigma^2_y\n\\end{bmatrix}\n\\right).\n\\]\n\nThen the formula collapses to:\n\\[\ny\\,|\\,\nx\n\\sim\n\\text{N}\n\\left(\n\\underbrace{\\left(\\mu_y-\\rho\\frac{\\sigma_y}{\\sigma_x}\\mu_x\\right)}_{\\beta_0}\n+\n\\underbrace{\\rho\\frac{\\sigma_y}{\\sigma_x}}_{\\beta_1}\nx\n,\\,\n\\underbrace{(1-\\rho)^2\n\\sigma^2_y}_{\\sigma^2}\n\\right).\n\\]\n\n\nIn other words:\n\\[\ny = \\beta_0+\\beta_1x + \\varepsilon,\\quad \\varepsilon\\sim\\text{N}(0,\\,\\sigma^2).\n\\]\n\n\nWelcome back to regression 101!"
  },
  {
    "objectID": "slides/00-welcome.html#affine-transformations",
    "href": "slides/00-welcome.html#affine-transformations",
    "title": "Welcome to STA 542!",
    "section": "Affine transformations",
    "text": "Affine transformations\nFix some objects:\n\nRandom \\(\\mathbf{x}\\sim\\text{N}_n(\\boldsymbol{\\mu},\\,\\boldsymbol{\\Sigma})\\);\nConstant \\(\\mathbf{A}\\in\\mathbb{R}^{m\\times n}\\);\nConstant \\(\\mathbf{c}\\in\\mathbb{R}^m\\).\n\n\nThen\n\\[\n\\mathbf{y}\n=\n\\mathbf{A}\n\\mathbf{x}\n+\n\\mathbf{c}\n\\sim\n\\text{N}_m\n\\left(\n\\mathbf{A}\n\\boldsymbol{\\mu}\n+\n\\mathbf{c}\n,\\,\n\\mathbf{A}\n\\boldsymbol{\\Sigma}\n\\mathbf{A}^{\\scriptscriptstyle\\mathsf{T}}\n\\right)\n.\n\\]\n\n\nProve it on Problem Set 0!"
  },
  {
    "objectID": "slides/00-welcome.html#a-useful-special-case-problem-set-0",
    "href": "slides/00-welcome.html#a-useful-special-case-problem-set-0",
    "title": "Welcome to STA 542!",
    "section": "A useful special case (Problem Set 0!)",
    "text": "A useful special case (Problem Set 0!)\n\n\n\n\n\n\nLinear combinations of independent normals are normal\n\n\nIf \\(x_i\\overset{\\text{indep}}{\\sim}\\text{N}(\\mu_i,\\,\\sigma^2_i)\\) and \\(a_i\\in\\mathbb{R}\\) are constant, then\n\\[\n\\sum\\limits_{i=1}^na_ix_i\\sim\\text{N}\\left(\\sum\\limits_{i=1}^na_i\\mu_i,\\, \\sum\\limits_{i=1}^na_i^2\\sigma_i^2\\right).\n\\]\n\n\n\n\n\nThe result on the previous slide is way more general, and the linear combination is still normal even if the \\(X_i\\) are dependent, but the formula for the variance is less nice;\nThe mean formula is right even if the \\(x_i\\) are dependent and non-Gaussian;\nThe variance formula is right even if the \\(x_i\\) are non-Gaussian."
  },
  {
    "objectID": "slides/00-welcome.html#independence",
    "href": "slides/00-welcome.html#independence",
    "title": "Welcome to STA 542!",
    "section": "Independence",
    "text": "Independence\n\nIf random variables are independent, then they are uncorrelated (their covariance is zero). The reverse is false in general!\nSo writing \\(x_1,\\,x_2,\\,...,\\,x_n\\overset{\\text{iid}}{\\sim}\\text{N}(\\mu,\\,\\sigma^2)\\) is the same as saying\n\n\n\\[\n\\begin{bmatrix}\nx_1\\\\\nx_2\\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix}\n\\sim\\text{N}_n\\left(\\mu\\mathbf{1}_n,\\,\\sigma^2\\mathbf{I}_n\\right).\n\\]\n\n\n\n\n\\(\\mathbf{1}_n\\) is the \\(n\\times 1\\) vector of all ones;\n\n\n\n\n\n\\(\\mathbf{I}_n\\) is the \\(n\\times n\\) identity matrix: ones on the diagonal, zeros off."
  },
  {
    "objectID": "slides/02-ar-1-inference.html#the-simplest-non-trivial-time-series-model",
    "href": "slides/02-ar-1-inference.html#the-simplest-non-trivial-time-series-model",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "The simplest non-trivial time series model",
    "text": "The simplest non-trivial time series model\nThe autoregression of order 1, or AR(1):\n\n\\[\n\\begin{aligned}\ny_t\n&=\n\\beta_0\n+\n\\beta_1\ny_{t-1}\n+\n\\varepsilon_t,\n\\quad\n\\varepsilon_t\\iid\\text{N}(0\\com\\sigma^2)\n\\\\\ny_0\n&\\sim\n\\text{N}(\\mu_0\\com \\initvar),\n\\end{aligned}\n\\]\n\n\nThat’s the recursive form. It implies a joint distribution written marginal-conditional style:\n\n\n\\[\n\\begin{aligned}\np(y_{0:T})\n&=\np(y_0)\n\\prod_{t=1}^T\np(y_t\\given y_{0:t-1})\n\\\\\n&=\np(y_0)\n\\prod_{t=1}^T\np(y_t\\given y_{t-1}).\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#the-ar1-joint-distribution",
    "href": "slides/02-ar-1-inference.html#the-ar1-joint-distribution",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "The AR(1) joint distribution",
    "text": "The AR(1) joint distribution\nJoint distribution:\n\\[\n\\begin{bmatrix}\ny_0 & y_1 & \\cdots & y_T\n\\end{bmatrix}^\\tr\n\\sim\\text{N}_{T+1}\\left(\\Bmu\\com \\BSigma\\right).\n\\]\n\nMoments:\n\\[\n\\begin{aligned}\nE(y_t)\n&=\n\\beta_0\\sum\\limits_{i=0}^{t-1}\\beta_1^i\n+\n\\beta_1^t\\mu_0\n\\\\\n\\var(y_t)\n&=\n\\sigma^2\n\\sum\\limits_{i=0}^{t-1}\\beta_1^{2i}+\n\\beta_1^{2t}\\initvar\n\\\\\n\\cov(y_t\\com y_s)\n&=\n\\begin{cases}\n\\beta_1^{s-t}\\var(y_t) & t\\leq s\\\\\n\\beta_1^{t-s}\\var(y_s) & s &lt; t.\n\\end{cases}\n.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#the-special-case-when-beta_11",
    "href": "slides/02-ar-1-inference.html#the-special-case-when-beta_11",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "The special case when \\(|\\beta_1|<1\\)\n",
    "text": "The special case when \\(|\\beta_1|&lt;1\\)\n\nThe mean and variance are time-invariant, and the covariance structure is shift-invariant:\n\n\\[\n\\begin{aligned}\nE(y_t)\n&=\n\\frac{\\beta_0}{1-\\beta_1}\n\\\\\n\\var(y_t)\n&=\n\\frac{\\sigma^2}{1-\\beta_1^2}\n\\\\\n\\gamma(h)\n&=\n\\cov(y_{t+h}\\com y_t)\n=\n\\beta_1^{h}\\frac{\\sigma^2}{1-\\beta_1^2}.\n\\end{aligned}\n\\]\n\n\nThe common marginal shared by all \\(y_t\\) is called the stationary distribution:\n\n\n\\[\ny_t\\sim\\text{N}\\left(\\frac{\\beta_0}{1-\\beta_1}\\com \\frac{\\sigma^2}{1-\\beta_1^2}\\right).\n\\]\n\n\nSo “did: dependent but identically distributed.”"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#a-neat-fact-to-notice",
    "href": "slides/02-ar-1-inference.html#a-neat-fact-to-notice",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "A neat fact to notice",
    "text": "A neat fact to notice\nNotice:\n\\[\n\\begin{bmatrix}\ny_0\n\\\\\ny_1\n\\\\\ny_2\n\\\\\ny_3\n\\end{bmatrix}\n\\sim\n\\text{N}_{4}\n\\left(\n\\frac{\\beta_0}{1-\\beta_1}\\mathbf{1}_{4}\n\\com\n\\begin{bmatrix}\n\\color{red}{\\gamma(0)} & \\color{blue}{\\gamma(1)} & \\color{orange}{\\gamma(2)} & \\color{green}{\\gamma(3)}\n\\\\\n\\color{blue}{\\gamma(1)} & \\color{red}{\\gamma(0)} & \\color{blue}{\\gamma(1)} & \\color{orange}{\\gamma(2)}\n\\\\\n\\color{orange}{\\gamma(2)} & \\color{blue}{\\gamma(1)} & \\color{red}{\\gamma(0)} & \\color{blue}{\\gamma(1)}\n\\\\\n\\color{green}{\\gamma(3)}  & \\color{orange}{\\gamma(2)} & \\color{blue}{\\gamma(1)} & \\color{red}{\\gamma(0)}\n\\end{bmatrix}\n\\right)\n\\]\n\nThe covariance is an example of a Toeplitz matrix."
  },
  {
    "objectID": "slides/02-ar-1-inference.html#this-is-all-an-example-of-strict-stationarity",
    "href": "slides/02-ar-1-inference.html#this-is-all-an-example-of-strict-stationarity",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "This is all an example of (strict) stationarity",
    "text": "This is all an example of (strict) stationarity\nA joint distribution is (strictly) stationary if it is “shift invariant”:\n\\[\n\\{y_{t_1}\\com y_{t_2}\\com ...\\com y_{t_n}\\}\\overset{d}{=}\\{y_{t_1+h}\\com y_{t_2+h}\\com ...\\com y_{t_n+h}\\}.\n\\]\nThe Gaussian AR(1) with \\(|\\beta_1|&lt;1\\) has this property."
  },
  {
    "objectID": "slides/02-ar-1-inference.html#estimating-equations",
    "href": "slides/02-ar-1-inference.html#estimating-equations",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Estimating equations",
    "text": "Estimating equations\nIf the AR(1) is stationary, then\n\\[\n\\begin{aligned}\n\\gamma(1)&=\\beta_1\\gamma(0) &&\\implies\\beta_1=\\frac{\\gamma(1)}{\\gamma(0)}\\\\\n\\mu&=\\frac{\\beta_0}{1-\\beta_1}&&\\implies\\beta_0=(1-\\beta_1)\\mu\\\\\n\\gamma(0)&=\\frac{\\sigma^2}{1-\\beta_1^2}&&\\implies \\sigma^2=(1-\\beta_1^2)\\gamma(0).\n\\end{aligned}\n\\]\nEverywhere you see a “population” expected value, plug in the corresponding sample average."
  },
  {
    "objectID": "slides/02-ar-1-inference.html#yule-walker",
    "href": "slides/02-ar-1-inference.html#yule-walker",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Yule-Walker",
    "text": "Yule-Walker\nMethod of moments estimators:\n\n\\[\n\\begin{aligned}\n\\hat{\\beta}_1&=\\frac{\\hat{\\gamma}_T(1)}{\\hat{\\gamma}_T(0)}\n\\\\\n\\hat{\\beta}_0&=(1-\\hat{\\beta}_1)\\hat{\\mu}_T\\\\\n\\hat{\\sigma^2_T}&=(1-\\hat{\\beta}_1^2)\\hat{\\gamma}_T(0).\n\\end{aligned}\n\\]\n\n\nThis slide is rank nonsense if the AR(1) isn’t stationary."
  },
  {
    "objectID": "slides/02-ar-1-inference.html#news-flash-beta_1-is-a-correlation",
    "href": "slides/02-ar-1-inference.html#news-flash-beta_1-is-a-correlation",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "News flash: \\(\\beta_1\\) is a correlation!",
    "text": "News flash: \\(\\beta_1\\) is a correlation!\n\nNote:\n\n\n\\[\n\\begin{aligned}\n\\beta_1 = \\frac{\\gamma(1)}{\\gamma(0)}\n&=\\frac{\\cov(y_{t+1}\\com y_t)}{\\var(y_t)}\n\\\\\n&=\\frac{\\cov(y_{t+1}\\com y_t)}{\\text{sd}(y_{t})\\text{sd}(y_t)}\n\\\\\n&=\\frac{\\cov(y_{t+1}\\com y_t)}{\\text{sd}(y_{t+1})\\text{sd}(y_t)}\n\\\\\n&=\\text{corr}(y_{t+1}\\com y_t).\n\\end{aligned}\n\\]\n\n\nSo \\(-1&lt; \\beta_1&lt; 1\\) is the lag-1 autocorrelation, and \\(\\hat{\\beta}_1\\) is the sample version."
  },
  {
    "objectID": "slides/02-ar-1-inference.html#asymptotics",
    "href": "slides/02-ar-1-inference.html#asymptotics",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Asymptotics",
    "text": "Asymptotics\n\n\n\n\n\n\nShumway and Stoffer (2025) Property 3.7\n\n\nFor a stationary AR(1), the Yuke-Walker estimator has:\n\\[\n\\sqrt{T}(\\hat{\\beta}_1-\\beta_1)\\cd\\N\\left(0\\com\\frac{\\sigma^2}{\\gamma(0)}\\right).\n\\] And note in this case that \\(\\sigma^2 / \\gamma(0)=1-\\beta_1^2\\). The result remains true if you plug in the estimator of the asymptotic variance.\n\n\n\n\nAsymptotically valid \\(100\\times(1-\\alpha)\\%\\) confidence interval:\n\\[\n\\hat{\\beta}_1\\pm z_{1-\\alpha/2}\\sqrt{\\frac{1-\\hat{\\beta}_1^2}{T}}.\n\\]"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#simulating-the-sampling-distribution",
    "href": "slides/02-ar-1-inference.html#simulating-the-sampling-distribution",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Simulating the sampling distribution",
    "text": "Simulating the sampling distribution\nWe will do a lot of this today:\n\\[\n\\begin{matrix}\n\\text{0. Ground truth} &&& \\text{AR(1)} && \\\\\n&\\swarrow &\\swarrow& \\cdots &\\searrow&\\searrow \\\\\n\\text{1. Many datasets}&y_{0:T}^{(1)} &y_{0:T}^{(2)}& \\cdots &y_{0:T}^{(k-1)}&y_{0:T}^{(k)} \\\\\n&\\downarrow &\\downarrow& \\cdots &\\downarrow&\\downarrow \\\\\n\\text{2. Many estimates}&\\hat{\\beta}_1^{(1)} &\\hat{\\beta}_1^{(2)}& \\cdots &\\hat{\\beta}_1^{(k-1)}&\\hat{\\beta}_1^{(k)} \\\\\n\\end{matrix}\n\\]\nA histogram or boxplot of the \\(\\hat{\\beta}_1^{(j)}\\) approximates the finite-\\(T\\) sampling distribution of the estimator."
  },
  {
    "objectID": "slides/02-ar-1-inference.html#what-happens-for-beta_1-close-to-pm1",
    "href": "slides/02-ar-1-inference.html#what-happens-for-beta_1-close-to-pm1",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "What happens for \\(\\beta_1\\) close to \\(\\pm1\\)?",
    "text": "What happens for \\(\\beta_1\\) close to \\(\\pm1\\)?\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\n# app.R\nlibrary(shiny)\n\n# ---------- Configuration ----------\nSAMPLE_SIZES &lt;- seq(10, 200, by = 20)   # smaller default for speed\nNSIM &lt;- 500                             # number of simulated AR(1) paths per sample size\nY_LIMITS &lt;- c(-1, 1)\n# ------------------------------------\n\n# Yule-Walker AR(1) estimator\nyule_walker_ar1 &lt;- function(x) {\n  n &lt;- length(x)\n  x &lt;- x - mean(x)  # center\n  gamma0 &lt;- mean(x^2)\n  gamma1 &lt;- mean(x[-1] * x[-n])\n  gamma1 / gamma0\n}\n\n# Simulate one stationary AR(1) with variance 1\nsimulate_ar_1 &lt;- function(T, b0, b1, s, m0, s0){\n  y &lt;- numeric(T)\n  y[1] &lt;- rnorm(1, m0, s0)\n  for(t in 2:T){\n    y[t] &lt;- b0 + b1 * y[t - 1] + rnorm(1, 0, s)\n  }\n  return(y)\n}\n\nui &lt;- fluidPage(\n  titlePanel(\"Sampling distribution of Yule–Walker AR(1) estimator\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"b1\", \"True value of β₁:\",\n                  min = -0.99, max = 0.99, value = 0.5, step = 0.01)\n    ),\n    mainPanel(\n      plotOutput(\"boxPlot\", height = \"600px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  \n  output$boxPlot &lt;- renderPlot({\n    b0 &lt;- 0\n    b1 &lt;- input$b1\n    s &lt;- 1\n    m0 &lt;- b0 / (1 - b1)\n    s0 &lt;- s / sqrt(1 - b1^2)\n    sizes &lt;- SAMPLE_SIZES\n    nsim &lt;- NSIM\n    \n    # Collect estimates in a list, one element per sample size\n    est_list &lt;- vector(\"list\", length(sizes))\n    \n    for (i in seq_along(sizes)) {\n      n &lt;- sizes[i]\n      phi_hats &lt;- numeric(nsim)\n      for (s in 1:nsim) {\n        x &lt;- simulate_ar_1(n, b0, b1, s, m0, s0)#simulate_ar1(n, phi)\n        phi_hats[s] &lt;- yule_walker_ar1(x)\n      }\n      est_list[[i]] &lt;- phi_hats\n    }\n    \n    # Draw boxplots side by side\n    par()\n    boxplot(est_list,\n            names = sizes,\n            ylim = Y_LIMITS,\n            xlab = \"Sample size T\",\n            ylab = \"Estimate\",\n            main = paste(\"True value: \", b1),\n            col = \"lightgray\", pch = 19)\n    \n    abline(h = b1, col = \"red\", lty = 2, lwd = 2)\n  })\n}\n\nshinyApp(ui, server)"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#assumptions",
    "href": "slides/02-ar-1-inference.html#assumptions",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Assumptions",
    "text": "Assumptions\n\nNormality was not essential, but stationarity was crucial;\n\nWhat if we flip that?\n\nFirmly assume normality;\nCast stationarity to the wind."
  },
  {
    "objectID": "slides/02-ar-1-inference.html#lets-improve-the-notation",
    "href": "slides/02-ar-1-inference.html#lets-improve-the-notation",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Let’s improve the notation",
    "text": "Let’s improve the notation\nThe autoregression of order 1, or AR(1):\n\n\\[\n\\begin{aligned}\ny_t\n&=\n\\beta_0\n+\n\\beta_1\ny_{t-1}\n+\n\\varepsilon_t,\n\\quad\n\\varepsilon_t\\iid\\text{N}(0\\com\\sigma^2)\n\\\\\ny_0\n&\\sim\n\\text{N}(\\mu_0\\com \\initvar),\n\\end{aligned}\n\\]\n\n\nThis implies a joint distribution governed by a finite set of static parameters \\(\\Btheta = \\begin{bmatrix}\\beta_0&\\beta_1&\\sigma^2\\end{bmatrix}^\\tr\\):\n\n\n\\[\n\\begin{aligned}\np(y_{0:T}\\given \\Btheta)\n&=\np(y_0)\n\\prod_{t=1}^T\np(y_t\\given y_{t-1}\\com\\Btheta).\n\\end{aligned}\n\\]\n\n\nViewed as a function of \\(\\Btheta\\), that’s a likelihood!\n\n\nMaximize it, or combine with a prior."
  },
  {
    "objectID": "slides/02-ar-1-inference.html#the-conditional-likelihood",
    "href": "slides/02-ar-1-inference.html#the-conditional-likelihood",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "The conditional likelihood",
    "text": "The conditional likelihood\nIn practice, we usually just condition on the initial value:\n\\[\np(y_{1:T}\\given  y_0\\com \\Btheta)\n=\n\\prod_{t=1}^T\np(y_t\\given y_{t-1}\\com\\Btheta).\n\\]\n\nDoesn’t affect results too much, but saves headache."
  },
  {
    "objectID": "slides/02-ar-1-inference.html#maximum-likelihood-estimation",
    "href": "slides/02-ar-1-inference.html#maximum-likelihood-estimation",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Maximum likelihood estimation",
    "text": "Maximum likelihood estimation\nTreating the observed data \\(y_{0:T}\\) as fixed, we want:\n\\[\n\\hat{\\Btheta}_T=\\argmax{\\Btheta}\\,p(y_{1:T}\\given y_0\\com \\Btheta).\n\\]\nTo do this, it helps to take the view that the AR(1) is “just” a simple linear regression with \\(x_t=y_{t-1}\\)."
  },
  {
    "objectID": "slides/02-ar-1-inference.html#maximum-likelihood-estimation-1",
    "href": "slides/02-ar-1-inference.html#maximum-likelihood-estimation-1",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Maximum likelihood estimation",
    "text": "Maximum likelihood estimation\nSince \\(y_t\\given y_{t-1}\\com\\Btheta\\sim\\N(\\beta_0+\\beta_1y_{t-1}\\com\\sigma^2)\\), we have\n\n\\[\n\\begin{aligned}\np(y_{1:T}\\given  y_0\\com \\Btheta)\n&=\n\\prod_{t=1}^T\np(y_t\\given y_{t-1}\\com\\Btheta)\n\\\\\n&=\n\\prod_{t=1}^T\n\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{1}{2}\\frac{(y_t-\\beta_0-\\beta_1y_{t-1})^2}{\\sigma^2}\\right)\n\\\\\n&=\n(2\\pi\\sigma^2)^{-T/2}\n\\exp\\left(-\\frac{1}{2\\sigma^2}\\sum\\limits_{t=1}^T(y_t-\\beta_0-\\beta_1y_{t-1})^2\\right)\n.\n\\end{aligned}\n\\]\n\n\nTo compute the MLE, we just treat \\(y_{0:T}\\) as fixed.\n\n\nWhere do you think this is headed?"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#stack-em-up",
    "href": "slides/02-ar-1-inference.html#stack-em-up",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Stack ’em up",
    "text": "Stack ’em up\nDefine some things:\n\n\\[\n\\begin{aligned}\n\\By_T\n&=\n\\begin{bmatrix}y_1&y_2 & \\cdots & y_T\\end{bmatrix}^\\tr\n\\\\\n\\BX_T\n&=\n\\begin{bmatrix}\n1 & 1 & \\cdots & 1 \\\\\ny_0 & y_1 & \\cdots & y_{T-1}\n\\end{bmatrix}^\\tr\n\\\\\n\\Bbeta\n&=\n\\begin{bmatrix}\\beta_0&\\beta_1\\end{bmatrix}^\\tr\n.\n\\end{aligned}\n\\]\n\n\nSo the log-likelihood is:\n\\[\n\\ln\np(y_{1:T}\\given  y_0\\com \\Btheta)\n=\n-\\frac{T}{2}\\ln(2\\pi\\sigma^2)\n-\\frac{1}{2\\sigma^2}\n||\\By_T-\\BX_T\\Bbeta||_2^2\n.\n\\]\n\n\nLook familiar?"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#were-just-back-to-regression-101",
    "href": "slides/02-ar-1-inference.html#were-just-back-to-regression-101",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "We’re just back to regression 101",
    "text": "We’re just back to regression 101\nWe have\n\\[\n\\ln\np(y_{1:T}\\given  y_0\\com \\Btheta)\n=\n-\\frac{T}{2}\\ln(2\\pi\\sigma^2)\n-\\frac{1}{2\\sigma^2}\n||\\By_T-\\BX_T\\Bbeta||_2^2\n,\n\\]\n\nand we know that\n\\[\n\\begin{aligned}\n\\hat{\\Btheta}_T\n&=\\argmax{\\Btheta}\\,\\ln p(y_{1:T}\\given y_0\\com \\Btheta)\n\\\\\n&=\\argmin{\\Btheta}\\,-\\ln p(y_{1:T}\\given y_0\\com \\Btheta).\n\\end{aligned}\n\\]\n\n\nThis gives\n\\[\n\\begin{aligned}\n\\hat{\\Bbeta}_T\n&=\n\\argmin{\\Bbeta}\n\\,||\\By_T-\\BX_T\\Bbeta||_2^2\n\\\\\n\\hat{\\sigma^2_T}\n&=\n||\\By_T-\\BX_T\\hat{\\Bbeta}_T||_2^2 / T.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#ols-for-the-ar1",
    "href": "slides/02-ar-1-inference.html#ols-for-the-ar1",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "OLS for the AR(1)",
    "text": "OLS for the AR(1)\n\n\nThe maximum (conditional) likelihood estimator in the AR(1) is the same as the ordinary least squares estimator:\n\n\n\n\\[\n\\begin{aligned}\n\\hat{\\Bbeta}_T\n&=\n(\\BX_T^\\tr\\BX_T)^{-1}\\BX_T^\\tr\\By_T\n\\\\\n\\hat{\\sigma^2_T}\n&=\n||\\By_T-\\BX_T\\hat{\\Bbeta}_T||_2^2 / T.\n\\end{aligned}\n\\]\n\n\n\nIt doesn’t matter that there’s time series dependence. Once the data are observed and fixed, everything you know about iid multiple regression applies unmodified.\n\n\n\n\nWhen we graduate to the AR(p) in a few weeks, everything looks identical. \\(\\BX_T\\) just has more columns."
  },
  {
    "objectID": "slides/02-ar-1-inference.html#if-the-data-are-streaming-apply-pset-0",
    "href": "slides/02-ar-1-inference.html#if-the-data-are-streaming-apply-pset-0",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "If the data are streaming, apply PSET 0!",
    "text": "If the data are streaming, apply PSET 0!\nDo not recompute that matrix inverse every period.\n\nInstead:\n\\[\n\\begin{aligned}\n\\hat{\\Bbeta}_t\n&=\n\\hat{\\Bbeta}_{t-1}\n+\n\\Bk_t(y_t-\\Bx_t^\\tr\\hat{\\Bbeta}_{t-1})\n\\\\\n\\Bk_t\n&=\n\\frac{\\BP_{t-1}\\Bx_t}\n{1+\\Bx_t^\\tr\\BP_{t-1}\\Bx_t}\n\\\\\n\\BP_t\n&=\n(\\BX_t^\\tr\\BX_t)^{-1}\n\\\\\n&=\n(\\BX_{t-1}^\\tr\\BX_{t-1}+\\Bx_t\\Bx_t^\\tr)^{-1}\n\\\\\n&=\n(\\BP_{t-1}+\\Bx_t\\Bx_t^\\tr)^{-1}\n\\\\\n&=\n\\BP_{t-1}\n-\n\\Bk_t\n\\Bx_t^\\tr\\BP_{t-1}\n.\n\\end{aligned}\n\\]\n\n\nKeyword: rank-1 update!"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#quick-aside",
    "href": "slides/02-ar-1-inference.html#quick-aside",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Quick aside",
    "text": "Quick aside\n\nLater in the semester, we will study something called the Kalman filter;\nThis is the beating heart of time series analysis in my humble opinion;\nIf you invest in understanding the recursive form of OLS and the Bayesian version we will see later today, you are well equipped to understand the Kalman filter;\nOnce you understand the Kalman filter, you can understand anything else in time series analysis;\n\n\n…the parts worth understanding, anyway."
  },
  {
    "objectID": "slides/02-ar-1-inference.html#asymptotics-if-the-truth-is-stationary",
    "href": "slides/02-ar-1-inference.html#asymptotics-if-the-truth-is-stationary",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Asymptotics if the truth is stationary",
    "text": "Asymptotics if the truth is stationary\n\n\n\n\n\n\nShumway and Stoffer (2025) Property 3.9\n\n\nFor a stationary AR(1), the MLE/OLS estimator has:\n\\[\n\\sqrt{T}(\\hat{\\beta}_1-\\beta_1)\\cd\\N\\left(0\\com 1-\\beta_1^2\\right).\n\\]\n\n\n\n\nSame as Yule-Walker!"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#simulation-in-the-stationary-case",
    "href": "slides/02-ar-1-inference.html#simulation-in-the-stationary-case",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Simulation in the stationary case",
    "text": "Simulation in the stationary case"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#asymptotics-if-the-truth-is-a-random-walk",
    "href": "slides/02-ar-1-inference.html#asymptotics-if-the-truth-is-a-random-walk",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Asymptotics if the truth is a random walk",
    "text": "Asymptotics if the truth is a random walk\n\nIt’s gross.\n\n\n\n\n\n\n\n\nHamilton (1994) [17.4.28]\n\n\nIf the true parameters are \\(\\beta_0=0\\) and \\(\\beta_1=1\\), then MLE/OLS estimator has \\[\nT(\\hat{\\beta}_1-1)\\cd\\frac{\\frac{1}{2}[W(1)^2-1]-W(1)\\int_0^1W(r)\\,\\dd r}{\\int_0^1 W(r)^2\\dd r-\\left(\\int_0^1 W(r)\\,\\dd r\\right)^2},\n\\]\nwhere \\(W\\) is a Wiener process (Brownian motion).\n\n\n\n\n\nSuffice it to say…the asymptotic distribution ain’t Gaussian.\n\n\nAnd the convergence rate is worse."
  },
  {
    "objectID": "slides/02-ar-1-inference.html#simulation-in-the-weird-rw-case",
    "href": "slides/02-ar-1-inference.html#simulation-in-the-weird-rw-case",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Simulation in the weird RW case",
    "text": "Simulation in the weird RW case\n\n\n\n\n\n\n\n\nDefinitely not Gaussian."
  },
  {
    "objectID": "slides/02-ar-1-inference.html#asymptotics-if-the-truth-is-rw-w-drift",
    "href": "slides/02-ar-1-inference.html#asymptotics-if-the-truth-is-rw-w-drift",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Asymptotics if the truth is RW w/ drift",
    "text": "Asymptotics if the truth is RW w/ drift\n\n\n\n\n\n\n\nHamilton (1994) [17.4.47]\n\n\nIf the true parameters are \\(\\beta_0\\neq 0\\) and \\(\\beta_1=1\\), then MLE/OLS estimator has \\[\n\\begin{bmatrix}\nT^{1/2}(\\hat{\\beta}_0-\\beta_0)\n\\\\\nT^{3/2}(\\hat{\\beta}_1-1)\n\\end{bmatrix}\n\\cd\n\\N_2\n\\left(\n\\Bzero\n\\com\n\\sigma^2\n\\begin{bmatrix}\n1 & \\beta_0/2\\\\\n\\beta_0/2 & \\beta_0^2/3\n\\end{bmatrix}^{-1}\n\\right)\n.\n\\]\n\n\n\nThings are Gaussian again!"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#simulation-in-the-rw-w-drift-case",
    "href": "slides/02-ar-1-inference.html#simulation-in-the-rw-w-drift-case",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Simulation in the RW w/ drift case",
    "text": "Simulation in the RW w/ drift case"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#asymptotics-if-the-truth-is-explosive",
    "href": "slides/02-ar-1-inference.html#asymptotics-if-the-truth-is-explosive",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Asymptotics if the truth is explosive",
    "text": "Asymptotics if the truth is explosive\n\n\n\n\n\n\n\nWhite (1958 AoMS)\n\n\nIf the true parameters are \\(\\beta_0= 0\\) and \\(|\\beta_1|&gt;1\\) and the errors are normal, then MLE/OLS estimator has \\[\n\\frac{|\\beta_1|^T}{\\beta_1^2-1}(\\hat{\\beta}_1-\\beta_1)\\cd \\text{Cauchy}.\n\\]"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#simulation-in-explosive-case",
    "href": "slides/02-ar-1-inference.html#simulation-in-explosive-case",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Simulation in explosive case",
    "text": "Simulation in explosive case"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#why-am-i-bothering-you-with-all-this",
    "href": "slides/02-ar-1-inference.html#why-am-i-bothering-you-with-all-this",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Why am I bothering you with all this?",
    "text": "Why am I bothering you with all this?\nHere’s the point:\n\nThe AR(1) has been very thoroughly studied;\nUnlike Yule-Walker, OLS “works” no matter the regime. Recall that relaxing the stationarity requirement was one of our motivations for exploring likelihood-based inference to begin with;\nNon-stationarity can make things weird and complicated, but it’s not so bad that we can’t prove theorems. Inference remains possible;\nThe math is…interesting. See Hamilton (1994) Chapter 17 if you are absolutely dying of curiosity;\nDoes any of this matter for “real-world” data analysis?\n\n\nProbably not.\n\n\nBut you know what does matter?"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#recall",
    "href": "slides/02-ar-1-inference.html#recall",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Recall",
    "text": "Recall\nGiven a prior distribution \\(p(\\Btheta)=p(\\beta_0\\com \\beta_1\\com \\sigma^2)\\) on the parameters and an observed time series \\(y_{0:T}\\), we seek to access the posterior distribution:\n\\[\np(\\Btheta\\given y_{0:T})\n=\n\\frac{p(y_{1:T}\\given y_0\\com \\Btheta)p(\\Btheta)}{p(y_{1:T}\\given y_0)}\n.\n\\]\n\nAs with MLE, everything you know about iid Bayesian regression applies pretty much unmodified."
  },
  {
    "objectID": "slides/02-ar-1-inference.html#conjugate-normal-inverse-gamma-prior",
    "href": "slides/02-ar-1-inference.html#conjugate-normal-inverse-gamma-prior",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Conjugate normal-inverse-gamma prior",
    "text": "Conjugate normal-inverse-gamma prior\nBayesian model with a conjugate prior:\n\\[\n\\begin{aligned}\n\\sigma^2\n&\\sim\n\\text{IG}(a_0\\com b_0)\n\\\\\n\\Bbeta\\given \\sigma^2\n&\\sim\n\\text{N}_2(\\Bm_0\\com\\sigma^2\\BH^{-1}_0)\n\\\\\ny_t\n\\given\ny_{t-1}\n\\com\n\\Bbeta\\com\\sigma^2\n&\\sim \\text{N}\n\\left(\n\\Bx_t^\\tr\\Bbeta\\com\\sigma^2\n\\right), && \\Bx_t=\\begin{bmatrix}1 & y_{t-1}\\end{bmatrix}^\\tr.\n\\end{aligned}\n\\]\n\nThe posterior is available in closed-form:\n\\[\n\\begin{aligned}\n\\sigma^2\\given y_{0:T}\n&\\sim\n\\text{IG}(a_T\\com b_T)\n\\\\\n\\Bbeta\\given \\sigma^2\\com y_{0:T}\n&\\sim\n\\text{N}_2(\\Bm_T\\com\\sigma^2\\BH^{-1}_T)\n\\\\\n\\\\\n\\BH_T\n&=\n\\BX_T^\\tr\\BX_T+\\BH_0\n\\\\\n\\Bm_T\n&=\n\\BH_T^{-1}(\\BX_T^\\tr\\By_T+\\BH_0\\Bm_0)\n\\\\\na_T\n&=\na_0 + T/2\n\\\\\nb_T\n&=\nb_0\n+\n(\\By_T^\\tr\\By_T+\\Bm_0^\\tr\\BH_0\\Bm_0-\\Bm_T^\\tr\\BH_T\\Bm_T)/2.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#make-it-nice-on-problem-set-1",
    "href": "slides/02-ar-1-inference.html#make-it-nice-on-problem-set-1",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Make it nice on Problem Set 1",
    "text": "Make it nice on Problem Set 1\n\nAfter \\(t-1\\) periods you have already computed\n\n\n\\[\n\\begin{aligned}\n\\sigma^2\\given y_{0:t-1}\n&\\sim\n\\text{IG}(a_{t-1}\\com b_{t-1})\n\\\\\n\\Bbeta\\given \\sigma^2\\com y_{0:t-1}\n&\\sim\n\\text{N}_2(\\Bm_{t-1}\\com\\sigma^2\\BH^{-1}_{t-1}).\n\\end{aligned}\n\\]\n\n\nThen \\(y_t\\) arrives. What’s the most efficient way to update?\n\n\n\\[\n\\begin{aligned}\n\\sigma^2\\given y_{0:t}\n&\\sim\n\\text{IG}(a_{t}\\com b_{t})\n\\\\\n\\Bbeta\\given \\sigma^2\\com y_{0:t}\n&\\sim\n\\text{N}_2(\\Bm_{t}\\com\\sigma^2\\BH^{-1}_{t}).\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#non-conjugate-normal-inverse-gamma-prior",
    "href": "slides/02-ar-1-inference.html#non-conjugate-normal-inverse-gamma-prior",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Non-conjugate normal-inverse-gamma prior",
    "text": "Non-conjugate normal-inverse-gamma prior\nInstead of \\(p(\\Bbeta\\com\\sigma^2)=p(\\Bbeta\\given\\sigma^2)p(\\sigma^2)\\), use \\(p(\\Bbeta\\com\\sigma^2)=p(\\Bbeta)p(\\sigma^2)\\):\n\n\\[\n\\begin{aligned}\n\\sigma^2\n&\\sim\n\\text{IG}(a_0\\com b_0)\n\\\\\n\\Bbeta\n&\\sim\n\\text{N}_2(\\Bm_0\\com\\BH^{-1}_0)\n\\\\\ny_t\n\\given\ny_{t-1}\n\\com\n\\Bbeta\\com\\sigma^2\n&\\sim \\text{N}\n\\left(\n\\Bx_t^\\tr\\Bbeta\\com\\sigma^2\n\\right), && \\Bx_t=\\begin{bmatrix}1 & y_{t-1}\\end{bmatrix}^\\tr.\n\\end{aligned}\n\\]\n\n\nThe full posterior is intractable, but the conditional posteriors are known:\n\n\n\\[\n\\begin{aligned}\n\\sigma^2\\given \\Bbeta\\com y_{0:T}\n&\\sim\\text{IG}(a_T\\com b_T)\\\\\n\\Bbeta\\given \\sigma^2\\com y_{0:T}\n&\\sim\\N_2(\\Bm_T\\com \\BH_T^{-1}).\n\\end{aligned}\n\\]\n\n\nAlternate sampling from them and you get a Gibbs sampler targeting \\(p(\\Bbeta\\com\\sigma^2\\given y_{0:T})\\)."
  },
  {
    "objectID": "slides/02-ar-1-inference.html#truncation-prior-to-enforce-stationarity",
    "href": "slides/02-ar-1-inference.html#truncation-prior-to-enforce-stationarity",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Truncation prior to enforce stationarity",
    "text": "Truncation prior to enforce stationarity\n\nAugment any prior with a truncation to enforce stationarity:\n\n\n\\[\n\\begin{aligned}\n\\tilde{p}(\\Btheta\\given y_{0:T})\n&\\propto\np(y_{1:T}\\given y_0\\com\\Btheta)\\tilde{p}(\\Btheta)\n\\\\\n&=\np(y_{1:T}\\given y_0\\com\\Btheta)p(\\Btheta)I(|\\beta_1|&lt;1)\n\\\\\n&\\propto\np(\\Btheta\\given y_{0:T})I(|\\beta_1|&lt;1)\n.\n\\end{aligned}\n\\]\n\n\n\nKeep doing Bayes however you were doing it, but accept/reject draws of \\(\\beta_1\\) if they are outside \\([-1\\com 1]\\);\n\n\n\n\nThis puts the stationarity assumption in its proper place: as something the modeler can choose to adopt or not by incorporating it into their chosen prior."
  },
  {
    "objectID": "slides/02-ar-1-inference.html#asymptotics-1",
    "href": "slides/02-ar-1-inference.html#asymptotics-1",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Asymptotics",
    "text": "Asymptotics\nWe also have theory that says that the posterior distribution concentrates around the “true value” of the AR(1) parameters."
  },
  {
    "objectID": "slides/02-ar-1-inference.html#bayesian-inference-is-inherently-recursive",
    "href": "slides/02-ar-1-inference.html#bayesian-inference-is-inherently-recursive",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Bayesian inference is inherently recursive!",
    "text": "Bayesian inference is inherently recursive!\nAs new information arrives, the old posterior becomes the new prior, and you just keep applying Bayes’ theorem:\n\nFirst data point arrives:\n\\[\n{\\color{red}{p(\\Btheta\\given \\By_{0:1})}}\\propto p(\\By_1\\given \\By_0\\com \\Btheta)p(\\Btheta)\n\\]\n\n\nSecond data point arrives:\n\\[\n{\\color{blue}{p(\\Btheta\\given \\By_{0:2})}}\\propto p(\\By_2\\given \\By_{0:1}\\com \\Btheta)\\color{red}{p(\\Btheta\\given \\By_{0:1})}\n\\]\n\n\nThird data point arrives:\n\\[\n{\\color{green}{p(\\Btheta\\given \\By_{0:3})}}\\propto p(\\By_3\\given \\By_{0:2}\\com \\Btheta)\\color{blue}{p(\\Btheta\\given \\By_{0:2})}\n\\]\n\n\nFourth data point arrives:\n\\[\np(\\Btheta\\given \\By_{0:4})\\propto p(\\By_4\\given \\By_{0:3}\\com \\Btheta)\\color{green}{p(\\Btheta\\given \\By_{0:3})}\n\\]\n\n\nAnd on and on ad infinitum.\n\n\n\n\n\n\nJust keep turning the crank!\n\n\n\\[\n\\begin{aligned}\np(\\Btheta\\given \\By_{0:t})\\propto p(\\By_t\\given \\By_{0:t-1}\\com \\Btheta)p(\\Btheta\\given \\By_{0:t-1}).\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#mcmc-breaks-this",
    "href": "slides/02-ar-1-inference.html#mcmc-breaks-this",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "MCMC breaks this!",
    "text": "MCMC breaks this!\n\nThe inherent recursivity of Bayesian inference is most perfectly realized when you have an exponential family model with conjugate priors. As the data stream, you just update sufficient statistics;\nIf instead you use Markov chain Monte Carlo (MCMC) to approximate the posterior, that’s not recursive;\nAs new data arrive, you cannot, strictly speaking, recycle the old MCMC draws. You just have to rerun everything from scratch. Gross!\n\n\nOne of our very last topics is sequential Monte Carlo (SMC), an alternative to MCMC that seeks to reclaim the recursive promise of Bayes."
  },
  {
    "objectID": "slides/02-ar-1-inference.html#this-course-has-a-predictive-pov",
    "href": "slides/02-ar-1-inference.html#this-course-has-a-predictive-pov",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "This course has a predictive POV",
    "text": "This course has a predictive POV"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#this-course-has-a-predictive-pov-1",
    "href": "slides/02-ar-1-inference.html#this-course-has-a-predictive-pov-1",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "This course has a predictive POV",
    "text": "This course has a predictive POV\n\nWe will study specific models and how to estimate them, but this is only a means to an end;\nWe don’t care about inference per se, and certainly won’t go anywhere near hypothesis testing;\nWe use estimated models to generate probabilistic predictions;\nOnce the predictions have been generated, we don’t necessarily care where they came from;\nWe evaluate them in a largely model-agnostic way, looking at historical performance on real data."
  }
]