[
  {
    "objectID": "syllabus/syllabus.html",
    "href": "syllabus/syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Class meeting: MoWe 11:45 AM - 1:00 PM in Old Chem 201;\nWhat the course catalog says: This is an introductory course in time series analysis with a focus on applications. Two basic approaches, including time domain and frequency domain methods, will be covered. A modern time-frequency analysis approach to study nonstationary time series analysis will also be introduced. The main goal is to guide the students to appreciate the main issues involved in time series analysis and solve practical challenges, particularly those coming from the high-frequency and ultra-long biomedical time series. The primary audience for this course is graduate students in statistics;\nWhat JZ says: Modern data science environments often feature high volumes of dependent, streaming data. Central banks, hedge funds, e-commerce sites, energy grids, meteorological stations, disease count trackers, radar systems, and autonomous vehicles are all sequentially analyzing a stream of incoming time series data and using it to forecast the future in real-time. But these forecasts are often more than just point predictions; they take the form of interval and density predictions that guide decision-making by incorporating many sources of uncertainty. This is what our course is about.\nA time series is a set of measurements collected over time. We model these data as a sequence of dependent random variables, and a time series model is their joint probability distribution. In this course, we will study many of the main methods for specifying this distribution: autoregressive moving average (ARMA) models, vector autoregressions (VARs), dynamic linear models (DLMs), hidden Markov models (HMMs), etc. Throughout, we will focus on sequential inference (recursive estimation when the data are streaming) and probabilistic prediction (point, interval, and density forecasting). Classical and Bayesian approaches to inference will be discussed, but the latter will be emphasized.\nPrerequisite: STA 521; Corequisite: STA 532.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#course-description",
    "href": "syllabus/syllabus.html#course-description",
    "title": "Syllabus",
    "section": "",
    "text": "Class meeting: MoWe 11:45 AM - 1:00 PM in Old Chem 201;\nWhat the course catalog says: This is an introductory course in time series analysis with a focus on applications. Two basic approaches, including time domain and frequency domain methods, will be covered. A modern time-frequency analysis approach to study nonstationary time series analysis will also be introduced. The main goal is to guide the students to appreciate the main issues involved in time series analysis and solve practical challenges, particularly those coming from the high-frequency and ultra-long biomedical time series. The primary audience for this course is graduate students in statistics;\nWhat JZ says: Modern data science environments often feature high volumes of dependent, streaming data. Central banks, hedge funds, e-commerce sites, energy grids, meteorological stations, disease count trackers, radar systems, and autonomous vehicles are all sequentially analyzing a stream of incoming time series data and using it to forecast the future in real-time. But these forecasts are often more than just point predictions; they take the form of interval and density predictions that guide decision-making by incorporating many sources of uncertainty. This is what our course is about.\nA time series is a set of measurements collected over time. We model these data as a sequence of dependent random variables, and a time series model is their joint probability distribution. In this course, we will study many of the main methods for specifying this distribution: autoregressive moving average (ARMA) models, vector autoregressions (VARs), dynamic linear models (DLMs), hidden Markov models (HMMs), etc. Throughout, we will focus on sequential inference (recursive estimation when the data are streaming) and probabilistic prediction (point, interval, and density forecasting). Classical and Bayesian approaches to inference will be discussed, but the latter will be emphasized.\nPrerequisite: STA 521; Corequisite: STA 532.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#teaching-team",
    "href": "syllabus/syllabus.html#teaching-team",
    "title": "Syllabus",
    "section": "Teaching team",
    "text": "Teaching team\n\n\n\n\n\n\n\n\n\nMug\nName\nRole\nOffice Hours\n\n\n\n\n\nLi, Aihua\nTA\nThu 10:30 AM - 12:30 PM\nremote (Zoom link on Canvas)\n\n\n\nZito, John\nInstructor\nTue 1:00 PM - 3:00 PM\nor by appointment\nOld Chem 207",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#textbooks",
    "href": "syllabus/syllabus.html#textbooks",
    "title": "Syllabus",
    "section": "Textbooks",
    "text": "Textbooks\nThe course does not have a designated textbook, and you are not required to purchase anything, but here are several you should know about:\n\n[SS] Time Series Analysis and Its Applications, 5e by Robert Shumway and David Stoffer (free pdf);\n[Skä] Bayesian Filtering and Smoothing, 1e by Simo Särkkä (free pdf);\n[WH] Bayesian Forecasting and Dynamic Models, 2e by Mike West and Jeff Harrison (free pdf);\n[PPC] Dynamic Linear Models with R by Giovanni Petris, Sonia Petrone, and Patrizia Campagnoli (free pdf);\n[HA] Forecasting: Principles and Practice, 3e by Rob Hyndman and George Athanasopoulos (online edition);\n[PFW] Time Series: Modeling, Computation, and Inference, 2e by Raquel Prado, Marco Ferreira, and Mike West;\n[Ham] Time Series Analysis by James Hamilton;\n[DK] Time Series Analysis by State Space Methods, 2e by James Durbin and Siem Jan Koopman;\n[BD] Time Series: Theory and Methods by Peter Brockwell and Richard Davis.\n\nI will list readings in the PREPARE column of the course schedule.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#assignments-and-grading",
    "href": "syllabus/syllabus.html#assignments-and-grading",
    "title": "Syllabus",
    "section": "Assignments and grading",
    "text": "Assignments and grading\nThere are four course components, weighted as follows:\n\n\n\n\nCategory\nPercentage\n\n\n\n\nProblem Sets\n25%\n\n\nExam 1\n25%\n\n\nExam 2\n25%\n\n\nProject\n25%\n\n\n\n\nYour final letter grade will be determined based on the usual thresholds, which will never move upward but may move downward in your favor:\n\n\n\n\nLetter Grade\nFinal Course Grade\n\n\n\n\nA+\n&gt;= 97\n\n\nA\n93 - 96.99\n\n\nA-\n90 - 92.99\n\n\nB+\n87 - 89.99\n\n\nB\n83 - 86.99\n\n\nB-\n80 - 82.99\n\n\nC+\n77 - 79.99\n\n\nC\n73 - 76.99\n\n\nC-\n70 - 72.99\n\n\nD+\n67 - 69.99\n\n\nD\n63 - 66.99\n\n\nD-\n60 - 62.99\n\n\nF\n&lt; 60\n\n\n\n\nHere are some details about the course assignments:\n\nProblem sets: there will be six in total, with one due roughly every two weeks;\nExams: there will be two written, in-class exams. The only resource you are allowed is both sides of one 8.5” x 11” sheet of notes prepared by you. The exam dates are…\n\nWednesday October 8 11:45 AM - 12:00 PM in Old Chem 201;\nMonday November 24 11:45 AM - 12:00 PM in Old Chem 201;\n\nFinal project: to end the semester, each student will write a final report that applies or extends ideas from the course. This is due at 9AM on Wednesday December 10. Full details about the project will be announced after the first exam.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#communication",
    "href": "syllabus/syllabus.html#communication",
    "title": "Syllabus",
    "section": "Communication",
    "text": "Communication\nIf you wish to ask content-related questions in writing, please do not do so via e-mail. Instead, please use the course discussion forum Ed Discussion. That way all members of the teaching team can see your question, and all students can benefit from the ensuing discussion. You are also encouraged to answer one another’s questions.\nIf you have questions about personal matters that may not be appropriate for the public course forum (e.g. illness, accommodations, etc), then please e-mail the instructor directly (john.zito@duke.edu).\n\n\n\n\n\n\nNote\n\n\n\nYou can ask questions anonymously on Ed. The teaching team will still know your identity, but your peers will not.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#late-work-and-extensions",
    "href": "syllabus/syllabus.html#late-work-and-extensions",
    "title": "Syllabus",
    "section": "Late work and extensions",
    "text": "Late work and extensions\nNo late work will be accepted unless you request an extension in advance by e-mailing the instructor directly (john.zito@duke.edu). All reasonable requests will be entertained, but extensions will not be long.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#attendance",
    "href": "syllabus/syllabus.html#attendance",
    "title": "Syllabus",
    "section": "Attendance",
    "text": "Attendance\nLive your life. Attendance is not strictly required for any of the class meetings. The responsibility lies with us to make class meetings sufficiently engaging and informative that you choose to attend.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#collaboration",
    "href": "syllabus/syllabus.html#collaboration",
    "title": "Syllabus",
    "section": "Collaboration",
    "text": "Collaboration\nYou are enthusiastically encouraged to work together and help one another on labs and problem sets. I would much rather you consult your peers than some daft language model. What I ask is that you acknowledge your collaborators. So, at the end of each problem in your write-up, leave a comment like “Ursula, Ignatius and I worked together on this problem,” or “Ethel explained to me how to do part b.” You are not being judged based on your acknowledgements, and there is no penalty for getting “too much” help from others. If you omit acknowledgements, I will assume you did your work solo. If your classmates tell a different story, I will have questions. But otherwise, so long as you are thorough and honest, there will be no problems.\nHaving said all of that, you should not be crassly handing your solutions to others for them to brainlessly copy. This is plagiarism, and all involved will earn a zero on the assignment and be referred to the conduct office, both sharers and recipients alike. The write-up you submit must be your own work.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#use-of-outside-resources-including-ai",
    "href": "syllabus/syllabus.html#use-of-outside-resources-including-ai",
    "title": "Syllabus",
    "section": "Use of outside resources, including AI",
    "text": "Use of outside resources, including AI\nThere are at least two reasons you might seek outside resources:\n\n✅ Extra practice or alternative instruction: Go crazy. Knock yourself out. Have a ball. The internet includes many good (and horrible) resources for learning this material, so if you find something that really resonates with you, have at it;\n❌ Doing the problems for you: If you find a solution online, or ask a language model to generate one, and you copy it down and submit it as your own work, that is plagiarism. If we detect it, you will earn a zero for that part of your write-up.\n\n“Using ChatGPT to complete assignments is like bringing a forklift into the weight room; you will never improve your cognitive fitness that way.” Furthermore, 50% of your final course grade is determined by your performance on old school, no-tech exams. As such, if you outsource all of your thinking to an AI, you will probably fail both exams. To avoid this, I suggest you refrain from using language models to do the problems for you.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#regrade-requests",
    "href": "syllabus/syllabus.html#regrade-requests",
    "title": "Syllabus",
    "section": "Regrade requests",
    "text": "Regrade requests\nIf you receive a graded assignment back, and you believe that some part of it was graded incorrectly, you may dispute the grade by submitting a regrade request in Gradescope. JZ is the sole reviewer of these. Note the following:\n\nYou have one week after you receive a grade to submit a regrade request;\nYou should submit separate regrade requests for each question you wish to dispute, not a single catch-all request;\nRequests will be considered if there was an error in the grade calculation or if a correct answer was mistakenly marked as incorrect;\nRequests to dispute the number of points deducted for an incorrect response will not be considered.\n\n\n\n\n\n\n\nWarning\n\n\n\nA regrade request can result in your grade going up, staying the same, or going down if we determine that, in fact, the original grader was too lenient.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#duke-community-standard",
    "href": "syllabus/syllabus.html#duke-community-standard",
    "title": "Syllabus",
    "section": "Duke Community Standard",
    "text": "Duke Community Standard\nDuke University is a community dedicated to scholarship, leadership, and service and to the principles of honesty, fairness, respect, and accountability. Members of this community commit to reflect upon and uphold these principles in all academic and non-academic endeavors, and to protect and promote a culture of integrity.\nDuke University has high expectations for students’ scholarship and conduct. In accepting admission, students indicate their willingness to subscribe to and be governed by the rules and regulations of the university, which flow from the Duke Community Standard (DCS).\nRegardless of course delivery format, it is the responsibility of all students to understand and follow all Duke policies, including but not limited to the academic integrity policy (e.g., completing one’s own work, following proper citation of sources, adhering to guidance around group work projects, and more). Ignoring these requirements is a violation of the DCS.\nStudents can direct any questions or concerns regarding academic integrity to the Office of Student Conduct and Community Standards at conduct@duke.edu and can access the DCS guide at https://dukecommunitystandard.students.duke.edu/.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "slides/07-model-selection.html#nber-nsf-time-series-conference",
    "href": "slides/07-model-selection.html#nber-nsf-time-series-conference",
    "title": "Model selection and combination",
    "section": "NBER-NSF Time Series Conference",
    "text": "NBER-NSF Time Series Conference\nSeptember 19 - 20 at Rutgers:\n\nnames to know: Richard Davis, Ruey Tsay, Rong Chen;\nvery academic;\nskews towards economics and finance;\na lot of frequentist asymptotics;\nplenty of “score-driven” stuff, factor models, (G)ARCH;\nnot much frequency-domain time series;\nnot much modern AI/ML;\nI get a kick out of the stuff where \\(\\By_t\\) lives on a weird manifold."
  },
  {
    "objectID": "slides/07-model-selection.html#arp",
    "href": "slides/07-model-selection.html#arp",
    "title": "Model selection and combination",
    "section": "AR(p)",
    "text": "AR(p)\nThe autoregression of order \\(p\\), or AR(\\(p\\)):\n\n\\[\n\\begin{aligned}\ny_t\n&=\n\\beta_0\n+\n\\beta_1\ny_{t-1}\n+\n\\beta_2\ny_{t-2}\n+\n\\cdots\n+\n\\beta_p\ny_{t-p}\n+\n\\varepsilon_t,\n\\quad\n\\varepsilon_t\\iid\\text{N}(0\\com\\sigma^2).\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/07-model-selection.html#lag-order-selection",
    "href": "slides/07-model-selection.html#lag-order-selection",
    "title": "Model selection and combination",
    "section": "Lag order selection",
    "text": "Lag order selection\nThe lag order \\(p\\) is a tuning parameter we must select:\n\\[\n\\hat{p}=\\argmin{p=1\\com 2\\com ...\\com \\bar{p}}\\, \\hat{C}(p).\n\\]\nOptions:\n\nAkaike information criterion (AIC);\nBayesian information criterion (BIC);\nLeave-future-out cross-validation (LFO-CV)."
  },
  {
    "objectID": "slides/07-model-selection.html#uncertainty",
    "href": "slides/07-model-selection.html#uncertainty",
    "title": "Model selection and combination",
    "section": "Uncertainty",
    "text": "Uncertainty\n\nThere is estimation uncertainty associated with lag order selection;\nIdeally this would be propagated to our forecast distributions, prediction intervals, etc;\nThis is challenging to achieve, and the gains may not be tremendous;\nIf you are dead set on making it happen…"
  },
  {
    "objectID": "slides/07-model-selection.html#a-model",
    "href": "slides/07-model-selection.html#a-model",
    "title": "Model selection and combination",
    "section": "A model",
    "text": "A model\nYou have some model named \\(M\\):\n\\[\n\\begin{aligned}\n\\Btheta&\\sim p(\\Btheta\\given M)\\\\\n\\By\\given \\Btheta&\\sim p(\\By\\given \\Btheta\\com M).\n\\end{aligned}\n\\]\n\nExample:\n\\[\n\\begin{aligned}\n\\Bbeta\\com\\sigma^2&\\sim \\text{NIG}_p(a_0\\com b_0\\com \\Bm_0\\com\\BH_0)\\\\\ny_t\\given y_{(1-p):(t-1)}\\com\\Bbeta\\com\\sigma^2&\\sim \\text{N}\\left(\\sum\\limits_{l=1}^p\\beta_ly_{t-l}\\com\\sigma^2\\right)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/07-model-selection.html#a-set-of-models",
    "href": "slides/07-model-selection.html#a-set-of-models",
    "title": "Model selection and combination",
    "section": "A set of models",
    "text": "A set of models\nYou have a finite set of possible models:\n\\[\n\\mathcal{M}=\\{M_1\\com M_2\\com ...\\com M_K\\}.\n\\]\n\nYou place a prior on this set:\n\\[\nP(M_1)+P(M_2)+...+P(M_K)=1.\n\\]\n\n\nUniform is a common choice: \\(P(M_k)=1/K\\)."
  },
  {
    "objectID": "slides/07-model-selection.html#bayesian-model-selection",
    "href": "slides/07-model-selection.html#bayesian-model-selection",
    "title": "Model selection and combination",
    "section": "Bayesian Model Selection",
    "text": "Bayesian Model Selection\nWe want to compare models \\(M_1, M_2, \\dots, M_K\\) given data \\(y\\).\nThe posterior model probability is\n\\[\nP(M_k \\mid y) = \\frac{p(y \\mid M_k) P(M_k)}{\\sum_{j=1}^K p(y \\mid M_j) P(M_j)}\n\\]\nwhere \\(p(y \\mid M_k)\\) is the marginal likelihood."
  },
  {
    "objectID": "slides/07-model-selection.html#marginal-likelihood",
    "href": "slides/07-model-selection.html#marginal-likelihood",
    "title": "Model selection and combination",
    "section": "Marginal Likelihood",
    "text": "Marginal Likelihood\nFor model \\(M_k\\) with parameter \\(\\theta_k\\):\n\\[\nP(y \\mid M_k) = \\int p(y \\mid \\theta_k, M_k) p(\\theta_k \\mid M_k) \\, d\\theta_k\n\\]\nThis integral is usually hard to compute exactly."
  },
  {
    "objectID": "slides/07-model-selection.html#we-can-do-it-with-conjugate-priors",
    "href": "slides/07-model-selection.html#we-can-do-it-with-conjugate-priors",
    "title": "Model selection and combination",
    "section": "We can do it with conjugate priors",
    "text": "We can do it with conjugate priors\n\\[\np(y_{1:T}\\mid \\text{lags}\\com p)=\\frac{1}{(2\\pi)^{T/2}}\\sqrt{\\frac{\\det(\\boldsymbol\\BH_0)}{\\det(\\boldsymbol\\BH_T)}} \\cdot \\frac{b_0^{a_0}}{b_T^{a_T}} \\cdot \\frac{\\Gamma(a_n)}{\\Gamma(a_0)}\n\\]"
  },
  {
    "objectID": "slides/07-model-selection.html#laplace-approximation-idea",
    "href": "slides/07-model-selection.html#laplace-approximation-idea",
    "title": "Model selection and combination",
    "section": "Laplace Approximation Idea",
    "text": "Laplace Approximation Idea\nAssume \\(n\\) is large and the posterior is peaked. Let \\(\\hat \\theta_k\\) be the MLE under \\(M_k\\). Then\n\\[\n\\int p(y \\mid \\theta_k, M_k) p(\\theta_k \\mid M_k) \\, d\\theta_k\n\\approx p(y \\mid \\hat\\theta_k, M_k) \\cdot (2 \\pi)^{p_k/2} |\\hat \\Sigma_k|^{1/2}\n\\]\nwhere \\(p_k\\) is the number of parameters in \\(M_k\\), and \\(\\hat \\Sigma_k\\) is the estimated covariance of \\(\\hat\\theta_k\\).\n\nLaplace approximation: use multivariate normal to aproximate posterior."
  },
  {
    "objectID": "slides/07-model-selection.html#log-marginal-likelihood-approximation",
    "href": "slides/07-model-selection.html#log-marginal-likelihood-approximation",
    "title": "Model selection and combination",
    "section": "Log Marginal Likelihood Approximation",
    "text": "Log Marginal Likelihood Approximation\nTake logs and keep leading terms for large \\(n\\):\n\\[\n\\ln p(y \\mid M_k) \\approx \\ln p(y \\mid \\hat\\theta_k, M_k) - \\frac{p_k}{2} \\log n + \\text{constant}\n\\]\nThe constant does not depend on the model."
  },
  {
    "objectID": "slides/07-model-selection.html#definition-of-bic",
    "href": "slides/07-model-selection.html#definition-of-bic",
    "title": "Model selection and combination",
    "section": "Definition of BIC",
    "text": "Definition of BIC\nThe Bayesian Information Criterion (BIC) is defined as\n\\[\n\\mathrm{BIC}_k = -2 \\ln p(y \\mid \\hat\\theta_k, M_k) + p_k \\log n\n\\]\nso that smaller BIC means higher posterior probability approximately."
  },
  {
    "objectID": "slides/07-model-selection.html#posterior-model-probability-approximation",
    "href": "slides/07-model-selection.html#posterior-model-probability-approximation",
    "title": "Model selection and combination",
    "section": "Posterior Model Probability Approximation",
    "text": "Posterior Model Probability Approximation\nUsing BIC, we have:\n\\[\nP(M_k \\mid y) \\approx \\frac{\\exp(-\\mathrm{BIC}_k/2) P(M_k)}{\\sum_j \\exp(-\\mathrm{BIC}_j/2) P(M_j)}\n\\]\nHence BIC gives a simple way to compare models using data likelihood and number of parameters."
  },
  {
    "objectID": "slides/07-model-selection.html#whats-bayesian-about-bic",
    "href": "slides/07-model-selection.html#whats-bayesian-about-bic",
    "title": "Model selection and combination",
    "section": "What’s “Bayesian” about BIC?",
    "text": "What’s “Bayesian” about BIC?\n\nPosterior model probability requires integrating over parameters.\nLaplace approximation for large \\(n\\) gives a simple formula.\nLeads to BIC: penalizes log-likelihood by number of parameters.\nSmaller BIC ≈ higher posterior probability."
  },
  {
    "objectID": "slides/02-ar-1-inference.html#the-simplest-non-trivial-time-series-model",
    "href": "slides/02-ar-1-inference.html#the-simplest-non-trivial-time-series-model",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "The simplest non-trivial time series model",
    "text": "The simplest non-trivial time series model\nThe autoregression of order 1, or AR(1):\n\n\\[\n\\begin{aligned}\ny_t\n&=\n\\beta_0\n+\n\\beta_1\ny_{t-1}\n+\n\\varepsilon_t,\n\\quad\n\\varepsilon_t\\iid\\text{N}(0\\com\\sigma^2)\n\\\\\ny_0\n&\\sim\n\\text{N}(\\mu_0\\com \\initvar),\n\\end{aligned}\n\\]\n\n\nThat’s the recursive form. It implies a joint distribution written marginal-conditional style:\n\n\n\\[\n\\begin{aligned}\np(y_{0:T})\n&=\np(y_0)\n\\prod_{t=1}^T\np(y_t\\given y_{0:t-1})\n\\\\\n&=\np(y_0)\n\\prod_{t=1}^T\np(y_t\\given y_{t-1}).\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#the-ar1-joint-distribution",
    "href": "slides/02-ar-1-inference.html#the-ar1-joint-distribution",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "The AR(1) joint distribution",
    "text": "The AR(1) joint distribution\nJoint distribution:\n\\[\n\\begin{bmatrix}\ny_0 & y_1 & \\cdots & y_T\n\\end{bmatrix}^\\tr\n\\sim\\text{N}_{T+1}\\left(\\Bmu\\com \\BSigma\\right).\n\\]\n\nMoments:\n\\[\n\\begin{aligned}\nE(y_t)\n&=\n\\beta_0\\sum\\limits_{i=0}^{t-1}\\beta_1^i\n+\n\\beta_1^t\\mu_0\n\\\\\n\\var(y_t)\n&=\n\\sigma^2\n\\sum\\limits_{i=0}^{t-1}\\beta_1^{2i}+\n\\beta_1^{2t}\\initvar\n\\\\\n\\cov(y_t\\com y_s)\n&=\n\\begin{cases}\n\\beta_1^{s-t}\\var(y_t) & t\\leq s\\\\\n\\beta_1^{t-s}\\var(y_s) & s &lt; t.\n\\end{cases}\n.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#the-special-case-when-beta_11",
    "href": "slides/02-ar-1-inference.html#the-special-case-when-beta_11",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "The special case when \\(|\\beta_1|<1\\)\n",
    "text": "The special case when \\(|\\beta_1|&lt;1\\)\n\nThe mean and variance are time-invariant, and the covariance structure is shift-invariant:\n\n\\[\n\\begin{aligned}\nE(y_t)\n&=\n\\frac{\\beta_0}{1-\\beta_1}\n\\\\\n\\var(y_t)\n&=\n\\frac{\\sigma^2}{1-\\beta_1^2}\n\\\\\n\\gamma(h)\n&=\n\\cov(y_{t+h}\\com y_t)\n=\n\\beta_1^{h}\\frac{\\sigma^2}{1-\\beta_1^2}.\n\\end{aligned}\n\\]\n\n\nThe common marginal shared by all \\(y_t\\) is called the stationary distribution:\n\n\n\\[\ny_t\\sim\\text{N}\\left(\\frac{\\beta_0}{1-\\beta_1}\\com \\frac{\\sigma^2}{1-\\beta_1^2}\\right).\n\\]\n\n\nSo “did: dependent but identically distributed.”"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#a-neat-fact-to-notice",
    "href": "slides/02-ar-1-inference.html#a-neat-fact-to-notice",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "A neat fact to notice",
    "text": "A neat fact to notice\nNotice:\n\\[\n\\begin{bmatrix}\ny_0\n\\\\\ny_1\n\\\\\ny_2\n\\\\\ny_3\n\\end{bmatrix}\n\\sim\n\\text{N}_{4}\n\\left(\n\\frac{\\beta_0}{1-\\beta_1}\\mathbf{1}_{4}\n\\com\n\\begin{bmatrix}\n\\color{red}{\\gamma(0)} & \\color{blue}{\\gamma(1)} & \\color{orange}{\\gamma(2)} & \\color{green}{\\gamma(3)}\n\\\\\n\\color{blue}{\\gamma(1)} & \\color{red}{\\gamma(0)} & \\color{blue}{\\gamma(1)} & \\color{orange}{\\gamma(2)}\n\\\\\n\\color{orange}{\\gamma(2)} & \\color{blue}{\\gamma(1)} & \\color{red}{\\gamma(0)} & \\color{blue}{\\gamma(1)}\n\\\\\n\\color{green}{\\gamma(3)}  & \\color{orange}{\\gamma(2)} & \\color{blue}{\\gamma(1)} & \\color{red}{\\gamma(0)}\n\\end{bmatrix}\n\\right)\n\\]\n\nThe covariance is an example of a Toeplitz matrix."
  },
  {
    "objectID": "slides/02-ar-1-inference.html#this-is-all-an-example-of-strict-stationarity",
    "href": "slides/02-ar-1-inference.html#this-is-all-an-example-of-strict-stationarity",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "This is all an example of (strict) stationarity",
    "text": "This is all an example of (strict) stationarity\nA joint distribution is (strictly) stationary if it is “shift invariant”:\n\\[\n\\{y_{t_1}\\com y_{t_2}\\com ...\\com y_{t_n}\\}\\overset{d}{=}\\{y_{t_1+h}\\com y_{t_2+h}\\com ...\\com y_{t_n+h}\\}.\n\\]\nThe Gaussian AR(1) with \\(|\\beta_1|&lt;1\\) has this property."
  },
  {
    "objectID": "slides/02-ar-1-inference.html#estimating-equations",
    "href": "slides/02-ar-1-inference.html#estimating-equations",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Estimating equations",
    "text": "Estimating equations\nIf the AR(1) is stationary, then\n\\[\n\\begin{aligned}\n\\gamma(1)&=\\beta_1\\gamma(0) &&\\implies\\beta_1=\\frac{\\gamma(1)}{\\gamma(0)}\\\\\n\\mu&=\\frac{\\beta_0}{1-\\beta_1}&&\\implies\\beta_0=(1-\\beta_1)\\mu\\\\\n\\gamma(0)&=\\frac{\\sigma^2}{1-\\beta_1^2}&&\\implies \\sigma^2=(1-\\beta_1^2)\\gamma(0).\n\\end{aligned}\n\\]\nEverywhere you see a “population” expected value, plug in the corresponding sample average."
  },
  {
    "objectID": "slides/02-ar-1-inference.html#yule-walker",
    "href": "slides/02-ar-1-inference.html#yule-walker",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Yule-Walker",
    "text": "Yule-Walker\nMethod of moments estimators:\n\n\\[\n\\begin{aligned}\n\\hat{\\beta}_1&=\\frac{\\hat{\\gamma}_T(1)}{\\hat{\\gamma}_T(0)}\n\\\\\n\\hat{\\beta}_0&=(1-\\hat{\\beta}_1)\\hat{\\mu}_T\\\\\n\\hat{\\sigma^2_T}&=(1-\\hat{\\beta}_1^2)\\hat{\\gamma}_T(0).\n\\end{aligned}\n\\]\n\n\nThis slide is rank nonsense if the AR(1) isn’t stationary."
  },
  {
    "objectID": "slides/02-ar-1-inference.html#news-flash-beta_1-is-a-correlation",
    "href": "slides/02-ar-1-inference.html#news-flash-beta_1-is-a-correlation",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "News flash: \\(\\beta_1\\) is a correlation!",
    "text": "News flash: \\(\\beta_1\\) is a correlation!\n\nNote:\n\n\n\\[\n\\begin{aligned}\n\\beta_1 = \\frac{\\gamma(1)}{\\gamma(0)}\n&=\\frac{\\cov(y_{t+1}\\com y_t)}{\\var(y_t)}\n\\\\\n&=\\frac{\\cov(y_{t+1}\\com y_t)}{\\text{sd}(y_{t})\\text{sd}(y_t)}\n\\\\\n&=\\frac{\\cov(y_{t+1}\\com y_t)}{\\text{sd}(y_{t+1})\\text{sd}(y_t)}\n\\\\\n&=\\text{corr}(y_{t+1}\\com y_t).\n\\end{aligned}\n\\]\n\n\nSo \\(-1&lt; \\beta_1&lt; 1\\) is the lag-1 autocorrelation, and \\(\\hat{\\beta}_1\\) is the sample version."
  },
  {
    "objectID": "slides/02-ar-1-inference.html#asymptotics",
    "href": "slides/02-ar-1-inference.html#asymptotics",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Asymptotics",
    "text": "Asymptotics\n\n\n\n\n\n\nShumway and Stoffer (2025) Property 3.7\n\n\nFor a stationary AR(1), the Yuke-Walker estimator has:\n\\[\n\\sqrt{T}(\\hat{\\beta}_1-\\beta_1)\\cd\\N\\left(0\\com\\frac{\\sigma^2}{\\gamma(0)}\\right).\n\\] And note in this case that \\(\\sigma^2 / \\gamma(0)=1-\\beta_1^2\\). The result remains true if you plug in the estimator of the asymptotic variance.\n\n\n\n\nAsymptotically valid \\(100\\times(1-\\alpha)\\%\\) confidence interval:\n\\[\n\\hat{\\beta}_1\\pm z_{1-\\alpha/2}\\sqrt{\\frac{1-\\hat{\\beta}_1^2}{T}}.\n\\]"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#simulating-the-sampling-distribution",
    "href": "slides/02-ar-1-inference.html#simulating-the-sampling-distribution",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Simulating the sampling distribution",
    "text": "Simulating the sampling distribution\nWe will do a lot of this today:\n\\[\n\\begin{matrix}\n\\text{0. Ground truth} &&& \\text{AR(1)} && \\\\\n&\\swarrow &\\swarrow& \\cdots &\\searrow&\\searrow \\\\\n\\text{1. Many datasets}&y_{0:T}^{(1)} &y_{0:T}^{(2)}& \\cdots &y_{0:T}^{(k-1)}&y_{0:T}^{(k)} \\\\\n&\\downarrow &\\downarrow& \\cdots &\\downarrow&\\downarrow \\\\\n\\text{2. Many estimates}&\\hat{\\beta}_1^{(1)} &\\hat{\\beta}_1^{(2)}& \\cdots &\\hat{\\beta}_1^{(k-1)}&\\hat{\\beta}_1^{(k)} \\\\\n\\end{matrix}\n\\]\nA histogram or boxplot of the \\(\\hat{\\beta}_1^{(j)}\\) approximates the finite-\\(T\\) sampling distribution of the estimator."
  },
  {
    "objectID": "slides/02-ar-1-inference.html#what-happens-for-beta_1-close-to-pm1",
    "href": "slides/02-ar-1-inference.html#what-happens-for-beta_1-close-to-pm1",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "What happens for \\(\\beta_1\\) close to \\(\\pm1\\)?",
    "text": "What happens for \\(\\beta_1\\) close to \\(\\pm1\\)?\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\n# app.R\nlibrary(shiny)\n\n# ---------- Configuration ----------\nSAMPLE_SIZES &lt;- seq(10, 200, by = 20)   # smaller default for speed\nNSIM &lt;- 500                             # number of simulated AR(1) paths per sample size\nY_LIMITS &lt;- c(-1, 1)\n# ------------------------------------\n\n# Yule-Walker AR(1) estimator\nyule_walker_ar1 &lt;- function(x) {\n  n &lt;- length(x)\n  x &lt;- x - mean(x)  # center\n  gamma0 &lt;- mean(x^2)\n  gamma1 &lt;- mean(x[-1] * x[-n])\n  gamma1 / gamma0\n}\n\n# Simulate one stationary AR(1) with variance 1\nsimulate_ar_1 &lt;- function(T, b0, b1, s, m0, s0){\n  y &lt;- numeric(T)\n  y[1] &lt;- rnorm(1, m0, s0)\n  for(t in 2:T){\n    y[t] &lt;- b0 + b1 * y[t - 1] + rnorm(1, 0, s)\n  }\n  return(y)\n}\n\nui &lt;- fluidPage(\n  titlePanel(\"Sampling distribution of Yule–Walker AR(1) estimator\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"b1\", \"True value of β₁:\",\n                  min = -0.99, max = 0.99, value = 0.5, step = 0.01)\n    ),\n    mainPanel(\n      plotOutput(\"boxPlot\", height = \"600px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  \n  output$boxPlot &lt;- renderPlot({\n    b0 &lt;- 0\n    b1 &lt;- input$b1\n    s &lt;- 1\n    m0 &lt;- b0 / (1 - b1)\n    s0 &lt;- s / sqrt(1 - b1^2)\n    sizes &lt;- SAMPLE_SIZES\n    nsim &lt;- NSIM\n    \n    # Collect estimates in a list, one element per sample size\n    est_list &lt;- vector(\"list\", length(sizes))\n    \n    for (i in seq_along(sizes)) {\n      n &lt;- sizes[i]\n      phi_hats &lt;- numeric(nsim)\n      for (s in 1:nsim) {\n        x &lt;- simulate_ar_1(n, b0, b1, s, m0, s0)#simulate_ar1(n, phi)\n        phi_hats[s] &lt;- yule_walker_ar1(x)\n      }\n      est_list[[i]] &lt;- phi_hats\n    }\n    \n    # Draw boxplots side by side\n    par()\n    boxplot(est_list,\n            names = sizes,\n            ylim = Y_LIMITS,\n            xlab = \"Sample size T\",\n            ylab = \"Estimate\",\n            main = paste(\"True value: \", b1),\n            col = \"lightgray\", pch = 19)\n    \n    abline(h = b1, col = \"red\", lty = 2, lwd = 2)\n  })\n}\n\nshinyApp(ui, server)"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#assumptions",
    "href": "slides/02-ar-1-inference.html#assumptions",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Assumptions",
    "text": "Assumptions\n\nNormality was not essential, but stationarity was crucial;\n\nWhat if we flip that?\n\nFirmly assume normality;\nCast stationarity to the wind."
  },
  {
    "objectID": "slides/02-ar-1-inference.html#lets-improve-the-notation",
    "href": "slides/02-ar-1-inference.html#lets-improve-the-notation",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Let’s improve the notation",
    "text": "Let’s improve the notation\nThe autoregression of order 1, or AR(1):\n\n\\[\n\\begin{aligned}\ny_t\n&=\n\\beta_0\n+\n\\beta_1\ny_{t-1}\n+\n\\varepsilon_t,\n\\quad\n\\varepsilon_t\\iid\\text{N}(0\\com\\sigma^2)\n\\\\\ny_0\n&\\sim\n\\text{N}(\\mu_0\\com \\initvar),\n\\end{aligned}\n\\]\n\n\nThis implies a joint distribution governed by a finite set of static parameters \\(\\Btheta = \\begin{bmatrix}\\beta_0&\\beta_1&\\sigma^2\\end{bmatrix}^\\tr\\):\n\n\n\\[\n\\begin{aligned}\np(y_{0:T}\\given \\Btheta)\n&=\np(y_0)\n\\prod_{t=1}^T\np(y_t\\given y_{t-1}\\com\\Btheta).\n\\end{aligned}\n\\]\n\n\nViewed as a function of \\(\\Btheta\\), that’s a likelihood!\n\n\nMaximize it, or combine with a prior."
  },
  {
    "objectID": "slides/02-ar-1-inference.html#the-conditional-likelihood",
    "href": "slides/02-ar-1-inference.html#the-conditional-likelihood",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "The conditional likelihood",
    "text": "The conditional likelihood\nIn practice, we usually just condition on the initial value:\n\\[\np(y_{1:T}\\given  y_0\\com \\Btheta)\n=\n\\prod_{t=1}^T\np(y_t\\given y_{t-1}\\com\\Btheta).\n\\]\n\nDoesn’t affect results too much, but saves headache."
  },
  {
    "objectID": "slides/02-ar-1-inference.html#maximum-likelihood-estimation",
    "href": "slides/02-ar-1-inference.html#maximum-likelihood-estimation",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Maximum likelihood estimation",
    "text": "Maximum likelihood estimation\nTreating the observed data \\(y_{0:T}\\) as fixed, we want:\n\\[\n\\hat{\\Btheta}_T=\\argmax{\\Btheta}\\,p(y_{1:T}\\given y_0\\com \\Btheta).\n\\]\nTo do this, it helps to take the view that the AR(1) is “just” a simple linear regression with \\(x_t=y_{t-1}\\)."
  },
  {
    "objectID": "slides/02-ar-1-inference.html#maximum-likelihood-estimation-1",
    "href": "slides/02-ar-1-inference.html#maximum-likelihood-estimation-1",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Maximum likelihood estimation",
    "text": "Maximum likelihood estimation\nSince \\(y_t\\given y_{t-1}\\com\\Btheta\\sim\\N(\\beta_0+\\beta_1y_{t-1}\\com\\sigma^2)\\), we have\n\n\\[\n\\begin{aligned}\np(y_{1:T}\\given  y_0\\com \\Btheta)\n&=\n\\prod_{t=1}^T\np(y_t\\given y_{t-1}\\com\\Btheta)\n\\\\\n&=\n\\prod_{t=1}^T\n\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{1}{2}\\frac{(y_t-\\beta_0-\\beta_1y_{t-1})^2}{\\sigma^2}\\right)\n\\\\\n&=\n(2\\pi\\sigma^2)^{-T/2}\n\\exp\\left(-\\frac{1}{2\\sigma^2}\\sum\\limits_{t=1}^T(y_t-\\beta_0-\\beta_1y_{t-1})^2\\right)\n.\n\\end{aligned}\n\\]\n\n\nTo compute the MLE, we just treat \\(y_{0:T}\\) as fixed.\n\n\nWhere do you think this is headed?"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#stack-em-up",
    "href": "slides/02-ar-1-inference.html#stack-em-up",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Stack ’em up",
    "text": "Stack ’em up\nDefine some things:\n\n\\[\n\\begin{aligned}\n\\By_T\n&=\n\\begin{bmatrix}y_1&y_2 & \\cdots & y_T\\end{bmatrix}^\\tr\n\\\\\n\\BX_T\n&=\n\\begin{bmatrix}\n1 & 1 & \\cdots & 1 \\\\\ny_0 & y_1 & \\cdots & y_{T-1}\n\\end{bmatrix}^\\tr\n\\\\\n\\Bbeta\n&=\n\\begin{bmatrix}\\beta_0&\\beta_1\\end{bmatrix}^\\tr\n.\n\\end{aligned}\n\\]\n\n\nSo the log-likelihood is:\n\\[\n\\ln\np(y_{1:T}\\given  y_0\\com \\Btheta)\n=\n-\\frac{T}{2}\\ln(2\\pi\\sigma^2)\n-\\frac{1}{2\\sigma^2}\n||\\By_T-\\BX_T\\Bbeta||_2^2\n.\n\\]\n\n\nLook familiar?"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#were-just-back-to-regression-101",
    "href": "slides/02-ar-1-inference.html#were-just-back-to-regression-101",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "We’re just back to regression 101",
    "text": "We’re just back to regression 101\nWe have\n\\[\n\\ln\np(y_{1:T}\\given  y_0\\com \\Btheta)\n=\n-\\frac{T}{2}\\ln(2\\pi\\sigma^2)\n-\\frac{1}{2\\sigma^2}\n||\\By_T-\\BX_T\\Bbeta||_2^2\n,\n\\]\n\nand we know that\n\\[\n\\begin{aligned}\n\\hat{\\Btheta}_T\n&=\\argmax{\\Btheta}\\,\\ln p(y_{1:T}\\given y_0\\com \\Btheta)\n\\\\\n&=\\argmin{\\Btheta}\\,-\\ln p(y_{1:T}\\given y_0\\com \\Btheta).\n\\end{aligned}\n\\]\n\n\nThis gives\n\\[\n\\begin{aligned}\n\\hat{\\Bbeta}_T\n&=\n\\argmin{\\Bbeta}\n\\,||\\By_T-\\BX_T\\Bbeta||_2^2\n\\\\\n\\hat{\\sigma^2_T}\n&=\n||\\By_T-\\BX_T\\hat{\\Bbeta}_T||_2^2 / T.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#ols-for-the-ar1",
    "href": "slides/02-ar-1-inference.html#ols-for-the-ar1",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "OLS for the AR(1)",
    "text": "OLS for the AR(1)\n\n\nThe maximum (conditional) likelihood estimator in the AR(1) is the same as the ordinary least squares estimator:\n\n\n\n\\[\n\\begin{aligned}\n\\hat{\\Bbeta}_T\n&=\n(\\BX_T^\\tr\\BX_T)^{-1}\\BX_T^\\tr\\By_T\n\\\\\n\\hat{\\sigma^2_T}\n&=\n||\\By_T-\\BX_T\\hat{\\Bbeta}_T||_2^2 / T.\n\\end{aligned}\n\\]\n\n\n\nIt doesn’t matter that there’s time series dependence. Once the data are observed and fixed, everything you know about iid multiple regression applies unmodified.\n\n\n\n\nWhen we graduate to the AR(p) in a few weeks, everything looks identical. \\(\\BX_T\\) just has more columns."
  },
  {
    "objectID": "slides/02-ar-1-inference.html#if-the-data-are-streaming-apply-pset-0",
    "href": "slides/02-ar-1-inference.html#if-the-data-are-streaming-apply-pset-0",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "If the data are streaming, apply PSET 0!",
    "text": "If the data are streaming, apply PSET 0!\nDo not recompute that matrix inverse every period.\n\nInstead:\n\\[\n\\begin{aligned}\n\\hat{\\Bbeta}_t\n&=\n\\hat{\\Bbeta}_{t-1}\n+\n\\Bk_t(y_t-\\Bx_t^\\tr\\hat{\\Bbeta}_{t-1})\n\\\\\n\\Bk_t\n&=\n\\frac{\\BP_{t-1}\\Bx_t}\n{1+\\Bx_t^\\tr\\BP_{t-1}\\Bx_t}\n\\\\\n\\BP_t\n&=\n(\\BX_t^\\tr\\BX_t)^{-1}\n\\\\\n&=\n(\\BX_{t-1}^\\tr\\BX_{t-1}+\\Bx_t\\Bx_t^\\tr)^{-1}\n\\\\\n&=\n(\\BP_{t-1}+\\Bx_t\\Bx_t^\\tr)^{-1}\n\\\\\n&=\n\\BP_{t-1}\n-\n\\Bk_t\n\\Bx_t^\\tr\\BP_{t-1}\n.\n\\end{aligned}\n\\]\n\n\nKeyword: rank-1 update!"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#quick-aside",
    "href": "slides/02-ar-1-inference.html#quick-aside",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Quick aside",
    "text": "Quick aside\n\nLater in the semester, we will study something called the Kalman filter;\nThis is the beating heart of time series analysis in my humble opinion;\nIf you invest in understanding the recursive form of OLS and the Bayesian version we will see later today, you are well equipped to understand the Kalman filter;\nOnce you understand the Kalman filter, you can understand anything else in time series analysis;\n\n\n…the parts worth understanding, anyway."
  },
  {
    "objectID": "slides/02-ar-1-inference.html#asymptotics-if-the-truth-is-stationary",
    "href": "slides/02-ar-1-inference.html#asymptotics-if-the-truth-is-stationary",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Asymptotics if the truth is stationary",
    "text": "Asymptotics if the truth is stationary\n\n\n\n\n\n\nShumway and Stoffer (2025) Property 3.9\n\n\nFor a stationary AR(1), the MLE/OLS estimator has:\n\\[\n\\sqrt{T}(\\hat{\\beta}_1-\\beta_1)\\cd\\N\\left(0\\com 1-\\beta_1^2\\right).\n\\]\n\n\n\n\nSame as Yule-Walker!"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#simulation-in-the-stationary-case",
    "href": "slides/02-ar-1-inference.html#simulation-in-the-stationary-case",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Simulation in the stationary case",
    "text": "Simulation in the stationary case"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#asymptotics-if-the-truth-is-a-random-walk",
    "href": "slides/02-ar-1-inference.html#asymptotics-if-the-truth-is-a-random-walk",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Asymptotics if the truth is a random walk",
    "text": "Asymptotics if the truth is a random walk\n\nIt’s gross.\n\n\n\n\n\n\n\n\nHamilton (1994) [17.4.28]\n\n\nIf the true parameters are \\(\\beta_0=0\\) and \\(\\beta_1=1\\), then MLE/OLS estimator has \\[\nT(\\hat{\\beta}_1-1)\\cd\\frac{\\frac{1}{2}[W(1)^2-1]-W(1)\\int_0^1W(r)\\,\\dd r}{\\int_0^1 W(r)^2\\dd r-\\left(\\int_0^1 W(r)\\,\\dd r\\right)^2},\n\\]\nwhere \\(W\\) is a Wiener process (Brownian motion).\n\n\n\n\n\nSuffice it to say…the asymptotic distribution ain’t Gaussian.\n\n\nAnd the convergence rate is worse."
  },
  {
    "objectID": "slides/02-ar-1-inference.html#simulation-in-the-weird-rw-case",
    "href": "slides/02-ar-1-inference.html#simulation-in-the-weird-rw-case",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Simulation in the weird RW case",
    "text": "Simulation in the weird RW case\n\n\n\n\n\n\n\n\nDefinitely not Gaussian."
  },
  {
    "objectID": "slides/02-ar-1-inference.html#asymptotics-if-the-truth-is-rw-w-drift",
    "href": "slides/02-ar-1-inference.html#asymptotics-if-the-truth-is-rw-w-drift",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Asymptotics if the truth is RW w/ drift",
    "text": "Asymptotics if the truth is RW w/ drift\n\n\n\n\n\n\n\nHamilton (1994) [17.4.47]\n\n\nIf the true parameters are \\(\\beta_0\\neq 0\\) and \\(\\beta_1=1\\), then MLE/OLS estimator has \\[\n\\begin{bmatrix}\nT^{1/2}(\\hat{\\beta}_0-\\beta_0)\n\\\\\nT^{3/2}(\\hat{\\beta}_1-1)\n\\end{bmatrix}\n\\cd\n\\N_2\n\\left(\n\\Bzero\n\\com\n\\sigma^2\n\\begin{bmatrix}\n1 & \\beta_0/2\\\\\n\\beta_0/2 & \\beta_0^2/3\n\\end{bmatrix}^{-1}\n\\right)\n.\n\\]\n\n\n\nThings are Gaussian again!"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#simulation-in-the-rw-w-drift-case",
    "href": "slides/02-ar-1-inference.html#simulation-in-the-rw-w-drift-case",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Simulation in the RW w/ drift case",
    "text": "Simulation in the RW w/ drift case"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#asymptotics-if-the-truth-is-explosive",
    "href": "slides/02-ar-1-inference.html#asymptotics-if-the-truth-is-explosive",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Asymptotics if the truth is explosive",
    "text": "Asymptotics if the truth is explosive\n\n\n\n\n\n\n\nWhite (1958 AoMS)\n\n\nIf the true parameters are \\(\\beta_0= 0\\) and \\(|\\beta_1|&gt;1\\) and the errors are normal, then MLE/OLS estimator has \\[\n\\frac{|\\beta_1|^T}{\\beta_1^2-1}(\\hat{\\beta}_1-\\beta_1)\\cd \\text{Cauchy}.\n\\]"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#simulation-in-explosive-case",
    "href": "slides/02-ar-1-inference.html#simulation-in-explosive-case",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Simulation in explosive case",
    "text": "Simulation in explosive case"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#why-am-i-bothering-you-with-all-this",
    "href": "slides/02-ar-1-inference.html#why-am-i-bothering-you-with-all-this",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Why am I bothering you with all this?",
    "text": "Why am I bothering you with all this?\nHere’s the point:\n\nThe AR(1) has been very thoroughly studied;\nUnlike Yule-Walker, OLS “works” no matter the regime. Recall that relaxing the stationarity requirement was one of our motivations for exploring likelihood-based inference to begin with;\nNon-stationarity can make things weird and complicated, but it’s not so bad that we can’t prove theorems. Inference remains possible;\nThe math is…interesting. See Hamilton (1994) Chapter 17 if you are absolutely dying of curiosity;\nDoes any of this matter for “real-world” data analysis?\n\n\nProbably not.\n\n\nBut you know what does matter?"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#recall",
    "href": "slides/02-ar-1-inference.html#recall",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Recall",
    "text": "Recall\nGiven a prior distribution \\(p(\\Btheta)=p(\\beta_0\\com \\beta_1\\com \\sigma^2)\\) on the parameters and an observed time series \\(y_{0:T}\\), we seek to access the posterior distribution:\n\\[\np(\\Btheta\\given y_{0:T})\n=\n\\frac{p(y_{1:T}\\given y_0\\com \\Btheta)p(\\Btheta)}{p(y_{1:T}\\given y_0)}\n.\n\\]\n\nAs with MLE, everything you know about iid Bayesian regression applies pretty much unmodified."
  },
  {
    "objectID": "slides/02-ar-1-inference.html#conjugate-normal-inverse-gamma-prior",
    "href": "slides/02-ar-1-inference.html#conjugate-normal-inverse-gamma-prior",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Conjugate normal-inverse-gamma prior",
    "text": "Conjugate normal-inverse-gamma prior\nBayesian model with a conjugate prior:\n\\[\n\\begin{aligned}\n\\sigma^2\n&\\sim\n\\text{IG}(a_0\\com b_0)\n\\\\\n\\Bbeta\\given \\sigma^2\n&\\sim\n\\text{N}_2(\\Bm_0\\com\\sigma^2\\BH^{-1}_0)\n\\\\\ny_t\n\\given\ny_{t-1}\n\\com\n\\Bbeta\\com\\sigma^2\n&\\sim \\text{N}\n\\left(\n\\Bx_t^\\tr\\Bbeta\\com\\sigma^2\n\\right), && \\Bx_t=\\begin{bmatrix}1 & y_{t-1}\\end{bmatrix}^\\tr.\n\\end{aligned}\n\\]\n\nThe posterior is available in closed-form:\n\\[\n\\begin{aligned}\n\\sigma^2\\given y_{0:T}\n&\\sim\n\\text{IG}(a_T\\com b_T)\n\\\\\n\\Bbeta\\given \\sigma^2\\com y_{0:T}\n&\\sim\n\\text{N}_2(\\Bm_T\\com\\sigma^2\\BH^{-1}_T)\n\\\\\n\\\\\n\\BH_T\n&=\n\\BX_T^\\tr\\BX_T+\\BH_0\n\\\\\n\\Bm_T\n&=\n\\BH_T^{-1}(\\BX_T^\\tr\\By_T+\\BH_0\\Bm_0)\n\\\\\na_T\n&=\na_0 + T/2\n\\\\\nb_T\n&=\nb_0\n+\n(\\By_T^\\tr\\By_T+\\Bm_0^\\tr\\BH_0\\Bm_0-\\Bm_T^\\tr\\BH_T\\Bm_T)/2.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#make-it-nice-on-problem-set-1",
    "href": "slides/02-ar-1-inference.html#make-it-nice-on-problem-set-1",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Make it nice on Problem Set 1",
    "text": "Make it nice on Problem Set 1\n\nAfter \\(t-1\\) periods you have already computed\n\n\n\\[\n\\begin{aligned}\n\\sigma^2\\given y_{0:t-1}\n&\\sim\n\\text{IG}(a_{t-1}\\com b_{t-1})\n\\\\\n\\Bbeta\\given \\sigma^2\\com y_{0:t-1}\n&\\sim\n\\text{N}_2(\\Bm_{t-1}\\com\\sigma^2\\BH^{-1}_{t-1}).\n\\end{aligned}\n\\]\n\n\nThen \\(y_t\\) arrives. What’s the most efficient way to update?\n\n\n\\[\n\\begin{aligned}\n\\sigma^2\\given y_{0:t}\n&\\sim\n\\text{IG}(a_{t}\\com b_{t})\n\\\\\n\\Bbeta\\given \\sigma^2\\com y_{0:t}\n&\\sim\n\\text{N}_2(\\Bm_{t}\\com\\sigma^2\\BH^{-1}_{t}).\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#non-conjugate-normal-inverse-gamma-prior",
    "href": "slides/02-ar-1-inference.html#non-conjugate-normal-inverse-gamma-prior",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Non-conjugate normal-inverse-gamma prior",
    "text": "Non-conjugate normal-inverse-gamma prior\nInstead of \\(p(\\Bbeta\\com\\sigma^2)=p(\\Bbeta\\given\\sigma^2)p(\\sigma^2)\\), use \\(p(\\Bbeta\\com\\sigma^2)=p(\\Bbeta)p(\\sigma^2)\\):\n\n\\[\n\\begin{aligned}\n\\sigma^2\n&\\sim\n\\text{IG}(a_0\\com b_0)\n\\\\\n\\Bbeta\n&\\sim\n\\text{N}_2(\\Bm_0\\com\\BH^{-1}_0)\n\\\\\ny_t\n\\given\ny_{t-1}\n\\com\n\\Bbeta\\com\\sigma^2\n&\\sim \\text{N}\n\\left(\n\\Bx_t^\\tr\\Bbeta\\com\\sigma^2\n\\right), && \\Bx_t=\\begin{bmatrix}1 & y_{t-1}\\end{bmatrix}^\\tr.\n\\end{aligned}\n\\]\n\n\nThe full posterior is intractable, but the conditional posteriors are known:\n\n\n\\[\n\\begin{aligned}\n\\sigma^2\\given \\Bbeta\\com y_{0:T}\n&\\sim\\text{IG}(a_T\\com b_T)\\\\\n\\Bbeta\\given \\sigma^2\\com y_{0:T}\n&\\sim\\N_2(\\Bm_T\\com \\BH_T^{-1}).\n\\end{aligned}\n\\]\n\n\nAlternate sampling from them and you get a Gibbs sampler targeting \\(p(\\Bbeta\\com\\sigma^2\\given y_{0:T})\\)."
  },
  {
    "objectID": "slides/02-ar-1-inference.html#truncation-prior-to-enforce-stationarity",
    "href": "slides/02-ar-1-inference.html#truncation-prior-to-enforce-stationarity",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Truncation prior to enforce stationarity",
    "text": "Truncation prior to enforce stationarity\n\nAugment any prior with a truncation to enforce stationarity:\n\n\n\\[\n\\begin{aligned}\n\\tilde{p}(\\Btheta\\given y_{0:T})\n&\\propto\np(y_{1:T}\\given y_0\\com\\Btheta)\\tilde{p}(\\Btheta)\n\\\\\n&=\np(y_{1:T}\\given y_0\\com\\Btheta)p(\\Btheta)I(|\\beta_1|&lt;1)\n\\\\\n&\\propto\np(\\Btheta\\given y_{0:T})I(|\\beta_1|&lt;1)\n.\n\\end{aligned}\n\\]\n\n\n\nKeep doing Bayes however you were doing it, but accept/reject draws of \\(\\beta_1\\) if they are outside \\([-1\\com 1]\\);\n\n\n\n\nThis puts the stationarity assumption in its proper place: as something the modeler can choose to adopt or not by incorporating it into their chosen prior."
  },
  {
    "objectID": "slides/02-ar-1-inference.html#asymptotics-1",
    "href": "slides/02-ar-1-inference.html#asymptotics-1",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Asymptotics",
    "text": "Asymptotics\nWe also have theory that says that the posterior distribution concentrates around the “true value” of the AR(1) parameters."
  },
  {
    "objectID": "slides/02-ar-1-inference.html#bayesian-inference-is-inherently-recursive",
    "href": "slides/02-ar-1-inference.html#bayesian-inference-is-inherently-recursive",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "Bayesian inference is inherently recursive!",
    "text": "Bayesian inference is inherently recursive!\nAs new information arrives, the old posterior becomes the new prior, and you just keep applying Bayes’ theorem:\n\nFirst data point arrives:\n\\[\n{\\color{red}{p(\\Btheta\\given \\By_{0:1})}}\\propto p(\\By_1\\given \\By_0\\com \\Btheta)p(\\Btheta)\n\\]\n\n\nSecond data point arrives:\n\\[\n{\\color{blue}{p(\\Btheta\\given \\By_{0:2})}}\\propto p(\\By_2\\given \\By_{0:1}\\com \\Btheta)\\color{red}{p(\\Btheta\\given \\By_{0:1})}\n\\]\n\n\nThird data point arrives:\n\\[\n{\\color{green}{p(\\Btheta\\given \\By_{0:3})}}\\propto p(\\By_3\\given \\By_{0:2}\\com \\Btheta)\\color{blue}{p(\\Btheta\\given \\By_{0:2})}\n\\]\n\n\nFourth data point arrives:\n\\[\np(\\Btheta\\given \\By_{0:4})\\propto p(\\By_4\\given \\By_{0:3}\\com \\Btheta)\\color{green}{p(\\Btheta\\given \\By_{0:3})}\n\\]\n\n\nAnd on and on ad infinitum.\n\n\n\n\n\n\nJust keep turning the crank!\n\n\n\\[\n\\begin{aligned}\np(\\Btheta\\given \\By_{0:t})\\propto p(\\By_t\\given \\By_{0:t-1}\\com \\Btheta)p(\\Btheta\\given \\By_{0:t-1}).\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#mcmc-breaks-this",
    "href": "slides/02-ar-1-inference.html#mcmc-breaks-this",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "MCMC breaks this!",
    "text": "MCMC breaks this!\n\nThe inherent recursivity of Bayesian inference is most perfectly realized when you have an exponential family model with conjugate priors. As the data stream, you just update sufficient statistics;\nIf instead you use Markov chain Monte Carlo (MCMC) to approximate the posterior, that’s not recursive;\nAs new data arrive, you cannot, strictly speaking, recycle the old MCMC draws. You just have to rerun everything from scratch. Gross!\n\n\nOne of our very last topics is sequential Monte Carlo (SMC), an alternative to MCMC that seeks to reclaim the recursive promise of Bayes."
  },
  {
    "objectID": "slides/02-ar-1-inference.html#this-course-has-a-predictive-pov",
    "href": "slides/02-ar-1-inference.html#this-course-has-a-predictive-pov",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "This course has a predictive POV",
    "text": "This course has a predictive POV"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#this-course-has-a-predictive-pov-1",
    "href": "slides/02-ar-1-inference.html#this-course-has-a-predictive-pov-1",
    "title": "How do you do likelihood-based inference in the AR(1)?",
    "section": "This course has a predictive POV",
    "text": "This course has a predictive POV\n\nWe will study specific models and how to estimate them, but this is only a means to an end;\nWe don’t care about inference per se, and certainly won’t go anywhere near hypothesis testing;\nWe use estimated models to generate probabilistic predictions;\nOnce the predictions have been generated, we don’t necessarily care where they came from;\nWe evaluate them in a largely model-agnostic way, looking at historical performance on real data."
  },
  {
    "objectID": "slides/10-ugly-arma-theory.html#where-were-we",
    "href": "slides/10-ugly-arma-theory.html#where-were-we",
    "title": "ARMA models II",
    "section": "Where were we?",
    "text": "Where were we?\n\\[\ny_t\n=\n\\beta_0\n+\n\\underbrace{\\sum\\limits_{l=1}^p\\beta_ly_{t-l}}_{\\text{autoregressive}}\n+\n\\underbrace{\\sum_{i=1}^q\\theta_i\\varepsilon_{t-i}}_{\\text{moving average}}\n+\n\\varepsilon_t,\n\\quad\n\\varepsilon_t\n\\iid\n\\N(0\\com \\sigma^2)\n.\n\\]\n\nCan’t do anything analytically:\n\nMLE requires numerical optimization;\nBayes requires ugly MCMC;\nForecasts with correct uncertainty quantification are hard;\nBootstrapping is a pain;\nModel combination is probably not worth it."
  },
  {
    "objectID": "slides/10-ugly-arma-theory.html#the-bottom-line-at-the-top",
    "href": "slides/10-ugly-arma-theory.html#the-bottom-line-at-the-top",
    "title": "ARMA models II",
    "section": "The bottom line, at the top",
    "text": "The bottom line, at the top\n\nA stationary AR(p) can be written as an MA\\((\\infty)\\);\nAn “invertible” MA(q) can be written as an AR\\((\\infty)\\);\nA stationary and invertible ARMA(p, q) can be written as either an MA\\((\\infty)\\) or an AR\\((\\infty)\\).\n\n\nThese suggest approximations that can ease the computational burden of estimation and forecasting."
  },
  {
    "objectID": "slides/10-ugly-arma-theory.html#lag-operator",
    "href": "slides/10-ugly-arma-theory.html#lag-operator",
    "title": "ARMA models II",
    "section": "Lag operator",
    "text": "Lag operator\n\n\n\n\n\n\nDefinition\n\n\nThe lab operator \\(L\\) acts on a time series \\(y_t\\) to return its first lagged value:\n\\[\nLy_{t}=y_{t-1}.\n\\]\n\n\n\n\nImagine multiple applications:\n\\[\nL^2 y_{t}= L L y_{t}=L y_{t-1}=y_{t-2}.\n\\]\n\n\nIn general:\n\\[\nL^ky_t=y_{t-k}.\n\\]"
  },
  {
    "objectID": "slides/10-ugly-arma-theory.html#ar1",
    "href": "slides/10-ugly-arma-theory.html#ar1",
    "title": "ARMA models II",
    "section": "AR(1)",
    "text": "AR(1)\n\\[\n\\begin{aligned}\ny_t\n&=\n\\beta_1y_{t-1}+\\varepsilon_t\n\\\\\ny_t\n&=\n\\beta_1 Ly_t+\\varepsilon_t\n\\\\\ny_t\n-\n\\beta_1 Ly_t\n&=\n\\varepsilon_t\n\\\\\n(1\n-\n\\beta_1 L)y_t\n&=\n\\varepsilon_t.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/10-ugly-arma-theory.html#ar2",
    "href": "slides/10-ugly-arma-theory.html#ar2",
    "title": "ARMA models II",
    "section": "AR(2)",
    "text": "AR(2)\n\\[\n\\begin{aligned}\ny_t\n&=\n\\beta_1y_{t-1}+\\beta_2y_{t-2}+\\varepsilon_t\n\\\\\ny_t\n&=\n\\beta_1Ly_t+\\beta_2L^2y_t+\\varepsilon_t\n\\\\\ny_t\n-\n\\beta_1Ly_t-\\beta_2L^2y_t\n&=\n\\varepsilon_t\n\\\\\n(1\n-\n\\beta_1 L-\\beta_2L^2)y_t\n&=\n\\varepsilon_t.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/10-ugly-arma-theory.html#the-arp",
    "href": "slides/10-ugly-arma-theory.html#the-arp",
    "title": "ARMA models II",
    "section": "The AR(p)",
    "text": "The AR(p)\nOld way of writing it:\n\\[\ny_t=\\sum\\limits_{l=1}^p\\beta_ly_{t-l}+\\varepsilon_t.\n\\]\n\nNew way of writing it:\n\\[\n\\left(1-\\sum\\limits_{l=1}^p\\beta_lL^l\\right)y_t=\\varepsilon_t.\n\\]"
  },
  {
    "objectID": "slides/10-ugly-arma-theory.html#the-ar-polynomial",
    "href": "slides/10-ugly-arma-theory.html#the-ar-polynomial",
    "title": "ARMA models II",
    "section": "The AR polynomial",
    "text": "The AR polynomial\n\n\n\n\n\n\nDefinition\n\n\nIf \\(\\beta_{1:p}\\) are the non-zero coefficients of an AR(p), then the AR polynomial is\n\\[\n\\beta(z) = 1-\\beta_1z-\\beta_2z^2-...-\\beta_pz^p,\\quad z\\in\\mathbb{C}.\n\\]\n\n\n\n\nPlug in the lag operator \\(L\\), and we see that the AR(p) is\n\\[\n\\beta(L)y_t=\\varepsilon_t.\n\\]"
  },
  {
    "objectID": "slides/10-ugly-arma-theory.html#ma1",
    "href": "slides/10-ugly-arma-theory.html#ma1",
    "title": "ARMA models II",
    "section": "MA(1)",
    "text": "MA(1)\n\\[\n\\begin{aligned}\ny_t\n&=\n\\varepsilon_t+\\theta_1\\varepsilon_{t-1}\n\\\\\n&=\n\\varepsilon_t+\\theta_1L\\varepsilon_t\n\\\\\n&=\n(1+\\theta_1L)\\varepsilon_t.\n\\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/10-ugly-arma-theory.html#ma2",
    "href": "slides/10-ugly-arma-theory.html#ma2",
    "title": "ARMA models II",
    "section": "MA(2)",
    "text": "MA(2)\n\\[\n\\begin{aligned}\ny_t\n&=\n\\varepsilon_t+\\theta_1\\varepsilon_{t-1} + \\theta_2\\varepsilon_{t-2}\n\\\\\n&=\n\\varepsilon_t+\\theta_1L\\varepsilon_t + \\theta_2L^2\\varepsilon_{t}\n\\\\\n&=\n(1+\\theta_1L+\\theta_2L^2)\\varepsilon_t.\n\\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/10-ugly-arma-theory.html#the-maq",
    "href": "slides/10-ugly-arma-theory.html#the-maq",
    "title": "ARMA models II",
    "section": "The MA(q)",
    "text": "The MA(q)\nOld way of writing it:\n\\[\ny_t\n=\n\\sum_{i=1}^q\\theta_i\\varepsilon_{t-i}\n+\n\\varepsilon_t.\n.\n\\]\n\nNew way of writing it:\n\\[\ny_t=\\left(1+\\sum\\limits_{i=1}^q\\theta_iL^i\\right)\\varepsilon_t.\n\\]"
  },
  {
    "objectID": "slides/10-ugly-arma-theory.html#the-ma-polynomial",
    "href": "slides/10-ugly-arma-theory.html#the-ma-polynomial",
    "title": "ARMA models II",
    "section": "The MA polynomial",
    "text": "The MA polynomial\n\n\n\n\n\n\nDefinition\n\n\nIf \\(\\theta_{1:q}\\) are the non-zero coefficients of an MA(q), then the MA polynomial is\n\\[\n\\theta(z) = 1+\\theta_1z+\\theta_2z^2+...+\\theta_qz^q,\\quad z\\in\\mathbb{C}.\n\\]\n\n\n\n\nPlug in the lag operator \\(L\\), and we see that the MA(q) is\n\\[\ny_t=\\theta(L)\\varepsilon_t.\n\\]"
  },
  {
    "objectID": "slides/10-ugly-arma-theory.html#arma1-1",
    "href": "slides/10-ugly-arma-theory.html#arma1-1",
    "title": "ARMA models II",
    "section": "ARMA(1, 1)",
    "text": "ARMA(1, 1)\n\\[\n\\begin{aligned}\ny_t\n&=\n\\beta_1y_{t-1}\n+\n\\varepsilon_t+\\theta_1\\varepsilon_{t-1}\n\\\\\ny_t\n&=\n\\beta_1Ly_t\n+\n(1+\\theta_1L)\\varepsilon_t\n\\\\\n(1-\\beta_1L)\ny_t\n&=\n(1+\\theta_1L)\\varepsilon_t.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/10-ugly-arma-theory.html#armap-q",
    "href": "slides/10-ugly-arma-theory.html#armap-q",
    "title": "ARMA models II",
    "section": "ARMA(p, q)",
    "text": "ARMA(p, q)\nOld way of writing it:\n\\[\ny_t\n=\n\\sum\\limits_{l=1}^p\\beta_ly_{t-l}\n+\n\\sum_{i=1}^q\\theta_i\\varepsilon_{t-i}\n+\n\\varepsilon_t\n\\]\n\nNew way of writing it:\n\\[\n\\begin{aligned}\n\\beta(L)y_t\n&=\n\\theta(L)\\varepsilon_t.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/10-ugly-arma-theory.html#a-tale-of-two-polynomials",
    "href": "slides/10-ugly-arma-theory.html#a-tale-of-two-polynomials",
    "title": "ARMA models II",
    "section": "A tale of two polynomials",
    "text": "A tale of two polynomials\nThe ARMA(p, q):\n\\[\n\\beta(L)y_t\n=\n\\theta(L)\\varepsilon_t,\n\\quad\\varepsilon\\iid\\N(0\\com\\sigma^2).\n\\]\n\nMuch hinges on the properties of these two goofy polynomials of a complex argument:\n\\[\n\\begin{aligned}\n\\beta(z)&=1-\\sum\\limits_{l=1}^p\\beta_lz^l\\\\\n\\theta(z)&=1+\\sum\\limits_{i=1}^q\\theta_iz^i.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/10-ugly-arma-theory.html#question",
    "href": "slides/10-ugly-arma-theory.html#question",
    "title": "ARMA models II",
    "section": "Question",
    "text": "Question\n\nCan we write this?\n\\[\ny_t=\\frac{\\theta(L)}{\\beta(L)}\\varepsilon_t.\n\\]\n\n\nCan we write this?\n\\[\n\\frac{\\beta(L)}{\\theta(L)}y_t=\\varepsilon_t.\n\\]\n\n\nThe inverses \\(1/\\beta(L)\\) and \\(1/\\theta(L)\\) only converge if the respective polynomials have roots outside the unit circle. Are you tingling?"
  },
  {
    "objectID": "slides/10-ugly-arma-theory.html#example-ar1-as-mainfty",
    "href": "slides/10-ugly-arma-theory.html#example-ar1-as-mainfty",
    "title": "ARMA models II",
    "section": "Example: AR(1) as MA(\\(\\infty\\))",
    "text": "Example: AR(1) as MA(\\(\\infty\\))\nAssuming mean-zero (\\(\\beta_0=0\\)), we know\n\\[\n\\begin{aligned}\ny_1&=\\beta_1y_0+\\varepsilon_1\\\\\ny_2&=\\beta_1y_1+\\varepsilon_2\\\\\ny_3&=\\beta_1y_2+\\varepsilon_3\\\\\n&\\vdots\\\\\ny_{t-1}&=\\beta_1y_{t-2}+\\varepsilon_{t-1}\\\\\ny_t&=\\beta_1y_{t-1}+\\varepsilon_t\\\\\n&\\vdots\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/10-ugly-arma-theory.html#example-ar1-as-mainfty-1",
    "href": "slides/10-ugly-arma-theory.html#example-ar1-as-mainfty-1",
    "title": "ARMA models II",
    "section": "Example: AR(1) as MA(\\(\\infty\\))",
    "text": "Example: AR(1) as MA(\\(\\infty\\))\nApply recursive substitution:\n\\[\n\\begin{aligned}\ny_t\n&=\n\\beta_1y_{t-1}+\\varepsilon_t\\\\\n&=\n\\beta_1(\\beta_1y_{t-2}+\\varepsilon_{t-1})+\\varepsilon_t\\\\\n&=\n\\beta_1^2y_{t-2}+\\beta_1\\varepsilon_{t-1}+\\varepsilon_t\\\\\n&=\n\\beta_1^2(\\beta_1y_{t-3}+\\varepsilon_{t-2})+\\beta_1\\varepsilon_{t-1}+\\varepsilon_t\\\\\n&=\n\\beta_1^3y_{t-3}+\\beta_1^2\\varepsilon_{t-2}+\\beta_1\\varepsilon_{t-1}+\\varepsilon_t\\\\\n&\\vdots\\\\\n&=\n\\beta_1^ky_{t-k}\n+\n\\sum\\limits_{j=0}^{k-1}\\beta_1^j\\varepsilon_{t-j}\n\\\\\n&\\vdots\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/10-ugly-arma-theory.html#example-ar1-as-mainfty-2",
    "href": "slides/10-ugly-arma-theory.html#example-ar1-as-mainfty-2",
    "title": "ARMA models II",
    "section": "Example: AR(1) as MA(\\(\\infty\\))",
    "text": "Example: AR(1) as MA(\\(\\infty\\))\nKeep doing that forever, and eventually\n\\[\ny_t\n=\n\\sum\\limits_{j=0}^\\infty \\beta_1^j\\varepsilon_{t-j}.\n\\] In order for the right-hand side to be finite, we need \\(|\\beta_1|&lt;1\\).\nIn other words, the AR(1) has to be stationary."
  },
  {
    "objectID": "slides/10-ugly-arma-theory.html#in-general-arp-as-mainfty",
    "href": "slides/10-ugly-arma-theory.html#in-general-arp-as-mainfty",
    "title": "ARMA models II",
    "section": "In general: AR(p) as MA(\\(\\infty\\))",
    "text": "In general: AR(p) as MA(\\(\\infty\\))\n\n\n\n\n\n\nMA(\\(\\infty\\)) representation\n\n\nIf an AR(p) is stationary, then it can be written\n\\[\ny_t=\\frac{1}{\\beta(L)}\\varepsilon_t=\\sum\\limits_{j=0}^\\infty \\psi_j\\varepsilon_{t-j},\n\\] where \\(\\psi_0=1\\) and \\(\\sum_{j=0}^\\infty |\\psi_j|&lt;\\infty\\).\n\n\n\n\nIn other words, there is an infinite order MA polynomial\n\\[\n\\psi(z)=1+\\psi_1z+\\psi_2z^2+\\psi_3z^3+...\n\\]\nand \\(y_t=\\psi(L)\\varepsilon_t\\)."
  },
  {
    "objectID": "slides/10-ugly-arma-theory.html#stationarity-redux",
    "href": "slides/10-ugly-arma-theory.html#stationarity-redux",
    "title": "ARMA models II",
    "section": "Stationarity redux",
    "text": "Stationarity redux\nCompanion form matrix:\n\\[\n\\BG\n=\n\\begin{bmatrix}\n\\beta_1 & \\beta_2  & \\cdots & \\beta_{p-1} & \\beta_p\\\\\n1 & 0  & \\cdots & 0 & 0\\\\\n0 & 1  & \\cdots & 0 & 0\\\\\n\\vdots & \\vdots  & \\ddots & \\vdots & \\vdots\\\\\n0 & 0  & \\cdots & 1 & 0\n\\end{bmatrix}\n\\]\n\nAR polynomial:\n\\[\n\\beta(z) = 1-\\beta_1z-\\beta_2z^2-...-\\beta_pz^p\n\\]\n\n\n\\(\\BG\\) having eigenvalues inside the unit circle is the same as \\(\\beta(z)\\) having roots outside the unit circle."
  },
  {
    "objectID": "slides/10-ugly-arma-theory.html#eigenvalues-of-the-companion-matrix",
    "href": "slides/10-ugly-arma-theory.html#eigenvalues-of-the-companion-matrix",
    "title": "ARMA models II",
    "section": "Eigenvalues of the companion matrix",
    "text": "Eigenvalues of the companion matrix\nThe eigenvalues solve\n\\[\n\\det(\\BG-\\lambda\\BI_p)=0\n\\]\n\nIf \\(p=2\\),\n\\[\n\\begin{aligned}\n\\det\\begin{pmatrix}\n\\beta_1-\\lambda & \\beta_2\\\\\n1 & -\\lambda\n\\end{pmatrix}\n&=\n-\\lambda(\\beta_1-\\lambda)-\\beta_2\n\\\\\n&=\n\\lambda^2-\\beta_1\\lambda-\\beta_2.\n\\end{aligned}\n\\]\n\n\nIn general, you can show that\n\\[\n\\det(\\BG-\\lambda\\BI_p)\n=\n\\lambda^p\n-\n\\beta_1\\lambda^{p-1}\n-\n\\beta_2\\lambda^{p-2}\n-\n\\cdots\n-\n\\beta_p\n.\n\\]"
  },
  {
    "objectID": "slides/10-ugly-arma-theory.html#relation-to-the-ar-polynomial",
    "href": "slides/10-ugly-arma-theory.html#relation-to-the-ar-polynomial",
    "title": "ARMA models II",
    "section": "Relation to the AR polynomial",
    "text": "Relation to the AR polynomial\nEigenvalues of \\(\\BG\\) are roots of:\n\\[\ng(\\lambda)=\n\\lambda^p\n-\n\\beta_1\\lambda^{p-1}\n-\n\\beta_2\\lambda^{p-2}\n-\n\\cdots\n-\n\\beta_p.\n\\]\n\nNotice that\n\\[\n\\begin{aligned}\n\\beta(1/\\lambda) &= 1-\\frac{\\beta_1}{\\lambda}-\\frac{\\beta_2}{\\lambda^2}-...-\\frac{\\beta_p}{\\lambda^p}\n\\\\\n&=\n\\frac{\\lambda^p}{\\lambda^p}\n\\left(\n1-\\frac{\\beta_1}{\\lambda}-\\frac{\\beta_2}{\\lambda^2}-...-\\frac{\\beta_p}{\\lambda^p}\\right)\\\\\n&=\n\\frac{1}{\\lambda^p}(\\lambda^p\n-\n\\beta_1\\lambda^{p-1}\n-\n\\beta_2\\lambda^{p-2}\n-\n\\cdots\n-\n\\beta_p)\n\\\\\n&=\ng(\\lambda)/\\lambda^p.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/10-ugly-arma-theory.html#main-idea",
    "href": "slides/10-ugly-arma-theory.html#main-idea",
    "title": "ARMA models II",
    "section": "Main idea",
    "text": "Main idea\n\n\nWe know\n\n\\[\n\\beta\\left(\\frac{1}{\\lambda}\\right)=\\frac{g(\\lambda)}{\\lambda^{p}}.\n\\]\n\n\n\nThe roots of \\(\\beta(z)\\) and the roots of \\(g(\\lambda)\\) are reciprocals of one another;\n\n\n\n\nIf \\(|\\lambda|&lt;1\\), then \\(|1/\\lambda|=1/|\\lambda|&gt;1\\)."
  },
  {
    "objectID": "slides/10-ugly-arma-theory.html#where-do-the-new-coefficients-come-from",
    "href": "slides/10-ugly-arma-theory.html#where-do-the-new-coefficients-come-from",
    "title": "ARMA models II",
    "section": "Where do the new coefficients come from?",
    "text": "Where do the new coefficients come from?\nTheoretically, we have\n\\[\ny_t=\\frac{1}{\\beta(L)}\\varepsilon_t=\\psi(L)\\varepsilon_t,\n\\]\nso whatever they are, the \\(\\psi_j\\) solve\n\\[\n\\psi(z)=\\sum\\limits_{j=0}^\\infty\\psi_jz^j=\\frac{1}{\\beta(z)}=\\frac{1}{1-\\sum\\limits_{l=1}^p\\beta_lz^l}.\n\\]"
  },
  {
    "objectID": "slides/10-ugly-arma-theory.html#parting-thought-summability",
    "href": "slides/10-ugly-arma-theory.html#parting-thought-summability",
    "title": "ARMA models II",
    "section": "Parting thought: summability",
    "text": "Parting thought: summability\n\nThe assumptions of the theorem (stationary AR) are enough to guarantee absolute summability;\nThe MA(\\(\\infty\\)) would remain coherent if we only had square summability, but it wouldn’t be ergodic."
  },
  {
    "objectID": "slides/10-ugly-arma-theory.html#ergodicity",
    "href": "slides/10-ugly-arma-theory.html#ergodicity",
    "title": "ARMA models II",
    "section": "Ergodicity",
    "text": "Ergodicity\n\n\n\n\n\n\nLaw of large numbers for dependent sequences\n\n\nIf \\(y_t\\) is stationary with \\(E(y_t) = \\mu\\), \\(\\gamma(h)=\\cov(y_{t+h},\\,y_t)\\), and\n\\[\n\\sum\\limits_{h=0}^\\infty|\\gamma(h)|&lt;\\infty,\n\\]\nthen\n\\[\n\\bar{y}_T=\\frac{1}{T}\\sum\\limits_{t=1}^Ty_t\\overset{L^2}{\\to}\\mu.\n\\]\nWhich implies convergence in probability, distribution, etc.\n\n\n\n\nThis is what is meant by ergodic;\nWithout it, stuff like method-of-moments breaks down;\nSquare summable is not enough."
  },
  {
    "objectID": "slides/10-ugly-arma-theory.html#example-ma1-as-arinfty",
    "href": "slides/10-ugly-arma-theory.html#example-ma1-as-arinfty",
    "title": "ARMA models II",
    "section": "Example: MA(1) as AR\\((\\infty)\\)",
    "text": "Example: MA(1) as AR\\((\\infty)\\)\nWe know that\n\\[\n\\begin{aligned}\ny_1&=\\varepsilon_1+\\theta_1\\varepsilon_{0} &&\\implies &&\\varepsilon_1=y_1-\\theta_1\\varepsilon_0\\\\\ny_2&=\\varepsilon_2+\\theta_1\\varepsilon_{1} &&\\implies &&\\varepsilon_2=y_2-\\theta_1\\varepsilon_1\\\\\ny_3&=\\varepsilon_3+\\theta_1\\varepsilon_{2} &&\\implies &&\\varepsilon_3=y_3-\\theta_1\\varepsilon_2\\\\\n& &&\\vdots\\\\\ny_{t-1}&=\\varepsilon_{t-1}+\\theta_1\\varepsilon_{t-2} &&\\implies &&\\varepsilon_{t-1}=y_{t-1}-\\theta_1\\varepsilon_{t-2}\\\\\ny_t&=\\varepsilon_t+\\theta_1\\varepsilon_{t-1} &&\\implies &&\\varepsilon_t=y_t-\\theta_1\\varepsilon_{t-1}\\\\\n& &&\\vdots\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/10-ugly-arma-theory.html#example-ma1-as-arinfty-1",
    "href": "slides/10-ugly-arma-theory.html#example-ma1-as-arinfty-1",
    "title": "ARMA models II",
    "section": "Example: MA(1) as AR\\((\\infty)\\)",
    "text": "Example: MA(1) as AR\\((\\infty)\\)\nRecursive substitution:\n\\[\n\\begin{aligned}\n\\varepsilon_t\n&=\ny_t\n-\n\\theta_1\\varepsilon_{t-1}\n\\\\\n&=\ny_t\n-\n\\theta_1(y_{t-1}-\\theta_1\\varepsilon_{t-2})\n\\\\\n&=\ny_t\n-\n\\theta_1y_{t-1}+\\theta_1^2\\varepsilon_{t-2}\n\\\\\n&=\ny_t\n-\n\\theta_1y_{t-1}+\\theta_1^2(y_{t-2}-\\theta_1\\varepsilon_{t-3})\n\\\\\n&=\ny_t\n-\n\\theta_1y_{t-1}+\\theta_1^2y_{t-2}-\\theta_1^3\\varepsilon_{t-3}\n\\\\\n&\\vdots \\\\\n&=\n\\sum\\limits_{j=0}^{k-1} (-\\theta_1)^jy_{t-j} + (-\\theta_1)^{k}\\varepsilon_{t-k}\n\\\\\n&\\vdots\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/10-ugly-arma-theory.html#example-ma1-as-arinfty-2",
    "href": "slides/10-ugly-arma-theory.html#example-ma1-as-arinfty-2",
    "title": "ARMA models II",
    "section": "Example: MA(1) as AR\\((\\infty)\\)",
    "text": "Example: MA(1) as AR\\((\\infty)\\)\nKeep doing that forever, and eventually\n\\[\n\\varepsilon_t\n=\n\\sum\\limits_{j=0}^\\infty (-\\theta_1)^jy_{t-j}.\n\\] In order for the right-hand side to be finite, we need \\(|\\theta_1|&lt;1\\).\nIn other words, the MA(1) has to be identified."
  },
  {
    "objectID": "slides/10-ugly-arma-theory.html#invertibility",
    "href": "slides/10-ugly-arma-theory.html#invertibility",
    "title": "ARMA models II",
    "section": "Invertibility",
    "text": "Invertibility\nIf the MA polynomial\n\\[\n\\theta(z)=1+\\theta_1z+\\theta_2z^2+...+\\theta_qz^q\n\\]\nhas roots outside the unit circle, then we can invert:\n\\[\ny_t=\\theta(L)\\varepsilon_t\\quad \\Longleftrightarrow\\quad \\frac{1}{\\theta(L)}y_t=\\varepsilon_t\n\\]\nThis condition on the roots is also what ensures identification."
  },
  {
    "objectID": "slides/10-ugly-arma-theory.html#in-general-maq-as-arinfty",
    "href": "slides/10-ugly-arma-theory.html#in-general-maq-as-arinfty",
    "title": "ARMA models II",
    "section": "In general: MA(q) as AR\\((\\infty)\\)",
    "text": "In general: MA(q) as AR\\((\\infty)\\)\n\n\n\n\n\n\nAR(\\(\\infty\\)) representation\n\n\nIf an MA(q) is invertible, then it can be written\n\\[\n\\varepsilon_t=\\frac{1}{\\theta(L)}y_t=\\sum\\limits_{j=0}^\\infty \\pi_jy_{t-j},\n\\] where \\(\\pi_0=1\\) and \\(\\sum_{j=0}^\\infty |\\pi_j|&lt;\\infty\\).\n\n\n\n\nIn other words, there is an infinite order AR polynomial\n\\[\n\\pi(z)=1+\\pi_1z+\\pi_2z^2+\\pi_3z^3+...\n\\]\nand \\(\\varepsilon_t=\\pi(L)y_t\\)."
  },
  {
    "objectID": "slides/10-ugly-arma-theory.html#where-do-the-new-coefficients-come-from-1",
    "href": "slides/10-ugly-arma-theory.html#where-do-the-new-coefficients-come-from-1",
    "title": "ARMA models II",
    "section": "Where do the new coefficients come from?",
    "text": "Where do the new coefficients come from?\nTheoretically, we have\n\\[\n\\varepsilon_t=\\frac{1}{\\theta(L)}y_t=\\pi(L)y_t,\n\\]\nso whatever they are, the \\(\\pi_j\\) solve\n\\[\n\\pi(z)=\\sum\\limits_{j=0}^\\infty\\pi_jz^j=\\frac{1}{\\theta(z)}=\\frac{1}{1+\\sum\\limits_{i=1}^q\\theta_iz^i}.\n\\]"
  },
  {
    "objectID": "slides/10-ugly-arma-theory.html#putting-it-together",
    "href": "slides/10-ugly-arma-theory.html#putting-it-together",
    "title": "ARMA models II",
    "section": "Putting it together",
    "text": "Putting it together\nWe know:\n\\[\n\\beta(L)y_t\n=\n\\theta(L)\\varepsilon_t.\n\\]\n\nIf it’s stationary, you get an MA(\\(\\infty\\)):\n\\[\ny_t=\\frac{\\theta(L)}{\\beta(L)}\\varepsilon_t.\n\\]\n\n\nIf it’s invertible, you get an AR(\\(\\infty\\)):\n\\[\n\\frac{\\beta(L)}{\\theta(L)}y_t=\\varepsilon_t.\n\\]\n\n\nIf it’s both stationary and invertible, take your pick."
  },
  {
    "objectID": "slides/10-ugly-arma-theory.html#arma-as-mainfty",
    "href": "slides/10-ugly-arma-theory.html#arma-as-mainfty",
    "title": "ARMA models II",
    "section": "ARMA as MA(\\(\\infty\\))",
    "text": "ARMA as MA(\\(\\infty\\))\nA stationary ARMA(p, q) can be written\n\\[\ny_t=\\sum\\limits_{j=0}^\\infty \\psi_j\\varepsilon_{t-j},\n\\]\nwhere the \\(\\psi_j\\) solve\n\\[\n\\psi(z)=\\sum\\limits_{j=0}^\\infty \\psi_jz^j=\\frac{\\theta(z)}{\\beta(z)},\\quad |z|\\leq 1.\n\\]\nHow?"
  },
  {
    "objectID": "slides/10-ugly-arma-theory.html#recursion-for-the-new-weights",
    "href": "slides/10-ugly-arma-theory.html#recursion-for-the-new-weights",
    "title": "ARMA models II",
    "section": "Recursion for the new weights",
    "text": "Recursion for the new weights\nMatch the coefficients:\n\\[\n\\begin{aligned}\n\\psi(z)\n&=\\frac{\\theta(z)}{\\beta(z)}\n\\\\\n\\beta(z)\\psi(z)\n&=\\theta(z)\n\\\\\n(1-\\beta_1z-\\beta_2z^2-...-\\beta_pz^p)(1+\\psi_1z+\\psi_2z^2+...)\n&=\n1+\\theta_1z+\\theta_2z^2+...+\\theta_qz^q.\n\\end{aligned}\n\\]\n\nMultiply the left side out, and it must be that\n\\[\n\\begin{aligned}\n\\psi_0 &= 1, \\\\\n\\psi_1 - \\beta_1 \\psi_0 &= \\theta_1, \\\\\n\\psi_2 - \\beta_1 \\psi_1 - \\phi_2 \\psi_0 &= \\theta_2, \\\\\n\\psi_3 - \\beta_1 \\psi_2 - \\phi_2 \\psi_1 - \\phi_3 \\psi_0 &= \\theta_3, \\\\\n&\\ \\ \\vdots\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/10-ugly-arma-theory.html#in-general",
    "href": "slides/10-ugly-arma-theory.html#in-general",
    "title": "ARMA models II",
    "section": "In general",
    "text": "In general\nGet a computer to grind through:\n\\[\n\\psi_j - \\sum_{k=1}^p \\beta_k \\psi_{j-k} = 0,\n\\qquad j \\geq \\max(p, q+1)\n\\]\nand\n\\[\n\\psi_j - \\sum_{k=1}^j \\beta_k \\psi_{j-k} = \\theta_j,\n\\qquad 0 \\leq j &lt; \\max(p, q+1).\n\\]"
  },
  {
    "objectID": "slides/10-ugly-arma-theory.html#arma-as-arinfty",
    "href": "slides/10-ugly-arma-theory.html#arma-as-arinfty",
    "title": "ARMA models II",
    "section": "ARMA as AR(\\(\\infty\\))",
    "text": "ARMA as AR(\\(\\infty\\))\nA invertible ARMA(p, q) can be written\n\\[\n\\varepsilon_t=\\sum\\limits_{j=0}^\\infty \\pi_jy_{t-j},\n\\]\nwhere the \\(\\pi_j\\) solve\n\\[\n\\pi(z)=\\sum\\limits_{j=0}^\\infty \\pi_jz^j=\\frac{\\beta(z)}{\\theta(z)},\\quad |z|\\leq 1.\n\\]\nWeights follow similar idea from before. Just let a computer do it."
  },
  {
    "objectID": "slides/10-ugly-arma-theory.html#big-ol-theorem",
    "href": "slides/10-ugly-arma-theory.html#big-ol-theorem",
    "title": "ARMA models II",
    "section": "Big ol’ theorem",
    "text": "Big ol’ theorem\n\n\n\n\n\n\nWold representation\n\n\nAny stationary mean-zero process \\(y_t\\) can be uniquely written as\n\\[\ny_t=\\nu_t+\\sum\\limits_{j=0}^\\infty \\psi_j\\varepsilon_{t-j},\n\\]\nwhere\n\n\\(\\varepsilon_t\\) uncorrelated with variance \\(\\sigma^2&gt;0\\);\n\\(\\psi_j\\) square summable;\n\\(\\nu_t\\) perfectly predictable given its history (deterministic).\n\n\n\n\n\nShumway and Stoffer:\n\nThe theorem…falls short of our needs because we would prefer the noise process…to be independent…\\(\\{\\psi_j\\}\\) to be absolutely summable. But the decomposition does give us the confidence that we will not be completely oﬀ the mark by fitting ARMA models to many types of time series.\n\n\n\nWhatever."
  },
  {
    "objectID": "slides/00-welcome.html#teaching-team",
    "href": "slides/00-welcome.html#teaching-team",
    "title": "Welcome to STA 542!",
    "section": "Teaching team",
    "text": "Teaching team\n\n\n\n\n\n\n\n\nMug\nName\nRole\nOffice Hours\n\n\n\n\nLi, Aihua\nTA\nThu 10:30AM - 12:30PM\n\n\n\nZito, John\nInstructor\nTue 1PM - 3PM"
  },
  {
    "objectID": "slides/00-welcome.html#motivation",
    "href": "slides/00-welcome.html#motivation",
    "title": "Welcome to STA 542!",
    "section": "Motivation",
    "text": "Motivation\n\n\n\n\n\n\n\nControversial statement\n\n\nStatistics is about quantifying uncertainty to help make decisions.\n\n\n\n\n\nIn modern data science environments, stakeholders are sequentially analyzing high volumes of dependent data and using it to forecast the future in real-time.\n\n\nLet’s see that in action."
  },
  {
    "objectID": "slides/00-welcome.html#central-banking",
    "href": "slides/00-welcome.html#central-banking",
    "title": "Welcome to STA 542!",
    "section": "Central banking",
    "text": "Central banking"
  },
  {
    "objectID": "slides/00-welcome.html#e-commerce-and-web-traffic",
    "href": "slides/00-welcome.html#e-commerce-and-web-traffic",
    "title": "Welcome to STA 542!",
    "section": "E-commerce and web traffic",
    "text": "E-commerce and web traffic\namazon.com is never “offline.”"
  },
  {
    "objectID": "slides/00-welcome.html#extreme-weather-forecasting",
    "href": "slides/00-welcome.html#extreme-weather-forecasting",
    "title": "Welcome to STA 542!",
    "section": "Extreme weather forecasting",
    "text": "Extreme weather forecasting"
  },
  {
    "objectID": "slides/00-welcome.html#monitoring-disease-case-counts",
    "href": "slides/00-welcome.html#monitoring-disease-case-counts",
    "title": "Welcome to STA 542!",
    "section": "Monitoring disease case counts",
    "text": "Monitoring disease case counts"
  },
  {
    "objectID": "slides/00-welcome.html#monitoring-the-energy-grid",
    "href": "slides/00-welcome.html#monitoring-the-energy-grid",
    "title": "Welcome to STA 542!",
    "section": "Monitoring the energy grid",
    "text": "Monitoring the energy grid"
  },
  {
    "objectID": "slides/00-welcome.html#object-tracking",
    "href": "slides/00-welcome.html#object-tracking",
    "title": "Welcome to STA 542!",
    "section": "Object tracking",
    "text": "Object tracking\n\n\n\n\n\nAny ethical complaints about this one?"
  },
  {
    "objectID": "slides/00-welcome.html#autonomous-vehicle-navigation",
    "href": "slides/00-welcome.html#autonomous-vehicle-navigation",
    "title": "Welcome to STA 542!",
    "section": "Autonomous vehicle navigation",
    "text": "Autonomous vehicle navigation"
  },
  {
    "objectID": "slides/00-welcome.html#lets-take-a-second-look-at-this",
    "href": "slides/00-welcome.html#lets-take-a-second-look-at-this",
    "title": "Welcome to STA 542!",
    "section": "Let’s take a second look at this",
    "text": "Let’s take a second look at this\n\n\n\n\n\nSequential inference and probabilistic prediction!"
  },
  {
    "objectID": "slides/00-welcome.html#point-forecast",
    "href": "slides/00-welcome.html#point-forecast",
    "title": "Welcome to STA 542!",
    "section": "Point forecast",
    "text": "Point forecast\nYour single-number best guess at tomorrow’s observation:"
  },
  {
    "objectID": "slides/00-welcome.html#forecast-interval",
    "href": "slides/00-welcome.html#forecast-interval",
    "title": "Welcome to STA 542!",
    "section": "Forecast interval",
    "text": "Forecast interval\nA range of likely values for tomorrow’s observation:"
  },
  {
    "objectID": "slides/00-welcome.html#forecast-density",
    "href": "slides/00-welcome.html#forecast-density",
    "title": "Welcome to STA 542!",
    "section": "Forecast density",
    "text": "Forecast density\nFull distribution capturing uncertainty about tomorrow:"
  },
  {
    "objectID": "slides/00-welcome.html#and-then-tomorrow-finally-comes",
    "href": "slides/00-welcome.html#and-then-tomorrow-finally-comes",
    "title": "Welcome to STA 542!",
    "section": "And then tomorrow finally comes",
    "text": "And then tomorrow finally comes\nSo…how’d we do?"
  },
  {
    "objectID": "slides/00-welcome.html#the-main-themes-of-the-course",
    "href": "slides/00-welcome.html#the-main-themes-of-the-course",
    "title": "Welcome to STA 542!",
    "section": "The main themes of the course",
    "text": "The main themes of the course\nWe will focus on the following:\n\nInference should be sequential. We want recursive estimation techniques to handle data that are streaming in real-time;\nPredictions should be probabilistic. We want point, interval, and density forecasts that incorporate many sources of uncertainty;\nIf you can manipulate joint distributions, you can do time series analysis. Too many TS textbooks quickly lose sight of this;\n\n\nAnd there is a secret fourth theme:\n\nA Bayesian approach is an excellent way of achieving the goals of sequential inference and probabilistic prediction."
  },
  {
    "objectID": "slides/00-welcome.html#topics-may-include",
    "href": "slides/00-welcome.html#topics-may-include",
    "title": "Welcome to STA 542!",
    "section": "Topics may include",
    "text": "Topics may include\n\nARMA models;\nVector autoregressions (VARs);\nDynamic linear models (DLMs);\nHidden Markov Models (HMMs);\nProbabilistic forecast evaluation;\nNonlinear non-Gaussian state-space models;\nSequential Monte Carlo (AKA particle filtering);\nForecast combination."
  },
  {
    "objectID": "slides/00-welcome.html#bookmark-the-course-page",
    "href": "slides/00-welcome.html#bookmark-the-course-page",
    "title": "Welcome to STA 542!",
    "section": "Bookmark the course page!",
    "text": "Bookmark the course page!\n\n\n\n\n\nhttps://sta542-f25.github.io/"
  },
  {
    "objectID": "slides/00-welcome.html#final-grade-breakdown",
    "href": "slides/00-welcome.html#final-grade-breakdown",
    "title": "Welcome to STA 542!",
    "section": "Final grade breakdown",
    "text": "Final grade breakdown\nYour final course grade will be calculated as follows:\n\n\nCategory\nPercentage\n\n\n\nProblem Sets\n25%\n\n\nExam 1\n25%\n\n\nExam 2\n25%\n\n\nFinal Project\n25%\n\n\n\n\n\n\n\n\n\nNote\n\n\nThe final letter grade will be based on the usual thresholds, and there might be a curve."
  },
  {
    "objectID": "slides/00-welcome.html#course-components",
    "href": "slides/00-welcome.html#course-components",
    "title": "Welcome to STA 542!",
    "section": "Course components",
    "text": "Course components\n\nProblem sets: 6 in total; one due about every 2 weeks;\n\nExams: in-class, with only an 8.5” x 11” note sheet:\n\nWednesday October 8 11:45 AM - 12:00 PM;\nMonday November 24 11:45 AM - 12:00 PM;\n\n\nFinal project: no clue, honestly. Talk to me in October."
  },
  {
    "objectID": "slides/00-welcome.html#late-policy",
    "href": "slides/00-welcome.html#late-policy",
    "title": "Welcome to STA 542!",
    "section": "Late policy",
    "text": "Late policy\nNo late work will be accepted unless you request an extension in advance by e-mailing JZ. All reasonable requests will be entertained, but extensions will not be long."
  },
  {
    "objectID": "slides/00-welcome.html#attendance",
    "href": "slides/00-welcome.html#attendance",
    "title": "Welcome to STA 542!",
    "section": "Attendance",
    "text": "Attendance\nNot required. Live your life."
  },
  {
    "objectID": "slides/00-welcome.html#communication",
    "href": "slides/00-welcome.html#communication",
    "title": "Welcome to STA 542!",
    "section": "Communication",
    "text": "Communication\nIf you wish to ask questions in writing…\n\nPost on Ed: about general course policies and content;\nEmail JZ directly: personal matters.\n\nYou should not really be emailing Aihua directly for any reason."
  },
  {
    "objectID": "slides/00-welcome.html#collaboration",
    "href": "slides/00-welcome.html#collaboration",
    "title": "Welcome to STA 542!",
    "section": "Collaboration",
    "text": "Collaboration\nYou are enthusiastically encouraged to work together on the problem sets. You will learn a lot from each other! Two policies:\n\n✅ Acknowledge your collaborators: “Aloysius, Cybill, and I worked together on this problem;”\n❌ Do not outright share or copy solutions. All submitted work must be your own.\n\nViolation of the second policy is plagiarism. Sharers and recipients alike are referred to the conduct office and receive zeros."
  },
  {
    "objectID": "slides/00-welcome.html#use-of-outside-resources-including-ai",
    "href": "slides/00-welcome.html#use-of-outside-resources-including-ai",
    "title": "Welcome to STA 542!",
    "section": "Use of outside resources, including AI",
    "text": "Use of outside resources, including AI\n\nUsing ChatGPT to complete assignments is like bringing a forklift into the weight room; you will never improve your cognitive fitness that way.\n\n\nIf you find a problem solution online (or prompt an LLM to generate one) and submit it as your own work, that will obviously be considered plagiarism;\nOtherwise, all outside resources are fair game for you to study and get extra practice;\nIf you outsource all of your thinking to a language model, you will probably fail both exams. Good luck!"
  },
  {
    "objectID": "slides/00-welcome.html#what-background-do-you-need",
    "href": "slides/00-welcome.html#what-background-do-you-need",
    "title": "Welcome to STA 542!",
    "section": "What background do you need?",
    "text": "What background do you need?\nI assume you have a working knowledge of…\n\nmatrix algebra;\nOLS regression;\nprobability and math stat at the level of Casella & Berger;\nBayesian statistics at the level of STA 602;\nThe R programming language.\n\n\nProblem Set 0 gives you a workout in all of the above."
  },
  {
    "objectID": "slides/00-welcome.html#time-series",
    "href": "slides/00-welcome.html#time-series",
    "title": "Welcome to STA 542!",
    "section": "Time series",
    "text": "Time series\n\nA time series is a set of measurements collected over time;\nWe model these data as a sequence of dependent random variables:\n\n\\[\n\\mathbf{y}_{0:T} = \\{\\mathbf{y}_0,\\,\\mathbf{y}_1,\\,\\mathbf{y}_2,\\,...,\\,\\mathbf{y}_T\\}.\n\\]\n\nA time series model is “just” their joint probability distribution:\n\n\\[\np(\\mathbf{y}_{0:T}) = p(\\mathbf{y}_0)\\prod_{t=1}^Tp(\\mathbf{y}_t\\,|\\,\\mathbf{y}_{0:t-1}).\n\\]\n\n\n\n\n\n\n\nStay grounded.\n\n\nLike much wisdom, that last bullet is simultaneously vacuous and profound. It tells you everything and it tells you nothing all at once. But don’t let this basic fact get lost in the sea of details."
  },
  {
    "objectID": "slides/00-welcome.html#notation-to-get-used-to",
    "href": "slides/00-welcome.html#notation-to-get-used-to",
    "title": "Welcome to STA 542!",
    "section": "Notation to get used to",
    "text": "Notation to get used to\n\nI will not use uppercase \\(Y_t\\) versus lowercase \\(y_t\\) to distinguish random variables and fixed realizations. It’s all just \\(y_t\\);\nA vector \\(\\mathbf{y}\\in\\RR^n\\) is always an \\(n\\times 1\\) column. The corresponding row vector is \\(\\By^\\tr\\);\nFor integers \\(i&lt;j\\), you will see this shorthand all the time:\n\n\\[\ny_{i:j}\n=\n\\{y_i\\com y_{i+1}\\com y_{i+2}\\com...\\com y_{j-2}\\com y_{j-1}\\com y_{j}\\}\n.\n\\]\n\nThe symbol “\\(p\\)” will be aggressively abused and overloaded to represent any probability distribution, sometimes several in the same line:\n\n\\[\np(\\mathbf{y}_{0:T}) = p(\\mathbf{y}_0)\\prod_{t=1}^Tp(\\mathbf{y}_t\\,|\\,\\mathbf{y}_{0:t-1}).\n\\]"
  },
  {
    "objectID": "slides/00-welcome.html#definition",
    "href": "slides/00-welcome.html#definition",
    "title": "Welcome to STA 542!",
    "section": "Definition",
    "text": "Definition\nA random vector \\(\\mathbf{x}=\\begin{bmatrix}x_1&x_2&\\cdots&x_n\\end{bmatrix}^{\\scriptscriptstyle\\mathsf{T}}\\in\\mathbb{R}^n\\) has the multivariate normal distribution with mean vector \\(\\boldsymbol{\\mu}\\in\\mathbb{R}^n\\) and covariance matrix \\(\\boldsymbol{\\Sigma}\\in\\mathbb{R}^{n\\times n}\\) if its density is\n\n\\[\np(\\mathbf{x})\n=\n\\frac\n{\n\\exp\n\\left(\n-\\frac{1}{2}\n(\\mathbf{x} - \\boldsymbol{\\mu})^{\\scriptscriptstyle\\mathsf{T}}\n\\boldsymbol{\\Sigma}^{-1}\n(\\mathbf{x}-\\boldsymbol{\\mu})\n\\right)\n}\n{\n(2\\pi)^{\\frac{n}{2}}\n|\\boldsymbol{\\Sigma}|^{1/2}\n}\n,\n\\quad\n\\mathbf{x}\n\\in\n\\mathbb{R}^n.\n\\]\n\n\nWe denote this \\(\\mathbf{x}\\sim\\text{N}_n(\\boldsymbol{\\mu},\\,\\boldsymbol{\\Sigma})\\).\n\n\n\n\n\n\n\n\nPlenty of linear algebra coming your way!\n\n\n\nTranspose \\((\\mathbf{x} - \\boldsymbol{\\mu})^{\\scriptscriptstyle\\mathsf{T}}\\);\nInverse \\(\\boldsymbol{\\Sigma}^{-1}\\);\nMatrix multiplication \\((\\mathbf{x} - \\boldsymbol{\\mu})^{\\scriptscriptstyle\\mathsf{T}}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\);\nDeterminant \\(|\\boldsymbol{\\Sigma}|\\);\n\n\\(\\boldsymbol{\\Sigma}\\) is a symmetric and positive definite matrix."
  },
  {
    "objectID": "slides/00-welcome.html#univariate",
    "href": "slides/00-welcome.html#univariate",
    "title": "Welcome to STA 542!",
    "section": "Univariate",
    "text": "Univariate\nIf \\(n=1\\), then we meet an old friend:\n\n\n\n\\[\np(x)=\\frac{\\exp\\left(-\\frac{1}{2}\\frac{(x-\\mu)^2}{\\sigma^2}\\right)}{\\sqrt{2\\pi\\sigma^2}}.\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nSo \\(\\mathbf{x} = [x]\\sim\\text{N}_1([\\mu],\\,[\\sigma^2])\\) is just \\(x\\sim\\text{N}(\\mu,\\,\\sigma^2)\\)."
  },
  {
    "objectID": "slides/00-welcome.html#bivariate-elliptical-contours",
    "href": "slides/00-welcome.html#bivariate-elliptical-contours",
    "title": "Welcome to STA 542!",
    "section": "Bivariate: elliptical contours!",
    "text": "Bivariate: elliptical contours!"
  },
  {
    "objectID": "slides/00-welcome.html#moments",
    "href": "slides/00-welcome.html#moments",
    "title": "Welcome to STA 542!",
    "section": "Moments",
    "text": "Moments\nFirst:\n\\[\n\\boldsymbol{\\mu}\n=\nE(\\mathbf{x})\n=\n\\begin{bmatrix}\n\\mu_1\n&\n\\mu_2\n&\n\\cdots\n&\n\\mu_n\n\\end{bmatrix}^{\\scriptscriptstyle\\mathsf{T}}\n=\n\\begin{bmatrix}\nE(x_1)\n&\nE(x_2)\n&\n\\cdots\n&\nE(x_n)\n\\end{bmatrix}^{\\scriptscriptstyle\\mathsf{T}}\n.\n\\]\n\nSecond:\n\\[\n\\begin{aligned}\n\\boldsymbol{\\Sigma}\n=\n\\text{cov}(\\mathbf{x})\n&=\n\\begin{bmatrix}\n\\sigma^2_1 & \\sigma_{1,2} & \\cdots & \\sigma_{1,n}\\\\\n\\sigma_{1,2} & \\sigma_{2}^2 & \\cdots & \\sigma_{2,n}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n\\sigma_{1,n} & \\sigma_{2,n} & \\cdots & \\sigma_{n}^2\\\\\n\\end{bmatrix}\n\\\\\n&=\n\\begin{bmatrix}\n\\text{var}(x_1) & \\text{cov}(x_1,\\,x_2) & \\cdots & \\text{cov}(x_1,\\,x_n)\\\\\n\\text{cov}(x_2,\\,x_1) & \\text{var}(x_2) & \\cdots & \\text{cov}(x_2,\\,x_n)\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n\\text{cov}(x_n,\\,x_1) & \\text{cov}(x_n,\\,x_2) & \\cdots & \\text{var}(x_n)\\\\\n\\end{bmatrix}\n.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/00-welcome.html#oh-my-aching-eigen",
    "href": "slides/00-welcome.html#oh-my-aching-eigen",
    "title": "Welcome to STA 542!",
    "section": "Oh my aching eigen",
    "text": "Oh my aching eigen\n\n\n\n\n\n\n\nEigenvectors and eigenvalues\n\n\nThere are \\(n\\) orthogonal vectors \\(\\mathbf{v}_i\\in\\mathbb{R}^n\\) and values \\(\\lambda_i&gt;0\\) satisfying:\n\\[\n\\boldsymbol{\\Sigma}\\mathbf{v}_i=\\lambda_i\\mathbf{v}_i.\n\\] Positive definite means all the eigenvalues are real and strictly postive.\n\n\n\n\n\n\n\n\n\n\n\nEigendecomposition (AKA spectral decomposition)\n\n\nA useful way to rewrite \\(\\boldsymbol{\\Sigma}\\): \\[\n\\begin{aligned}\n\\boldsymbol{\\Sigma}\n&=\n\\begin{bmatrix}\n\\mathbf{v}_1 & \\mathbf{v}_2 & \\cdots & \\mathbf{v}_n\n\\end{bmatrix}\n\\begin{bmatrix}\n\\lambda_1 & & &\\mathbf{0}\\\\\n& \\lambda_2 & & \\\\\n  & & \\ddots& \\\\\n  \\mathbf{0}  & & & \\lambda_n\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{v}_1^{\\scriptscriptstyle\\mathsf{T}} \\\\ \\mathbf{v}_2^{\\scriptscriptstyle\\mathsf{T}} \\\\ \\vdots \\\\ \\mathbf{v}_n^{\\scriptscriptstyle\\mathsf{T}}\n\\end{bmatrix}\n=\n\\mathbf{Q}\\boldsymbol{\\Lambda}\\mathbf{Q}^{\\scriptscriptstyle\\mathsf{T}}.\n\\end{aligned}\n\\]\nWe often set \\(||\\mathbf{v}_i||_2=1\\), and so the \\(n\\times n\\) matrix \\(\\mathbf{Q}\\) is orthogonal: \\(\\mathbf{Q}\\mathbf{Q}^{\\scriptscriptstyle\\mathsf{T}}=\\mathbf{Q}^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{Q}=\\mathbf{I}_n\\)."
  },
  {
    "objectID": "slides/00-welcome.html#who-cares",
    "href": "slides/00-welcome.html#who-cares",
    "title": "Welcome to STA 542!",
    "section": "Who cares?",
    "text": "Who cares?\nThe eigenvectors of \\(\\boldsymbol{\\Sigma}\\) point along the axes of the elliptical density contours. These are the directions of the principal components!"
  },
  {
    "objectID": "slides/00-welcome.html#review-joint-distributions",
    "href": "slides/00-welcome.html#review-joint-distributions",
    "title": "Welcome to STA 542!",
    "section": "Review: joint distributions",
    "text": "Review: joint distributions\n\nThe marginal distribution:\n\\[\np(\\mathbf{y}) = \\int p(\\mathbf{x},\\,\\mathbf{y})\\,\\text{d}\\mathbf{x}.\n\\]\n\n\nThe conditional distribution:\n\\[\np(\\mathbf{y}\\,|\\,\\mathbf{x})\n=\n\\frac{p(\\mathbf{x},\\,\\mathbf{y})}{p(\\mathbf{x})}\n=\n\\frac{p(\\mathbf{x}\\,|\\,\\mathbf{y})p(\\mathbf{y})}{p(\\mathbf{x})}\n.\n\\]"
  },
  {
    "objectID": "slides/00-welcome.html#marginals-and-conditionals",
    "href": "slides/00-welcome.html#marginals-and-conditionals",
    "title": "Welcome to STA 542!",
    "section": "Marginals and conditionals",
    "text": "Marginals and conditionals\nIf you apply those formulas to this\n\\[\n\\begin{bmatrix}\n\\mathbf{x} \\\\\n\\mathbf{y}\n\\end{bmatrix}\n\\sim\n\\text{N}_{n+m}\n\\left(\n\\begin{bmatrix}\n\\boldsymbol{\\mu}_x \\\\\n\\boldsymbol{\\mu}_y\n\\end{bmatrix}\n,\\,\n\\begin{bmatrix}\n\\boldsymbol{\\Sigma}_x & \\boldsymbol{\\Sigma}_{xy}\\\\\n\\boldsymbol{\\Sigma}_{xy}^{\\scriptscriptstyle\\mathsf{T}} & \\boldsymbol{\\Sigma}_y\n\\end{bmatrix}\n\\right),\n\\]\n\nthen you get this:\n\\[\n\\begin{aligned}\n\\mathbf{x} &\\sim  \\text{N}_n(\\boldsymbol{\\mu}_x,\\,\\boldsymbol{\\Sigma}_x)\\\\\n\\mathbf{y} &\\sim  \\text{N}_m(\\boldsymbol{\\mu}_y,\\,\\boldsymbol{\\Sigma}_y)\\\\\n\\mathbf{x} \\,|\\,\\mathbf{y}\n&\\sim\n\\text{N}_n\n\\left(\n\\boldsymbol{\\mu}_x + \\boldsymbol{\\Sigma}_{xy}\\boldsymbol{\\Sigma}_y^{-1}(\\mathbf{y}-\\boldsymbol{\\mu}_y)\n,\\,\n\\boldsymbol{\\Sigma}_x - \\boldsymbol{\\Sigma}_{xy}\n\\boldsymbol{\\Sigma}_y^{-1}\n\\boldsymbol{\\Sigma}_{xy}^{\\scriptscriptstyle\\mathsf{T}}\n\\right)\n\\\\\n\\mathbf{y} \\,|\\,\\mathbf{x}\n&\\sim\n\\text{N}_m\n\\left(\n\\boldsymbol{\\mu}_y + \\boldsymbol{\\Sigma}_{xy}^{\\scriptscriptstyle\\mathsf{T}}\\boldsymbol{\\Sigma}_x^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_x)\n,\\,\n\\boldsymbol{\\Sigma}_y - \\boldsymbol{\\Sigma}_{xy}^{\\scriptscriptstyle\\mathsf{T}}\n\\boldsymbol{\\Sigma}_x^{-1}\n\\boldsymbol{\\Sigma}_{xy}\n\\right).\n\\end{aligned}\n\\]\n\n\nSo the individual components are all normal: \\(x_i\\sim\\text{N}(\\mu_i,\\,\\sigma^2_i)\\)."
  },
  {
    "objectID": "slides/00-welcome.html#those-ugly-conditional-formulas-are-not-new",
    "href": "slides/00-welcome.html#those-ugly-conditional-formulas-are-not-new",
    "title": "Welcome to STA 542!",
    "section": "Those ugly conditional formulas are not new",
    "text": "Those ugly conditional formulas are not new\nConsider the bivariate case:\n\\[\n\\begin{bmatrix}\nx \\\\\ny\n\\end{bmatrix}\n\\sim\\text{N}_2\n\\left(\n\\begin{bmatrix}\n\\mu_x \\\\\n\\mu_y\n\\end{bmatrix}\n,\\,\n\\begin{bmatrix}\n\\sigma_x^2\n&\n\\rho\\sigma_x\\sigma_y\n\\\\\n\\rho\\sigma_x\\sigma_y\n&\n\\sigma^2_y\n\\end{bmatrix}\n\\right).\n\\]\n\nThen the formula collapses to:\n\\[\ny\\,|\\,\nx\n\\sim\n\\text{N}\n\\left(\n\\underbrace{\\left(\\mu_y-\\rho\\frac{\\sigma_y}{\\sigma_x}\\mu_x\\right)}_{\\beta_0}\n+\n\\underbrace{\\rho\\frac{\\sigma_y}{\\sigma_x}}_{\\beta_1}\nx\n,\\,\n\\underbrace{(1-\\rho)^2\n\\sigma^2_y}_{\\sigma^2}\n\\right).\n\\]\n\n\nIn other words:\n\\[\ny = \\beta_0+\\beta_1x + \\varepsilon,\\quad \\varepsilon\\sim\\text{N}(0,\\,\\sigma^2).\n\\]\n\n\nWelcome back to regression 101!"
  },
  {
    "objectID": "slides/00-welcome.html#affine-transformations",
    "href": "slides/00-welcome.html#affine-transformations",
    "title": "Welcome to STA 542!",
    "section": "Affine transformations",
    "text": "Affine transformations\nFix some objects:\n\nRandom \\(\\mathbf{x}\\sim\\text{N}_n(\\boldsymbol{\\mu},\\,\\boldsymbol{\\Sigma})\\);\nConstant \\(\\mathbf{A}\\in\\mathbb{R}^{m\\times n}\\);\nConstant \\(\\mathbf{c}\\in\\mathbb{R}^m\\).\n\n\nThen\n\\[\n\\mathbf{y}\n=\n\\mathbf{A}\n\\mathbf{x}\n+\n\\mathbf{c}\n\\sim\n\\text{N}_m\n\\left(\n\\mathbf{A}\n\\boldsymbol{\\mu}\n+\n\\mathbf{c}\n,\\,\n\\mathbf{A}\n\\boldsymbol{\\Sigma}\n\\mathbf{A}^{\\scriptscriptstyle\\mathsf{T}}\n\\right)\n.\n\\]\n\n\nProve it on Problem Set 0!"
  },
  {
    "objectID": "slides/00-welcome.html#a-useful-special-case-problem-set-0",
    "href": "slides/00-welcome.html#a-useful-special-case-problem-set-0",
    "title": "Welcome to STA 542!",
    "section": "A useful special case (Problem Set 0!)",
    "text": "A useful special case (Problem Set 0!)\n\n\n\n\n\n\nLinear combinations of independent normals are normal\n\n\nIf \\(x_i\\overset{\\text{indep}}{\\sim}\\text{N}(\\mu_i,\\,\\sigma^2_i)\\) and \\(a_i\\in\\mathbb{R}\\) are constant, then\n\\[\n\\sum\\limits_{i=1}^na_ix_i\\sim\\text{N}\\left(\\sum\\limits_{i=1}^na_i\\mu_i,\\, \\sum\\limits_{i=1}^na_i^2\\sigma_i^2\\right).\n\\]\n\n\n\n\nThe result on the previous slide is way more general, and the linear combination is still normal even if the \\(X_i\\) are dependent, but the formula for the variance is less nice;\nThe mean formula is right even if the \\(x_i\\) are dependent and non-Gaussian;\nThe variance formula is right even if the \\(x_i\\) are non-Gaussian."
  },
  {
    "objectID": "slides/00-welcome.html#independence",
    "href": "slides/00-welcome.html#independence",
    "title": "Welcome to STA 542!",
    "section": "Independence",
    "text": "Independence\n\nIf random variables are independent, then they are uncorrelated (their covariance is zero). The reverse is false in general!\nSo writing \\(x_1,\\,x_2,\\,...,\\,x_n\\overset{\\text{iid}}{\\sim}\\text{N}(\\mu,\\,\\sigma^2)\\) is the same as saying\n\n\n\\[\n\\begin{bmatrix}\nx_1\\\\\nx_2\\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix}\n\\sim\\text{N}_n\\left(\\mu\\mathbf{1}_n,\\,\\sigma^2\\mathbf{I}_n\\right).\n\\]\n\n\n\n\n\\(\\mathbf{1}_n\\) is the \\(n\\times 1\\) vector of all ones;\n\n\n\n\n\n\\(\\mathbf{I}_n\\) is the \\(n\\times n\\) identity matrix: ones on the diagonal, zeros off."
  },
  {
    "objectID": "slides/01-ar-1-structure.html#time-series",
    "href": "slides/01-ar-1-structure.html#time-series",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Time series",
    "text": "Time series\n\nA time series is a set of measurements collected over time;\nWe model these data as a sequence of dependent random variables:\n\n\\[\n\\By_{0:T} = \\{\\By_0,\\,\\By_1,\\,\\By_2,\\,...,\\,\\By_T\\}.\n\\]\n\nA time series model is “just” their joint probability distribution:\n\n\\[\np(\\By_{0:T}) = p(\\By_0)\\prod_{t=1}^Tp(\\By_t\\,|\\,\\By_{0:t-1}).\n\\]"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#course-themes",
    "href": "slides/01-ar-1-structure.html#course-themes",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Course themes",
    "text": "Course themes\nWe will focus on a small set of themes, but go deep on them:\n\nInference should be sequential. We want recursive estimation techniques to handle data that are streaming in real-time;\nPredictions should be probabilistic. We want point, interval, and density forecasts that incorporate many sources of uncertainty;\nIf you can manipulate joint distributions, you can do time series analysis. This is obscured in too many TS texts;\n\n\nAnd there is a secret fourth theme:\n\nA Bayesian approach is an excellent way of achieving the goals of sequential inference and probabilistic prediction."
  },
  {
    "objectID": "slides/01-ar-1-structure.html#the-simplest-non-trivial-time-series-model",
    "href": "slides/01-ar-1-structure.html#the-simplest-non-trivial-time-series-model",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "The simplest non-trivial time series model",
    "text": "The simplest non-trivial time series model\nThe autoregression of order 1, or AR(1):\n\n\\[\n\\begin{aligned}\ny_t\n&=\n\\beta_0\n+\n\\beta_1\ny_{t-1}\n+\n\\varepsilon_t,\n\\quad\n\\varepsilon_t\\iid\\text{N}(0\\com\\sigma^2)\n\\\\\ny_0\n&\\sim\n\\text{N}(\\mu_0\\com \\initvar),\n\\end{aligned}\n\\]\n\n\nThat’s the recursive form. It implies a joint distribution written marginal-conditional style:\n\n\n\\[\n\\begin{aligned}\np(y_{0:T})\n&=\np(y_0)\n\\prod_{t=1}^T\np(y_t\\given y_{0:t-1})\n\\\\\n&=\np(y_0)\n\\prod_{t=1}^T\np(y_t\\given y_{t-1}).\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#putting-the-regression-in-autoregression",
    "href": "slides/01-ar-1-structure.html#putting-the-regression-in-autoregression",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Putting the “regression” in autoregression",
    "text": "Putting the “regression” in autoregression\nIn some sense the AR(1) is “just” a simple linear regression\n\\[\ny_t\n=\n\\beta_0\n+\n\\beta_1\nx_t\n+\n\\varepsilon_t,\n\\quad\n\\varepsilon_t\\iid\\text{N}(0\\com\\sigma^2),\n\\]\nwhere we took the predictor to be \\(x_t=y_{t-1}\\).\nThis perspective obscures the dependence structure, but will be useful for likelihood-based inference next week."
  },
  {
    "objectID": "slides/01-ar-1-structure.html#the-game-plan-for-the-next-few-lectures",
    "href": "slides/01-ar-1-structure.html#the-game-plan-for-the-next-few-lectures",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "The game plan for the next few lectures",
    "text": "The game plan for the next few lectures\nDo “All of Time Series Analysis” for this simple model:\n\nwhat is the joint distribution and what is its structure (marginals, conditionals, moments);\nstationarity;\nclassical inference;\nBayesian inference;\n(emphasizing recursive estimation in both cases);\nprobabilistic forecasting from both inferential perspectives;\nevaluating probabilistic forecasts.\n\n\nThe rest of the course is in some sense theme and variations."
  },
  {
    "objectID": "slides/01-ar-1-structure.html#last-time-substitution-fest",
    "href": "slides/01-ar-1-structure.html#last-time-substitution-fest",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Last time: substitution fest",
    "text": "Last time: substitution fest\n\n\\(t=0\\):\n\\[\ny_0=y_0\n\\]\n\n\n\\(t=1\\):\n\\[\ny_1=\\beta_0+\\beta_1y_0+\\varepsilon_1\n\\]\n\n\n\\(t=2\\):\n\\[\n\\begin{aligned}\ny_2\n&=\n\\beta_0\n+\n\\beta_1\ny_1\n+\n\\varepsilon_2\n\\\\\n&=\n\\beta_0\n+\n\\beta_1\n(\\beta_0+\\beta_1y_0+\\varepsilon_1)\n+\n\\varepsilon_2\n\\\\\n&=\n\\beta_0+\\beta_0\\beta_1+\\beta_1^2y_0+\\beta_1\\varepsilon_1+\\varepsilon_2.\n\\end{aligned}\n\\]\n\n\n\\(t=3\\):\n\\[\n\\begin{aligned}\ny_3\n&=\n\\beta_0\n+\n\\beta_1\ny_2\n+\n\\varepsilon_3\n\\\\\n&=\n\\beta_0\n+\n\\beta_1\n(\\beta_0+\\beta_0\\beta_1+\\beta_1^2y_0+\\beta_1\\varepsilon_1+\\varepsilon_2)\n+\n\\varepsilon_3\n\\\\\n&=\n\\beta_0+\\beta_0\\beta_1+\\beta_0\\beta_1^2+\\beta_1^3y_0+\\beta_1^2\\varepsilon_1+\\beta_1\\varepsilon_2+\\varepsilon_3.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#get-organized-and-notice-the-pattern",
    "href": "slides/01-ar-1-structure.html#get-organized-and-notice-the-pattern",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Get organized and notice the pattern",
    "text": "Get organized and notice the pattern\n\\[\n\\begin{aligned}\ny_0\n&=\n&\ny_0\n\\\\\ny_1\n&=\n\\beta_0\n&+\\,\n\\beta_1y_0\n&+\\,\n{\\color{white}\\beta_{\\color{white}1}}\\varepsilon_1\n\\\\\ny_2\n&=\n\\beta_0+\\beta_0\\beta_1\n&+\\,\n\\beta_1^2y_0\n&+\\,\n\\beta_1\\varepsilon_1\n+\n{\\color{white}\\beta_{\\color{white}1}}\\varepsilon_2\n\\\\\ny_3\n&=\n\\beta_0+\\beta_0\\beta_1+\\beta_0\\beta_1^2\n&+\\,\n\\beta_1^3y_0\n&+\\,\n\\beta_1^2\\varepsilon_1\n+\n\\beta_1\\varepsilon_2\n+\n\\varepsilon_3\n\\\\\n&\\vdots\n\\\\\ny_t\n&=\n\\beta_0\\sum\\limits_{i=0}^{t-1}\\beta_1^i\n&+\\,\n\\beta_1^ty_0\n&+\\,\n\\sum\\limits_{i=0}^{t-1}\\beta_1^i\\varepsilon_{t-i}\n\\\\\n&\\vdots\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#the-matrix-equation",
    "href": "slides/01-ar-1-structure.html#the-matrix-equation",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "The matrix equation",
    "text": "The matrix equation\nWriting the linear system as a matrix equation, you get:\n\n\\[\n\\begin{aligned}\n\\underbrace{\n\\begin{bmatrix}\ny_0\n\\\\\ny_1\n\\\\\ny_2\n\\\\\ny_3\n\\\\\n\\vdots\n\\\\\ny_T\n\\end{bmatrix}\n}_{\\By}\n&=\n\\underbrace{\n\\beta_0\n\\begin{bmatrix}\n0\n\\\\\n1\n\\\\\n1+\\beta_1\n\\\\\n1+\\beta_1+\\beta_1^2\n\\\\\n\\vdots\n\\\\\n\\sum\\limits_{i=0}^{T-1}\\beta_1^i\n\\end{bmatrix}\n}_{\\Bc}\n+\n\\underbrace{\n\\begin{bmatrix}\n1 & 0 & 0 & 0 & \\cdots & 0 \\\\\n\\beta_1 & 1 & 0 & 0 & \\cdots & 0 \\\\\n\\beta_1^2 & \\beta_1 & 1 & 0 & \\cdots & 0 \\\\\n\\beta_1^3 & \\beta_1^2 & \\beta_1 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\beta_1^T & \\beta_1^{T-1} & \\beta_1^{T-2} & \\beta_1^{T-3} & \\cdots & 1 \\\\\n\\end{bmatrix}\n}_{\\BA}\n\\underbrace{\n\\begin{bmatrix}\ny_0\n\\\\\n\\varepsilon_1\n\\\\\n\\varepsilon_2\n\\\\\n\\varepsilon_3\n\\\\\n\\vdots\n\\\\\n\\varepsilon_T\n\\end{bmatrix}\n}_{\\Be}\n.\n\\end{aligned}\n\\]\n\n\nBy assumption,\n\\[\n\\Be\n\\sim\n\\text{N}_{T+1}\n\\left(\n\\Bm=\n\\begin{bmatrix}\n\\mu_0\n\\\\\n\\Bzero\n\\end{bmatrix}\n\\com\n\\BS\n=\n\\begin{bmatrix}\n\\initvar\n&\n\\Bzero^\\tr\\\\\n\\Bzero & \\sigma^2\\BI_T\n\\end{bmatrix}\n\\right)\n.\n\\]\n\n\nSo by linearity,\n\\[\n\\By\n\\sim\n\\text{N}_{T+1}\n\\left(\n\\Bmu=\\Bc+\\BA\\Bm\n\\com\n\\BSigma\n=\n\\BA\\BS\\BA^\\tr\n\\right)\n.\n\\]"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#whats-the-mean",
    "href": "slides/01-ar-1-structure.html#whats-the-mean",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "What’s the mean?",
    "text": "What’s the mean?\n\nRecall that\n\\[\nE\\left(\n\\sum\\limits_{i=1}^na_iX_i\n\\right)\n=\n\\sum\\limits_{i=1}^na_iE(X_i).\n\\]\n\n\nSo:\n\\[\n\\begin{aligned}\nE(y_t)\n&=\nE\n\\left(\n\\beta_0\\sum\\limits_{i=0}^{t-1}\\beta_1^i\n+\n\\beta_1^ty_0\n+\n\\sum\\limits_{i=0}^{t-1}\\beta_1^i\\varepsilon_{t-i}\n\\right)\n\\\\\n&=\n\\beta_0\\sum\\limits_{i=0}^{t-1}\\beta_1^i\n+\n\\beta_1^t\nE\n\\left(\ny_0\n\\right)\n+\n\\sum\\limits_{i=0}^{t-1}\\beta_1^i\nE\n\\left(\n\\varepsilon_{t-i}\n\\right)\n\\\\\n&=\n\\beta_0\\sum\\limits_{i=0}^{t-1}\\beta_1^i\n+\n\\beta_1^t\n\\mu_0.\n\\end{aligned}\n\\]\n\n\nWe only used \\(E(\\varepsilon_t)=0\\). Didn’t need independence or normality."
  },
  {
    "objectID": "slides/01-ar-1-structure.html#whats-the-variance",
    "href": "slides/01-ar-1-structure.html#whats-the-variance",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "What’s the variance?",
    "text": "What’s the variance?\n\nRecall that for independent random variables,\n\\[\n\\var\\left(\n\\sum\\limits_{i=1}^na_iX_i\n\\right)\n=\n\\sum\\limits_{i=1}^na_i^2\\var(X_i).\n\\]\n\n\nSo:\n\\[\n\\begin{aligned}\n\\var(y_t)\n&=\n\\var\n\\left(\n\\beta_0\\sum\\limits_{i=0}^{t-1}\\beta_1^i\n+\n\\beta_1^ty_0\n+\n\\sum\\limits_{i=0}^{t-1}\\beta_1^i\\varepsilon_{t-i}\n\\right)\n\\\\\n&=\n\\beta_1^{2t}\n\\var\n\\left(\ny_0\n\\right)\n+\n\\sum\\limits_{i=0}^{t-1}\\beta_1^{2i}\n\\var\n\\left(\n\\varepsilon_{t-i}\n\\right)\n\\\\\n&=\n\\beta_1^{2t}\\initvar\n+\n\\sigma^2\n\\sum\\limits_{i=0}^{t-1}\\beta_1^{2i}.\n\\end{aligned}\n\\]\n\n\nWe used time-invariance and independence of \\(\\varepsilon_t\\) but not normality."
  },
  {
    "objectID": "slides/01-ar-1-structure.html#whats-the-covariance",
    "href": "slides/01-ar-1-structure.html#whats-the-covariance",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "What’s the covariance?",
    "text": "What’s the covariance?\nHere you go:\n\\[\n\\cov(y_t\\com y_s)\n=\n\\begin{cases}\n\\beta_1^{s-t}\\var(y_t) & t\\leq s\\\\\n\\beta_1^{t-s}\\var(y_s) & s &lt; t.\n\\end{cases}\n\\]\nDerivation deferred to Problem Set 1."
  },
  {
    "objectID": "slides/01-ar-1-structure.html#summary",
    "href": "slides/01-ar-1-structure.html#summary",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Summary",
    "text": "Summary\n\nRecursive form:\n\\[\n\\begin{aligned}\ny_t\n&=\n\\beta_0\n+\n\\beta_1\ny_{t-1}\n+\n\\varepsilon_t,\n\\quad\n\\varepsilon_t\\iid\\text{N}(0\\com\\sigma^2)\n\\\\\ny_0\n&\\sim\n\\text{N}(\\mu_0\\com \\initvar).\n\\end{aligned}\n\\]\n\n\nJoint distribution:\n\\[\n\\begin{bmatrix}\ny_0 & y_1 & \\cdots & y_T\n\\end{bmatrix}^\\tr\n\\sim\\text{N}_{T+1}\\left(\\Bmu\\com \\BSigma\\right).\n\\]\n\n\nMoments:\n\\[\n\\begin{aligned}\nE(y_t)\n&=\n\\beta_0\\sum\\limits_{i=0}^{t-1}\\beta_1^i\n+\n\\beta_1^t\\mu_0\n\\\\\n\\var(y_t)\n&=\n\\sigma^2\n\\sum\\limits_{i=0}^{t-1}\\beta_1^{2i}+\n\\beta_1^{2t}\\initvar\n\\\\\n\\cov(y_t\\com y_s)\n&=\n\\begin{cases}\n\\beta_1^{s-t}\\var(y_t) & t\\leq s\\\\\n\\beta_1^{t-s}\\var(y_s) & s &lt; t.\n\\end{cases}\n.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#some-special-cases",
    "href": "slides/01-ar-1-structure.html#some-special-cases",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Some special cases",
    "text": "Some special cases\n\n\niid: set \\(\\beta_1=0\\) (and \\(\\mu_0=\\beta_0\\); \\(\\initvar=\\sigma^2\\)), and\n\n\n\\[\ny_t\\iid\\text{N}(\\beta_0\\com\\sigma^2).\n\\]\n\n\n\n\nrandom walk with drift: set \\(\\beta_1=1\\), and\n\n\n\n\\[\n\\begin{aligned}\nE(y_t)\n&=\n\\beta_0t+\\mu_0\n\\\\\n\\var(y_t)\n&=\n\\sigma^2t+\\initvar.\n\\end{aligned}\n\\]\n\n\n\n\nfunky: set \\(\\beta_1=-1\\), and\n\n\n\n\\[\n\\begin{aligned}\nE(y_t)\n&=\n\\beta_0\\frac{1 - (-1)^t}{2}+(-1)^t\\mu_0\n\\\\\n\\var(y_t)\n&=\n\\sigma^2t+\\initvar.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#what-does-this-look-like",
    "href": "slides/01-ar-1-structure.html#what-does-this-look-like",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "What does this look like?",
    "text": "What does this look like?\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\nlibrary(shiny)\n\nsimulate_ar_1 &lt;- function(T, b0, b1, s, m0, s0){\n  y &lt;- numeric(T)\n  y[1] &lt;- rnorm(1, m0, s0)\n  for(t in 2:T){\n    y[t] &lt;- b0 + b1 * y[t - 1] + rnorm(1, 0, s)\n  }\n  return(y)\n}\n\nar_1_mean &lt;- function(t, b0, b1, m0){\n  if(t == 0){\n    return(m0)\n  }else{\n    return(b0 * sum(b1 ^ (0:(t-1))) + m0 * (b1^t)) \n  }\n}\n\nar_1_var &lt;- function(t, b1, s, s0){\n  if(t == 0){\n    return(s0^2)\n  }else{\n    return((s0^2) * (b1^(2*t)) + (s^2) * sum(b1 ^ (2*(0:(t-1)))))\n  }\n}\n\nar_1_sd &lt;- function(t, b1, s, s0){\n  sqrt(ar_1_var(t, b1, s, s0))\n}\n\n# Define UI for application that draws a histogram\nui &lt;- fluidPage(\n  \n  # Application title\n  titlePanel(\"Marginal distributions and sample paths of a Gaussian AR(1)\"),\n  \n  # Sidebar with a slider input for number of bins \n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"b0\",\n                  \"β₀\",\n                  min = -5,\n                  max = 5,\n                  value = 0,\n                  step = 0.1),\n      sliderInput(\"b1\",\n                  \"β₁\",\n                  min = -2,\n                  max = 2,\n                  value = 0,\n                  step = 0.1),\n      sliderInput(\"s\",\n                  \"σ\",\n                  min = 0,\n                  max = 2,\n                  value = 1, \n                  step = 0.1),\n      sliderInput(\"m0\",\n                  \"μ₀\",\n                  min = -5,\n                  max = 5,\n                  value = 0,\n                  step = 0.1),\n      sliderInput(\"T\",\n                  \"T\",\n                  min = 20,\n                  max = 200,\n                  step = 20,\n                  value = 100),\n      actionButton(\"redo\", \"New sample path\"),\n    ),\n    \n    # Show a plot of the generated distribution\n    mainPanel(\n      plotOutput(\"distPlot\", height = \"600px\")\n    )\n  )\n)\n\n# Define server logic required to draw a histogram\nserver &lt;- function(input, output) {\n  \n  output$distPlot &lt;- renderPlot({\n    input$redo\n    b0 &lt;- input$b0\n    b1 &lt;- input$b1\n    redo &lt;- input$redo\n    T &lt;- input$T\n    s &lt;- input$s\n    m0 &lt;- input$m0\n    s0 = 1\n    \n    range = 0:T\n    alpha = c(0.01, seq(0.1, 0.9, by = 0.1))\n    \n    middle &lt;- sapply(range, ar_1_mean, b0, b1, m0)\n    sds &lt;- sapply(range, ar_1_sd, b1, s, s0)\n    \n    \n    plot(range, middle, type = \"l\",\n         xaxt = \"n\", \n         yaxt = \"n\",\n         xlab = \"t\",\n         ylab = expression(y[t]),\n         ylim = c(-20, 20), bty = \"n\",\n         col = \"white\")\n    \n    for(a in alpha){\n      \n      U = qnorm(1 - a / 2, mean = middle, sd = sds)\n      L = qnorm(a / 2, mean = middle, sd = sds)\n      \n      polygon(\n        c(range, rev(range)),\n        c(U, rev(L)),\n        col = rgb(1, 0, 0, 0.15),\n        border = NA\n      )\n    }\n    \n    inc = 20\n    axis(1, pos = 0, at = seq(0, max(range), by = inc), \n         labels = c(NA, seq(inc, max(range), by = inc)))\n    axis(2, pos = 0)\n    \n    lines(range, simulate_ar_1(max(range) + 1, b0, b1, s, m0, s0), col = \"black\", lwd = 2)\n  })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#assume-beta_1neq-1",
    "href": "slides/01-ar-1-structure.html#assume-beta_1neq-1",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Assume \\(\\beta_1\\neq 1\\)\n",
    "text": "Assume \\(\\beta_1\\neq 1\\)\n\nFinite geometric sum formula gives:\n\\[\n\\begin{aligned}\nE(y_t)\n&=\n\\beta_0\\frac{1-\\beta_1^t}{1-\\beta_1}\n+\n\\beta_1^t\\mu_0\n\\\\\n\\var(y_t)\n&=\n\\sigma^2\n\\frac{1-\\beta_1^{2t}}{1-\\beta_1^2}+\n\\beta_1^{2t}\\initvar.\n\\end{aligned}\n\\]\nWhat happens as \\(t\\to\\infty\\)?"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#stationarity",
    "href": "slides/01-ar-1-structure.html#stationarity",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Stationarity",
    "text": "Stationarity\nA joint distribution is (strictly) stationary if it is “shift invariant”:\n\\[\n\\{y_{t_1}\\com y_{t_2}\\com ...\\com y_{t_n}\\}\\overset{d}{=}\\{y_{t_1+h}\\com y_{t_2+h}\\com ...\\com y_{t_n+h}\\}.\n\\]\nThe Gaussian AR(1) with \\(|\\beta_1|&lt;1\\) has this property."
  },
  {
    "objectID": "slides/01-ar-1-structure.html#stationary-ar1",
    "href": "slides/01-ar-1-structure.html#stationary-ar1",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Stationary AR(1)",
    "text": "Stationary AR(1)\nIf \\(-1&lt;\\beta_1&lt;1\\), \\(\\mu_0=\\beta_0/(1-\\beta_1)\\), and \\(\\initvar=\\sigma^2/(1-\\beta_1^2)\\), then the AR(1) is strictly stationary with the following:\n\\[\n\\begin{aligned}\nE(y_t)\n&=\n\\frac{\\beta_0}{1-\\beta_1}\n\\\\\n\\var(y_t)\n&=\n\\frac{\\sigma^2}{1-\\beta_1^2}\n\\\\\n\\cov(y_t\\com y_s)\n&=\n\\beta_1^{|t-s|}\\var(y_t)\n=\n\\beta_1^{|t-s|}\\frac{\\sigma^2}{1-\\beta_1^2}.\n\\end{aligned}\n\\]\nThe common marginal shared by all \\(y_t\\) is called the stationary distribution:\n\\[\ny_t\\sim\\text{N}\\left(\\frac{\\beta_0}{1-\\beta_1}\\com \\frac{\\sigma^2}{1-\\beta_1^2}\\right).\n\\]\nSo “did: dependent but identically distributed.”"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#chat-about-stationarity",
    "href": "slides/01-ar-1-structure.html#chat-about-stationarity",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Chat about stationarity",
    "text": "Chat about stationarity\n\nHopefully your first thought upon encountering this concept is “real data won’t be stationary.” True!\n\nSo why care about this?\n\nCute from a pure math point of view;\nIf you simulate distributions with Markov chain Monte Carlo (MCMC), you bow down at the altar of stationarity;\nStationarity means “dependent but not too dependent.” You can redo classical statistical theory replacing “iid” with “stationary,” and you don’t necessarily have to change a whole lot;\n(A Bayesian won’t necessarily care about that last point.)"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#autocovariance-of-a-stationary-process",
    "href": "slides/01-ar-1-structure.html#autocovariance-of-a-stationary-process",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Autocovariance of a stationary process",
    "text": "Autocovariance of a stationary process\nFor a stationary process, the covariance kernel satisfies\n\\[\n\\cov(y_t\\com y_{s})=\\cov(y_{t+h}\\com y_{s+h})\\quad \\forall (t\\com s\\com h).\n\\]\nSo you can define something called the autocovariance function:\n\\[\n\\gamma(h)=\\cov(y_{t+h}\\com y_{t}).\n\\]\nFor the AR(1), this is\n\\[\n\\begin{aligned}\n\\gamma(0)&=\\sigma^2/(1-\\beta_1^2)\n\\\\\n\\gamma(h)&=\\beta_1^h\\gamma(0).\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#what-does-the-autocov-function-look-like",
    "href": "slides/01-ar-1-structure.html#what-does-the-autocov-function-look-like",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "What does the autocov function look like?",
    "text": "What does the autocov function look like?"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#what-does-the-autocov-function-look-like-1",
    "href": "slides/01-ar-1-structure.html#what-does-the-autocov-function-look-like-1",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "What does the autocov function look like?",
    "text": "What does the autocov function look like?"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#what-does-the-autocov-function-look-like-2",
    "href": "slides/01-ar-1-structure.html#what-does-the-autocov-function-look-like-2",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "What does the autocov function look like?",
    "text": "What does the autocov function look like?"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#what-does-the-autocov-function-look-like-3",
    "href": "slides/01-ar-1-structure.html#what-does-the-autocov-function-look-like-3",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "What does the autocov function look like?",
    "text": "What does the autocov function look like?"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#what-does-the-autocov-function-look-like-4",
    "href": "slides/01-ar-1-structure.html#what-does-the-autocov-function-look-like-4",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "What does the autocov function look like?",
    "text": "What does the autocov function look like?"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#what-does-the-autocov-function-look-like-5",
    "href": "slides/01-ar-1-structure.html#what-does-the-autocov-function-look-like-5",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "What does the autocov function look like?",
    "text": "What does the autocov function look like?"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#plug-in-estimators-for-the-moments",
    "href": "slides/01-ar-1-structure.html#plug-in-estimators-for-the-moments",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Plug-in estimators for the moments",
    "text": "Plug-in estimators for the moments\nIf your AR(1) is stationary, then given data, expected values like\n\n\\[\n\\begin{aligned}\n\\mu\n&=\nE(y_t)\\\\\n\\gamma(h)\n&=\nE[(y_{t+h}-\\mu)(y_t-\\mu)],\n\\end{aligned}\n\\]\n\n\ncan be estimated with simple sample averages\n\n\n\\[\n\\begin{aligned}\n\\hat{\\mu}_T\n&=\n\\frac{1}{T}\\sum\\limits_{t=1}^Ty_t\n\\\\\n\\hat{\\gamma}_T(h)\n&=\n\\frac{1}{T}\\sum\\limits_{t=1}^{T-h}(y_{t+h}-\\hat{\\mu}_T)(y_t-\\hat{\\mu}_T).\n\\end{aligned}\n\\]\n\n\nThe last is the sample autocovariance function."
  },
  {
    "objectID": "slides/01-ar-1-structure.html#recall-some-facts-about-the-stationary-ar1",
    "href": "slides/01-ar-1-structure.html#recall-some-facts-about-the-stationary-ar1",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Recall some facts about the stationary AR(1)",
    "text": "Recall some facts about the stationary AR(1)\nHere’s what we’ve got:\n\n\\[\n\\begin{aligned}\n\\gamma(1)&=\\beta_1\\gamma(0) &&\\implies\\beta_1=\\frac{\\gamma(1)}{\\gamma(0)}\\\\\n\\mu&=\\frac{\\beta_0}{1-\\beta_1}&&\\implies\\beta_0=(1-\\beta_1)\\mu\\\\\n\\gamma(0)&=\\frac{\\sigma^2}{1-\\beta_1^2}&&\\implies \\sigma^2=(1-\\beta_1^2)\\gamma(0).\n\\end{aligned}\n\\]\n\n\nSo…any ideas how to estimate these?"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#the-baby-yule-walker-equations",
    "href": "slides/01-ar-1-structure.html#the-baby-yule-walker-equations",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "The (baby) Yule-Walker equation(s)",
    "text": "The (baby) Yule-Walker equation(s)\nEverywhere you see a “population” expected value, plug in the sample version:\n\n\\[\n\\begin{aligned}\n\\hat{\\beta}_1&=\\frac{\\hat{\\gamma}_T(1)}{\\hat{\\gamma}_T(0)}\n\\\\\n\\hat{\\beta}_0&=(1-\\hat{\\beta}_1)\\hat{\\mu}_T\\\\\n\\hat{\\sigma^2_T}&=(1-\\hat{\\beta}_1^2)\\hat{\\gamma}_T(0).\n\\end{aligned}\n\\]\nThese are method of moments estimators for the AR(1) parameters."
  },
  {
    "objectID": "slides/01-ar-1-structure.html#asymptotics",
    "href": "slides/01-ar-1-structure.html#asymptotics",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Asymptotics",
    "text": "Asymptotics\n\n\n\n\n\n\nShumway and Stoffer (2025) Property 3.7\n\n\nFor a stationary AR(1), the Yuke-Walker estimator has:\n\\[\n\\sqrt{T}(\\hat{\\beta}_1-\\beta_1)\\cd\\N\\left(0\\com\\frac{\\sigma^2}{\\gamma(0)}\\right).\n\\] And note in this case that \\(\\sigma^2 / \\gamma(0)=1-\\beta_1^2\\). The result remains true if you plug in the estimator of the asymptotic variance.\n\n\n\n\nAsymptotically valid \\(100\\times(1-\\alpha)\\%\\) confidence interval:\n\\[\n\\hat{\\beta}_1\\pm z_{1-\\alpha/2}\\sqrt{\\frac{1-\\hat{\\beta}_1^2}{T}}.\n\\]"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#what-happens-for-beta_1-close-to-pm1",
    "href": "slides/01-ar-1-structure.html#what-happens-for-beta_1-close-to-pm1",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "What happens for \\(\\beta_1\\) close to \\(\\pm1\\)?",
    "text": "What happens for \\(\\beta_1\\) close to \\(\\pm1\\)?\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\n# app.R\nlibrary(shiny)\n\n# ---------- Configuration ----------\nSAMPLE_SIZES &lt;- seq(10, 200, by = 20)   # smaller default for speed\nNSIM &lt;- 500                             # number of simulated AR(1) paths per sample size\nY_LIMITS &lt;- c(-1, 1)\n# ------------------------------------\n\n# Yule-Walker AR(1) estimator\nyule_walker_ar1 &lt;- function(x) {\n  n &lt;- length(x)\n  x &lt;- x - mean(x)  # center\n  gamma0 &lt;- mean(x^2)\n  gamma1 &lt;- mean(x[-1] * x[-n])\n  gamma1 / gamma0\n}\n\n# Simulate one stationary AR(1) with variance 1\nsimulate_ar_1 &lt;- function(T, b0, b1, s, m0, s0){\n  y &lt;- numeric(T)\n  y[1] &lt;- rnorm(1, m0, s0)\n  for(t in 2:T){\n    y[t] &lt;- b0 + b1 * y[t - 1] + rnorm(1, 0, s)\n  }\n  return(y)\n}\n\nui &lt;- fluidPage(\n  titlePanel(\"Sampling distribution of Yule–Walker AR(1) estimator\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"b1\", \"True value of β₁:\",\n                  min = -0.99, max = 0.99, value = 0.5, step = 0.01)\n    ),\n    mainPanel(\n      plotOutput(\"boxPlot\", height = \"600px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  \n  output$boxPlot &lt;- renderPlot({\n    b0 &lt;- 0\n    b1 &lt;- input$b1\n    s &lt;- 1\n    m0 &lt;- b0 / (1 - b1)\n    s0 &lt;- s / sqrt(1 - b1^2)\n    sizes &lt;- SAMPLE_SIZES\n    nsim &lt;- NSIM\n    \n    # Collect estimates in a list, one element per sample size\n    est_list &lt;- vector(\"list\", length(sizes))\n    \n    for (i in seq_along(sizes)) {\n      n &lt;- sizes[i]\n      phi_hats &lt;- numeric(nsim)\n      for (s in 1:nsim) {\n        x &lt;- simulate_ar_1(n, b0, b1, s, m0, s0)#simulate_ar1(n, phi)\n        phi_hats[s] &lt;- yule_walker_ar1(x)\n      }\n      est_list[[i]] &lt;- phi_hats\n    }\n    \n    # Draw boxplots side by side\n    par()\n    boxplot(est_list,\n            names = sizes,\n            ylim = Y_LIMITS,\n            xlab = \"Sample size T\",\n            ylab = \"Estimate\",\n            main = paste(\"True value: \", b1),\n            col = \"lightgray\", pch = 19)\n    \n    abline(h = b1, col = \"red\", lty = 2, lwd = 2)\n  })\n}\n\nshinyApp(ui, server)"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#a-word-about-assumptions",
    "href": "slides/01-ar-1-structure.html#a-word-about-assumptions",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "A word about assumptions",
    "text": "A word about assumptions\n\n\nNormality was not essential, but stationarity absolutely was:\n\nthere is no such thing as \\(\\gamma(h)\\) without it;\nthe estimating equations for method-of-moments make no sense if the process isn’t stationary;\n\n\nBut interesting “real-world” data probably are not stationary, and so it would be nice to have estimation techniques that don’t completely break down without it.\n\n\n…what’s that?\n\n\n\n\n\nDo you hear something?\n\n\n\n\n\nPeekaboo"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STA 542 Introduction to Time Series Analysis",
    "section": "",
    "text": "Below is a prospective outline for the course. Due dates are firm, but topics may change with advanced notice.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWEEK\nDATE\nPREPARE\nTOPIC\nMATERIALS\nDUE\n\n\n\n1\nMon, Aug 25\n\n🧑‍🏫 Welcome!\nslides\n\n\n\n\nWed, Aug 27\n\n🧑‍🏫 AR(1) structure\nslides\n\n\n\n2\nMon, Sep 1\n\n❌ Labor Day - No Lecture\n\n\n\n\n\n\nWed, Sep 3\n\n🧑‍🏫 AR(1) inference\nslides\n\n\n\n\nFri, Sep 5\n\n\n\n\nPSET 0 @ 5PM\n\n\n3\nMon, Sep 8\n\n🧑‍🏫 AR(1) forecasting\nslides\n\n\n\n\nWed, Sep 10\nGneiting et al (2007 JRSSB)\n🧑‍🏫 Forecast evaluation\nslides\n\n\n\n4\nMon, Sep 15\n\nSS: Ch. 3\n🧑‍🏫 AR(p) structure\ndraft\n\n\n\n\nWed, Sep 17\n\nSS: Ch. 3\n🧑‍🏫 AR(p) inference\ndraft\n\n\n\n5\nMon, Sep 22\n\nSS: Ch. 3\n🧑‍🏫 Model selection\n\n\nPSET 1 @ 11:30AM\n\n\n\nWed, Sep 24\n\nSS: Ch. 3\n🧑‍🏫 MA\ndraft\n\n\n\n6\nMon, Sep 29\n\nSS: Ch. 3\n🧑‍🏫 ARMA\ndraft\n\n\n\n\nWed, Oct 1\n\nSS: Ch. 3\n🧑‍🏫 ARMA\ndraft\n\n\n\n\nFri, Oct 3\n\n\n\n\n\n\n7\nMon, Oct 6\n\nSS: Ch. 3\n🧑‍🏫 ARMA\n\n\nPSET 2 @ 11:30AM\n\n\n\nWed, Oct 8\n\n📝 Exam 1\n\n\n\n\n\n8\nMon, Oct 13\n\n❌ Fall Break - No Lecture\n\n\n\n\n\n\nWed, Oct 15\n\nSS: Ch. 5.5\n🧑‍🏫 VAR\n\n\n\n\n9\nMon, Oct 20\nKarlsson (2013 chapter)\n🧑‍🏫 VAR\n\n\n\n\n\nWed, Oct 22\n\n🧑‍🏫 VAR\n\n\n\n\n\nFri, Oct 24\n\n\n\nPSET 3 @ 5PM\n\n\n10\nMon, Oct 27\n\nPPC: Ch. 2\n🧑‍🏫 DLMs\n\n\n\n\n\nWed, Oct 29\n\nSkä: Ch. 4\n🧑‍🏫 DLMs\n\n\n\n\n11\nMon, Nov 3\n\nSkä: Ch. 8\n🧑‍🏫 DLMs\n\n\n\n\n\nWed, Nov 5\n\n🧑‍🏫 DLMs\n\n\n\n\n\nFri, Nov 7\n\n\n\nPSET 4 @ 5PM\n\n\n12\nMon, Nov 10\nScott (2002 JASA)\n🧑‍🏫 HMMs\n\n\n\n\n\nWed, Nov 12\n\n🧑‍🏫 HMMs\n\n\n\n\n13\nMon, Nov 17\nDoucet, Johansen (2008 chapter)\n🧑‍🏫 Nonlinear models\n\n\n\n\n\nWed, Nov 19\n\n🧑‍🏫 Nonlinear models\n\n\n\n\n\nFri, Nov 21\n\n\n\nPSET 5 @ 5PM\n\n\n14\nMon, Nov 24\n\n📝 Exam 2\n\n\n\n\n\n\nWed, Nov 26\n\n❌ Thanksgiving - No Lecture\n\n\n\n\n\n16\nWed, Dec 10\n\n\n\nProject @ 9AM"
  },
  {
    "objectID": "problems/pset-0.html",
    "href": "problems/pset-0.html",
    "title": "Problem Set 0",
    "section": "",
    "text": "In this class, I expect you to have a working knowledge of:\nBelow is a workout in all of the above.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "problems/pset-0.html#problem-1",
    "href": "problems/pset-0.html#problem-1",
    "title": "Problem Set 0",
    "section": "Problem 1",
    "text": "Problem 1\nSlicing-and-dicing the multivariate normal distribution is a very important skill in time series analysis, so let’s make sure we can do that.\n\nLet \\(\\mathbf{x}\\sim\\text{N}_n(\\boldsymbol{\\mu},\\,\\boldsymbol{\\Sigma})\\), and fix constants \\(\\mathbf{A}\\in\\mathbb{R}^{m\\times n}\\) and \\(\\mathbf{c}\\in\\mathbb{R}^m\\). Show that\n\n\\[\n\\mathbf{y}\n=\n\\mathbf{A}\n\\mathbf{x}\n+\n\\mathbf{c}\n\\sim\n\\text{N}_m\n\\left(\n\\mathbf{A}\n\\boldsymbol{\\mu}\n+\n\\mathbf{c}\n,\\,\n\\mathbf{A}\n\\boldsymbol{\\Sigma}\n\\mathbf{A}^{\\scriptscriptstyle\\mathsf{T}}\n\\right)\n.\n\\]\n\nConsider the joint distribution \\(p(\\mathbf{x},\\,\\mathbf{y})=p(\\mathbf{y}\\,|\\,\\mathbf{x})p(\\mathbf{x})\\) written hierarchically as \\(\\mathbf{x}\\sim\\text{N}_n(\\boldsymbol{\\mu},\\,\\boldsymbol{\\Sigma})\\) and \\(\\mathbf{y}\\,|\\,\\mathbf{x}\\sim\\text{N}_m(\\mathbf{A}\\mathbf{x}+\\mathbf{c},\\,\\mathbf{R})\\). Use the result in part a to show that the joint distribution implied by this hierarchy is\n\n\\[\n\\begin{bmatrix}\n\\mathbf{x}\n\\\\\n\\mathbf{y}\n\\end{bmatrix}\n\\sim\n\\text{N}_{n+m}\n\\left(\n\\begin{bmatrix}\n\\boldsymbol{\\mu}\n\\\\\n\\mathbf{A}\\boldsymbol{\\mu}+\\mathbf{c}\n\\end{bmatrix}\n,\\,\n\\begin{bmatrix}\n\\boldsymbol{\\Sigma}\n&\n\\boldsymbol{\\Sigma}\\mathbf{A}^{\\scriptscriptstyle\\mathsf{T}}\n\\\\\n\\mathbf{A}\n\\boldsymbol{\\Sigma}\n&\n\\mathbf{A}\n\\boldsymbol{\\Sigma}\n\\mathbf{A}^{\\scriptscriptstyle\\mathsf{T}} + \\mathbf{R}\n\\end{bmatrix}\n\\right)\n.\n\\]\n\nUse the result in part a to show that the linear combination of independent normals is normal. That is, if \\(x_i\\sim\\text{N}(\\mu_i,\\,\\sigma^2_i)\\) are independent and \\(a_i\\in\\mathbb{R}\\) are constant, then prove that\n\n\\[\n\\sum\\limits_{i=1}^na_ix_i\\sim\\text{N}\\left(\\sum\\limits_{i=1}^na_i\\mu_i,\\, \\sum\\limits_{i=1}^na_i^2\\sigma_i^2\\right).\n\\]\n\n\n\n\n\n\nHint\n\n\n\n\n\nJZ changed this hint on 8/30/2025. The original hint mentioned using the change-of-variables formula. That’s a fine solution if \\(\\mathbf{A}\\) is invertible, but the result holds even if \\(\\mathbf{A}\\) is not invertible, and we will often use that version (e.g. part c here). If you used change-of-variables, don’t redo it. We’ll award full credit if you did it right. But otherwise, consider using the moment-generating function (mgf) of a random vector \\(\\mathbf{x}\\):\n\\[\nM(\\mathbf{t})=E\\left(e^{\\mathbf{t}^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{x}}\\right).\n\\]\nAs in the univariate case, when it exists, the mgf uniquely characterizes the entire distribution of a random vector, just like the density, cdf, or characteristic function do.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "problems/pset-0.html#problem-2",
    "href": "problems/pset-0.html#problem-2",
    "title": "Problem Set 0",
    "section": "Problem 2",
    "text": "Problem 2\nRecall that if \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are jointly distributed according to\n\\[\n\\begin{bmatrix}\n\\mathbf{x} \\\\\n\\mathbf{y}\n\\end{bmatrix}\n\\sim\n\\text{N}_{n+m}\n\\left(\n\\begin{bmatrix}\n\\boldsymbol{\\mu}_x \\\\\n\\boldsymbol{\\mu}_y\n\\end{bmatrix}\n,\\,\n\\begin{bmatrix}\n\\boldsymbol{\\Sigma}_x & \\boldsymbol{\\Sigma}_{xy}\\\\\n\\boldsymbol{\\Sigma}_{xy}^{\\scriptscriptstyle\\mathsf{T}} & \\boldsymbol{\\Sigma}_y\n\\end{bmatrix}\n\\right),\n\\]\nthen the conditional distribution is\n\\[\n\\mathbf{y} \\,|\\,\\mathbf{x}\n\\sim\n\\text{N}_m\n\\left(\n\\boldsymbol{\\mu}_y + \\boldsymbol{\\Sigma}_{xy}^{\\scriptscriptstyle\\mathsf{T}}\\boldsymbol{\\Sigma}_x^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_x)\n,\\,\n\\boldsymbol{\\Sigma}_y - \\boldsymbol{\\Sigma}_{xy}^{\\scriptscriptstyle\\mathsf{T}}\n\\boldsymbol{\\Sigma}_x^{-1}\n\\boldsymbol{\\Sigma}_{xy}\n\\right).\n\\]\nWe will use this fact often, but proving it is tedious. I’m not going to make you do it. This problem set is unreasonable enough already. Instead, I want you to read the proof very carefully. If you’re like me, there is some step in the derivation that will make you say “what just happened?” Drill down on that. Pick a section of the proof where you get confused, or where a technique is being used that you are unfamiliar with. Study that part carefully until you understand it, and then in your problem set write-up, explain it to me like I’m an idiot (shouldn’t be too difficult). Use pictures, numerical examples, specialize to the bivariate case, whatever you need to do.\n\n\n\n\n\n\nDon’t be a silly goose.\n\n\n\nWe are grading this part of the problem set for completion, but give it a good faith effort so that you actually, like, learn something.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "problems/pset-0.html#problem-3",
    "href": "problems/pset-0.html#problem-3",
    "title": "Problem Set 0",
    "section": "Problem 3",
    "text": "Problem 3\nConsider these data\n\\[\nX_1,\\,X_2,\\,...,\\,X_n\\overset{\\text{iid}}{\\sim}\\text{N}(\\theta,\\,1)\n\\]\nand the usual estimator \\(\\hat{\\theta}_n=\\sum\\limits_{i=1}^nX_i/n\\).\n\nDerive an exact \\(100\\times (1-\\alpha)\\%\\) classical confidence interval for the unknown mean \\(\\theta\\). That is, find a random interval \\((L_n,\\,U_n)\\) satisfying:\n\n\\[\nP\\left(L_n&lt;\\theta&lt;U_n\\right)=1-\\alpha.\n\\]\n\nDerive an exact \\(100\\times (1-\\alpha)\\%\\) classical prediction interval for a new observation \\(X_{n+1}\\). That is, find a random interval \\((L_n',\\,U_n')\\) satisfying\n\n\\[\nP\\left(L_n'&lt;X_{n+1}&lt;U_n'\\right)=1-\\alpha.\n\\]\n\nComment on the difference between the two intervals.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "problems/pset-0.html#problem-4",
    "href": "problems/pset-0.html#problem-4",
    "title": "Problem Set 0",
    "section": "Problem 4",
    "text": "Problem 4\nHere is a cdf indexed by a parameter \\(\\theta&gt;0\\):\n\\[\nF(x;\\,\\theta)\n=\n1 - \\frac{1}{(1+x)^\\theta},\\quad x\\geq 0.\n\\]\n\nDescribe an algorithm that will simulate (pseudo)random numbers from this distribution;\nLet \\(x_1,\\,x_2,\\,...,\\,x_n\\overset{\\text{iid}}{\\sim}F(x;\\,\\theta)\\) and derive the maximum likelihood estimator \\(\\hat{\\theta}_n^{(\\text{MLE})}\\) for the parameter \\(\\theta\\);\nDerive the exact sampling distribution of \\(\\hat{\\theta}_n^{(\\text{MLE})}\\);\nImplement the bootstrap to simulate the sampling distribution of \\(\\hat{\\theta}_n^{(\\text{MLE})}\\), and compare the bootstrap distribution to the exact one you derived in part c. In order to do this, you will probably need your algorithm from part a.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nHere is a schematic of how the bootstrap works. We start with some ground truth parameter value \\(\\theta_0&gt;0\\), and then implement this:\n\\[\n\\begin{matrix}\n\\text{0. True distribution} &&& F_{\\theta_0} && \\\\\n&&& \\downarrow && \\\\\n\\text{1. Original data} &&& x_{1:n} && \\\\\n&&& \\downarrow && \\\\\n\\text{2. Empirical distribution} &&& \\hat{F}_{n} && \\\\\n&\\swarrow &\\swarrow& \\cdots &\\searrow&\\searrow \\\\\n\\text{3. Synthetic data}&\\tilde{x}_{1:n}^{(1)} &\\tilde{x}_{1:n}^{(2)}& \\cdots &\\tilde{x}_{1:n}^{(k-1)}&\\tilde{x}_{1:n}^{(k)} \\\\\n&\\downarrow &\\downarrow& \\cdots &\\downarrow&\\downarrow \\\\\n\\text{4. Bootstrap estimates}&\\hat{\\theta}_n^{(1)} &\\hat{\\theta}_n^{(2)}& \\cdots &\\hat{\\theta}_n^{(k-1)}&\\hat{\\theta}_n^{(k)} \\\\\n\\end{matrix}\n\\]\nIf you implemented this correctly, and if the original sample size \\(n\\) is “large enough,” then a histogram of the \\(k\\) estimates \\(\\hat{\\theta}_n^{(1)}\\), \\(\\hat{\\theta}_n^{(2)}\\), …, \\(\\hat{\\theta}_n^{(k)}\\) ought to resemble the exact density you derived in part c.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "problems/pset-0.html#problem-5",
    "href": "problems/pset-0.html#problem-5",
    "title": "Problem Set 0",
    "section": "Problem 5",
    "text": "Problem 5\nRecall the parametric family from the previous part, and consider the Bayesian model\n\\[\n\\begin{aligned}\n\\theta &\\sim \\text{Gamma}(a_0,\\,b_0) \\\\\nx_i\\,|\\, \\theta &\\overset{\\text{iid}}{\\sim }F(x;\\,\\theta).\n\\end{aligned}\n\\]\n\nDerive the posterior distribution \\(p(\\theta\\,|\\,x_{1:n})\\);\nShow that the posterior mean \\(E(\\theta\\,|\\,x_{1:n})\\) can be written as a convex combination of the prior mean and the MLE;\nDerive the posterior predictive distribution \\(p(x_{n+1}\\,|\\,x_{1:n})\\).",
    "crumbs": [
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "problems/pset-0.html#problem-6",
    "href": "problems/pset-0.html#problem-6",
    "title": "Problem Set 0",
    "section": "Problem 6",
    "text": "Problem 6\nAn important consideration in time series analysis is sequential or online inference. In this environment, our data are not available to us all at once in a batch. Instead, the observations are streaming; they are arriving one-after-another in real-time, and we want to come up with a scheme that allows us to recursively update our inferences as new information arrives. Let’s explore this in the context of least squares regression.\n\n\n\n\n\n\nOLS review\n\n\n\nConsider the usual setting where we observe iid pairs \\(y_i\\in\\mathbb{R}\\) and \\(\\mathbf{x}_i\\in\\mathbb{R}^p\\) from the linear model:\n\\[\ny_i= \\mathbf{x}_i^{\\scriptscriptstyle\\mathsf{T}}\\boldsymbol{\\beta}+\\varepsilon_i,\\quad \\varepsilon_i\\overset{\\text{iid}}{\\sim}\\text{N}(0,\\,\\sigma^2).\n\\] Given \\(n\\) observations, we can form the matrices\n\\[\n\\mathbf{y}_n\n=\n\\underbrace{\n\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix}\n}\n_{n\\times 1}\n\\quad\n\\mathbf{X}_n\n=\n\\underbrace{\n\\begin{bmatrix}\n\\mathbf{x}_1^{\\scriptscriptstyle\\mathsf{T}} \\\\\n\\mathbf{x}_2^{\\scriptscriptstyle\\mathsf{T}} \\\\\n\\vdots \\\\\n\\mathbf{x}_n^{\\scriptscriptstyle\\mathsf{T}}\n\\end{bmatrix}\n}_{n\\times p}\n\\quad\n\\boldsymbol{\\varepsilon}_n\n=\n\\underbrace{\n\\begin{bmatrix}\n\\varepsilon_1 \\\\\n\\varepsilon_2 \\\\\n\\vdots \\\\\n\\varepsilon_n\n\\end{bmatrix}\n}\n_{n\\times 1}\n,\n\\] and we can rewrite the model in matrix form:\n\\[\n\\mathbf{y}_n=\\mathbf{X}_n\\boldsymbol{\\beta}+\\boldsymbol{\\varepsilon}_n,\\quad \\boldsymbol{\\varepsilon}_n\\sim\\text{N}_n(\\mathbf{0},\\,\\sigma^2\\mathbf{I}_n).\n\\] The ordinary least squares (OLS) estimator of the regression coefficients is \\(\\hat{\\boldsymbol{\\beta}}_n=(\\mathbf{X}_n^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{X}_n)^{-1}\\mathbf{X}_n^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{y}_n.\\)\n\n\nHopefully that was all review, and it describes how to do batch or offline inference for the linear model. We are assuming that all of our data are available in a single batch, and we only need to compute the estimate once. But in our class, we will often care about streaming or online inference: efficiently computing the sequence of estimates \\(\\hat{\\boldsymbol{\\beta}}_1\\), \\(\\hat{\\boldsymbol{\\beta}}_2\\), \\(\\hat{\\boldsymbol{\\beta}}_3\\), … as the pairs \\((\\mathbf{x}_1,\\,y_1)\\), \\((\\mathbf{x}_2,\\,y_2)\\), \\((\\mathbf{x}_3,\\,y_3)\\), … arrive one-after-another in real-time.\n\nSo, imagine we have observed \\(n-1\\) pairs, and we have computed the estimate \\(\\hat{\\boldsymbol{\\beta}}_{n-1}=(\\mathbf{X}_{n-1}^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{X}_{n-1})^{-1}\\mathbf{X}_{n-1}^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{y}_{n-1}.\\) Then, a new observation \\((\\mathbf{x}_n,\\,y_n)\\) arrives. How can we efficiently update our estimate of \\(\\boldsymbol{\\beta}\\) to incorporate this new information? In other words, how can we quickly compute \\(\\hat{\\boldsymbol{\\beta}}_{n}\\) given only \\(\\hat{\\boldsymbol{\\beta}}_{n-1}\\) and the new \\((\\mathbf{x}_n,\\,y_n)\\)? Of course, we could always just add new rows to \\(\\mathbf{X}_{n-1}\\) and \\(\\mathbf{y}_{n-1}\\) and recompute the whole estimate from scratch, but that is super inefficient and we can do better. To that end, show that the new estimate is related to the old estimate and the new data by the following recursion: \\(\\hat{\\boldsymbol{\\beta}}_n=\\hat{\\boldsymbol{\\beta}}_{n-1}+\\mathbf{k}_n(y_n-\\mathbf{x}_n^{\\scriptscriptstyle\\mathsf{T}}\\hat{\\boldsymbol{\\beta}}_{n-1})\\). What is \\(\\mathbf{k}_n\\)?\nInterpret the recursion you derived in part a. Isn’t \\((y_n-\\mathbf{x}_n^{\\scriptscriptstyle\\mathsf{T}}\\hat{\\boldsymbol{\\beta}}_{n-1})\\) a residual? What is this \\(\\mathbf{k}_n\\) thing doing? What would it mean if we had \\(\\hat{\\boldsymbol{\\beta}}_n=\\hat{\\boldsymbol{\\beta}}_{n-1}\\)?\nWrite a for loop in R that uses your recursion to process a dataset one observation at a time, and verify that at the end of the loop, you get the same estimates that you would have gotten if you had just used lm. Do this for the mtcars dataset and the regression mpg ~ wt.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe Sherman–Morrison formula is a great tool! For invertible \\(\\mathbf{A}\\in\\mathbb{R}^{p\\times p}\\) and \\(\\mathbf{v}\\in\\mathbb{R}^p\\), we have\n\\[\n(\\mathbf{A}+\\mathbf{v}\\mathbf{v}^{\\scriptscriptstyle\\mathsf{T}})^{-1}\n=\n\\mathbf{A}^{-1}\n-\n\\frac{\n\\mathbf{A}^{-1}\n\\mathbf{v}\\mathbf{v}^{\\scriptscriptstyle\\mathsf{T}}\n\\mathbf{A}^{-1}\n}{\n1\n+\n\\mathbf{v}^{\\scriptscriptstyle\\mathsf{T}}\n\\mathbf{A}^{-1}\n\\mathbf{v}\n}\n.\n\\]",
    "crumbs": [
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "problems/pset-0.html#submission",
    "href": "problems/pset-0.html#submission",
    "title": "Problem Set 0",
    "section": "Submission",
    "text": "Submission\nYou are free to compose your solutions for this problem set however you wish (scan or photograph written work, handwriting capture on a tablet device, LaTeX, Quarto, whatever) as long as the final product is a single PDF file. You must upload this to Gradescope and mark the pages associated with each problem.\nDo not forget to include the following:\n\nFor each problem, please acknowledge your collaborators;\nIf a problem required you to code something, please include both the code and the output.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "problems/bank/review/mvnormal-2.html",
    "href": "problems/bank/review/mvnormal-2.html",
    "title": "STA 542 Fall 2025",
    "section": "",
    "text": "Recall that if \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are jointly distributed according to\n\\[\n\\begin{bmatrix}\n\\mathbf{x} \\\\\n\\mathbf{y}\n\\end{bmatrix}\n\\sim\n\\text{N}_{n+m}\n\\left(\n\\begin{bmatrix}\n\\boldsymbol{\\mu}_x \\\\\n\\boldsymbol{\\mu}_y\n\\end{bmatrix}\n,\\,\n\\begin{bmatrix}\n\\boldsymbol{\\Sigma}_x & \\boldsymbol{\\Sigma}_{xy}\\\\\n\\boldsymbol{\\Sigma}_{xy}^{\\scriptscriptstyle\\mathsf{T}} & \\boldsymbol{\\Sigma}_y\n\\end{bmatrix}\n\\right),\n\\]\nthen the conditional distribution is\n\\[\n\\mathbf{y} \\,|\\,\\mathbf{x}\n\\sim\n\\text{N}_m\n\\left(\n\\boldsymbol{\\mu}_y + \\boldsymbol{\\Sigma}_{xy}^{\\scriptscriptstyle\\mathsf{T}}\\boldsymbol{\\Sigma}_x^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_x)\n,\\,\n\\boldsymbol{\\Sigma}_y - \\boldsymbol{\\Sigma}_{xy}^{\\scriptscriptstyle\\mathsf{T}}\n\\boldsymbol{\\Sigma}_x^{-1}\n\\boldsymbol{\\Sigma}_{xy}\n\\right).\n\\]\nWe will use this fact often, but proving it is tedious. I’m not going to make you do it. This problem set is unreasonable enough already. Instead, I want you to read the proof very carefully. If you’re like me, there is some step in the derivation that will make you say “what just happened?” Drill down on that. Pick a section of the proof where you get confused, or where a technique is being used that you are unfamiliar with. Study that part carefully until you understand it, and then in your problem set write-up, explain it to me like I’m an idiot (shouldn’t be too difficult). Use pictures, numerical examples, specialize to the bivariate case, whatever you need to do.\n\n\n\n\n\n\nDon’t be a silly goose.\n\n\n\nWe are grading this part of the problem set for completion, but give it a good faith effort so that you actually, like, learn something."
  },
  {
    "objectID": "problems/bank/review/students-t.html",
    "href": "problems/bank/review/students-t.html",
    "title": "STA 542 Fall 2025",
    "section": "",
    "text": "If \\(X\\sim t_\\nu\\) has Student’s \\(t\\)-distribution, then its density is\n\\[\nf_X(x)=\\frac{\\Gamma \\left(\\frac{\\nu + 1}{2}\\right)}{\\sqrt{\\pi\\nu}\\, \\Gamma\\left(\\frac{\\nu}{2}\\right)} \\left(1 + \\frac{x^2}{\\nu}\\right)^{-\\frac{\\nu+1}{2}},\\quad x\\in\\mathbb{R}.\n\\]\nIf you define a location-scale transformation \\(Y=\\mu+\\tau X\\) for constants \\(\\mu\\in\\mathbb{R}\\) and \\(\\tau&gt;0\\), then the new random variable \\(Y\\sim t(\\nu,\\,\\mu,\\,\\tau^2)\\) has a non-standard Student’s \\(t\\)-distribution. If we did this to the Cauchy for example (\\(\\nu=1\\)), then we get:\n\n\n\n\n\n\n\n\nBe careful not to immediately interpret \\(\\mu\\) and \\(\\tau^2\\) as mean and variance. Only if \\(\\nu&gt;1\\) is \\(E(Y)=\\mu\\), and \\(\\tau^2\\) is only proportional to the variance if \\(\\nu&gt;2\\).\nAnyway, consider a bivariate distribution \\(p(y,\\,\\sigma^2)\\) written hierarchically:\n\\[\n\\begin{aligned}\n\\sigma^2\n&\n\\sim\n\\text{IG}(a,\\,b)\n\\\\\ny\n\\,|\\,\n\\sigma^2\n&\n\\sim\n\\text{N}\n(m\n,\\,\n\\sigma^2v^2)\n.\n\\end{aligned}\n\\]\nDerive the marginal density of \\(y\\) and show that it has a non-standard Student’s \\(t\\)-distribution. What are the parameters?"
  },
  {
    "objectID": "problems/bank/review/bayes.html",
    "href": "problems/bank/review/bayes.html",
    "title": "STA 542 Fall 2025",
    "section": "",
    "text": "Recall the parametric family from the previous part, and consider the Bayesian model\n\\[\n\\begin{aligned}\n\\theta &\\sim \\text{Gamma}(a_0,\\,b_0) \\\\\nx_i\\,|\\, \\theta &\\overset{\\text{iid}}{\\sim }F(x;\\,\\theta).\n\\end{aligned}\n\\]\n\nDerive the posterior distribution \\(p(\\theta\\,|\\,x_{1:n})\\);\nShow that the posterior mean \\(E(\\theta\\,|\\,x_{1:n})\\) can be written as a convex combination of the prior mean and the MLE;\nDerive the posterior predictive distribution \\(p(x_{n+1}\\,|\\,x_{1:n})\\)."
  },
  {
    "objectID": "problems/bank/autoregression/prior-predictive.html",
    "href": "problems/bank/autoregression/prior-predictive.html",
    "title": "STA 542 Fall 2025",
    "section": "",
    "text": "Models are wrong and parameters don’t exist. All that matters is observables.\n\nThat statement is the essence of the predictive view of inference. In its Bayesian incarnation, this view says that the only objects that truly matter are the predictive distributions:\n\\[\n\\begin{aligned}\np(\\mathbf{y}_{1:T})\n&=\n\\int\np(\\mathbf{y}_{1:T}\\,|\\,\\boldsymbol{\\theta})\np(\\boldsymbol{\\theta})\n\\,\\text{d}\\boldsymbol{\\theta}\n&&\n(\\text{prior predictive})\n\\\\\np(\\mathbf{y}_{T+1:T+H}\\,|\\,\\mathbf{y}_{1:T})\n&=\n\\int\np(\\mathbf{y}_{T+1:T+H}\\,|\\,\\mathbf{y}_{1:T},\\,\\boldsymbol{\\theta})\np(\\boldsymbol{\\theta}\\,|\\,\\mathbf{y}_{1:T})\n\\,\\text{d}\\boldsymbol{\\theta}.\n&&\n(\\text{posterior predictive})\n\\end{aligned}\n\\]\nModels, parameters, likelihoods, priors, even Bayes’ theorem – these are all just a means to the end of specifying and accessing predictive distributions for observable quantities. So, when you write down a model and a prior on the model’s parameters, you should tune the prior on parameters to capture your prior beliefs about how the data will look before you see them. In other words, the goal of prior elicitation should be accurately eliciting the prior predictive distribution, not the prior on parameters per se. Let’s take this seriously in a toy example.\nImagine you are a data scientist at Feta Platforms Inc, and one day your manager Zohn Jito knocks on your office door. Zohn Jito is an irritating fool who knows nothing about statistics, and he is always making ridiculous requests. On this day, he asks you to build a model for the log of daily feta sales, and he describes his beliefs about how the data should look:\n\nThe series should look smoother than white noise, but not as smooth as a random walk. Runs of consecutive positive values are expected, but they should not typically last more than 5 – 6 periods. The series should hover around zero, but occasionally wander away and then return.\n\nYou decide to build an AR(1) model with normal-inverse-gamma prior (maybe conjugate, maybe not?). Your tasks:\n\nchoose a prior and set the hyperparameters so that the prior predictive distribution is consistent with Zohn’s description;\nsimulate prior predictive sample paths of length 100 and visually check whether they look the way they should;\nif not, adjust your prior until they do."
  },
  {
    "objectID": "problems/bank/autoregression/covariance-kernel.html",
    "href": "problems/bank/autoregression/covariance-kernel.html",
    "title": "STA 542 Fall 2025",
    "section": "",
    "text": "Recall our good ol’ pal the Gaussian AR(1):\n\\[\n\\begin{aligned}\ny_t\n&=\n\\beta_0+\\beta_1y_{t-1}+\\varepsilon_t, && \\varepsilon_t\\overset{\\text{iid}}{\\sim}\\text{N}(0,\\,\\sigma^2)\n\\\\\ny_0\n&\\sim\n\\text{N}(\\mu_0,\\,v_0^2).\n\\end{aligned}\n\\]\nWithout assuming anything about \\(\\beta_1\\in\\mathbb{R}\\), derive the covariance kernel of the joint distribution:\n\\[\n\\gamma(s,\\, t) = \\text{cov}(y_s,\\,y_t),\\quad s,\\,t\\in\\mathbb{N}.\n\\]"
  },
  {
    "objectID": "problems/bank/autoregression/new-process.html",
    "href": "problems/bank/autoregression/new-process.html",
    "title": "STA 542 Fall 2025",
    "section": "",
    "text": "Consider this silly time series process:\n\\[\ny_t\n=\n\\alpha\n+\n\\varepsilon_t\n+\n\\theta_1\n\\varepsilon_{t-1}\n+\n\\theta_2\n\\varepsilon_{t-2}\n,\\quad \\varepsilon_t\\overset{\\text{iid}}{\\sim}\\text{N}(0,\\,\\sigma^2).\n\\]\nDerive the joint distribution of \\(\\mathbf{y}=\\begin{bmatrix}y_1&y_2&\\cdots&y_T\\end{bmatrix}^{\\scriptscriptstyle\\mathsf{T}}\\). In particular…\n\nWhat distribution family does the joint distribution belong to? Justify your answer; don’t just assert it.\nDerive the mean function:\n\n\\[\n\\mu(t)=E(y_t).\n\\]\n\nDerive the covariance kernel:\n\n\\[\n\\gamma(s,\\, t) = \\text{cov}(y_s,\\,y_t),\\quad s,\\,t\\in\\mathbb{N}.\n\\]\n\nIs \\(y_t\\) stationary? Why or why not?"
  },
  {
    "objectID": "problems/bank/autoregression/streaming-conjugate-bayes.html",
    "href": "problems/bank/autoregression/streaming-conjugate-bayes.html",
    "title": "STA 542 Fall 2025",
    "section": "",
    "text": "As we saw in lecture, (conditional) likelihood-based inference for the AR(1) proceeds identically to iid multiple regression. Whether it’s MLE or Bayes, nothing changes. In particular, imagine you place a conjugate normal-inverse-gamma prior on the model parameters:\n\\[\n\\begin{aligned}\n\\sigma^2\n&\\sim\n\\text{IG}(a_0,\\, b_0)\n\\\\\n\\boldsymbol{\\beta}\\,|\\, \\sigma^2\n&\\sim\n\\text{N}_2(\\mathbf{m}_0,\\,\\sigma^2\\mathbf{H}^{-1}_0)\n\\\\\ny_t\n\\,|\\,\ny_{t-1}\n,\\,\n\\boldsymbol{\\beta},\\,\\sigma^2\n&\\sim \\text{N}\n\\left(\n\\mathbf{x}_t^{\\scriptscriptstyle\\mathsf{T}}\\boldsymbol{\\beta},\\,\\sigma^2\n\\right), && \\mathbf{x}_t=\\begin{bmatrix}1 & y_{t-1}\\end{bmatrix}^{\\scriptscriptstyle\\mathsf{T}}.\n\\end{aligned}\n\\]\nThen you get a normal-inverse-gamma posterior:\n\\[\n\\begin{aligned}\n\\sigma^2\\,|\\, y_{0:T}\n&\\sim\n\\text{IG}(a_T,\\, b_T)\n\\\\\n\\boldsymbol{\\beta}\\,|\\, \\sigma^2,\\, y_{0:T}\n&\\sim\n\\text{N}_2(\\mathbf{m}_T,\\,\\sigma^2\\mathbf{H}^{-1}_T)\n\\\\\n\\\\\n\\mathbf{H}_T\n&=\n\\mathbf{X}_T^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{X}_T+\\mathbf{H}_0\n\\\\\n\\mathbf{m}_T\n&=\n\\mathbf{H}_T^{-1}(\\mathbf{X}_T^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{y}_T+\\mathbf{H}_0\\mathbf{m}_0)\n\\\\\na_T\n&=\na_0 + T/2\n\\\\\nb_T\n&=\nb_0\n+\n(\\mathbf{y}_T^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{y}_T+\\mathbf{m}_0^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{H}_0\\mathbf{m}_0-\\mathbf{m}_T^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{H}_T\\mathbf{m}_T)/2.\n\\end{aligned}\n\\]\nThose formulas for updating the hyperparameters are great for batch inference, but inefficient if the data are streaming. So let’s improve them. Imagine we have collected data \\(y_{0:t-1}\\) and characterized the posterior up to that point:\n\\[\n\\begin{aligned}\n\\sigma^2\\,|\\, y_{0:t-1}\n&\\sim\n\\text{IG}(a_{t-1},\\, b_{t-1})\n\\\\\n\\boldsymbol{\\beta}\\,|\\, \\sigma^2,\\, y_{0:t-1}\n&\\sim\n\\text{N}_2(\\mathbf{m}_{t-1},\\,\\sigma^2\\mathbf{H}^{-1}_{t-1}).\n\\end{aligned}\n\\]\nThen a single new observation \\(y_t\\) arrives and you want to characterize the new posterior:\n\\[\n\\begin{aligned}\n\\sigma^2\\,|\\, y_{0:t}\n&\\sim\n\\text{IG}(a_{t},\\, b_{t})\n\\\\\n\\boldsymbol{\\beta}\\,|\\, \\sigma^2,\\, y_{0:t}\n&\\sim\n\\text{N}_2(\\mathbf{m}_{t},\\,\\sigma^2\\mathbf{H}^{-1}_{t}).\n\\end{aligned}\n\\]\n\nDerive a recursion that takes the old hyperparameters and the new data and computes the new hyperparameters without inverting any matrices;\nc(sunspots) is a monthly time series of mean relative sunspot numbers from 1749 to 1983. Write a for loop in R that uses your recursion to fit an AR(1) to these data one observation at a time. Verify at the end of the loop that you get the same posterior hyperparameters that you would have gotten if you had applied the batch update to the entire data set all at once."
  },
  {
    "objectID": "problems/bank/autoregression/ar-p-bma.html",
    "href": "problems/bank/autoregression/ar-p-bma.html",
    "title": "STA 542 Fall 2025",
    "section": "",
    "text": "The arima.sim function in base R does exactly what it says it does. Feel free to read the documentation. Furthermore, below the fold are some helper functions that implement basic operations for Bayesian linear regression with conjugate priors:\n\n\n\n\n\n\nFree stuff!\n\n\n\n\n\n\nblr_nig_update &lt;- function(y_t, x_t, a, b, m, invH) {\n  # Given old prior (a, b, m, invH) and new data point (y, x), compute posterior\n  # x_t: column vector (p x 1), m: column vector (p x 1), invH: (p x p)\n  \n  e &lt;- as.numeric(y_t - t(x_t) %*% m)\n  r2 &lt;- as.numeric(1 + t(x_t) %*% invH %*% x_t)\n  k &lt;- invH %*% x_t / r2\n  \n  # update mean\n  m_new &lt;- m + k * e\n  \n  # update covariance\n  invH_new &lt;- invH - k %*% t(x_t) %*% invH\n  \n  # update shape and scale\n  a_new &lt;- a + 0.5\n  b_new &lt;- b + 0.5 * (e^2 / r2)\n  \n  return(list(a = a_new, b = b_new, m = m_new, invH = invH_new))\n}\n\nblr_1step_predictive &lt;- function(x, a, b, m, invH){\n  list(df = 2*a, \n       ybar = c(t(x) %*% m), \n       s2 = (b/a) * c(1 + t(x) %*% invH %*% x))\n}\n\nblr_marginal_likelihood &lt;- function(y, X, mu0, Lambda0, a0, b0, \n                                    mu_n, Lambda_n, a_n, b_n) {\n  n &lt;- length(y)\n  k &lt;- ncol(X)\n  \n  # determinants via Cholesky for stability\n  det_ratio &lt;- determinant(Lambda0, logarithm = TRUE)$modulus -\n    determinant(Lambda_n, logarithm = TRUE)$modulus\n  \n  log_ml &lt;- lgamma(a_n) - lgamma(a0) +\n    a0 * log(b0) - a_n * log(b_n) +\n    0.5 * det_ratio -\n    (n / 2) * log(2 * pi)\n  \n  ml &lt;- exp(log_ml)\n  return(list(log_marginal_lik = as.numeric(log_ml),\n              marginal_lik = as.numeric(ml)))\n}\n\n\n\n\nUsing these raw materials, write a simulation study that generates a synthetic time series from a pure MA(1) with \\(\\theta_1=0.99\\). Then, mistakenly fit Bayesian AR(p) to the simulated data using these initial priors:\n\\[\n\\begin{aligned}\np&\\sim\\text{Unif}(\\{1,\\,2,\\,3\\})\\\\\n\\sigma^2&\\sim\\text{IG}(1,\\,1)\\\\\n\\boldsymbol{\\beta}\\,|\\,\\sigma^2,\\,p&\\sim\\text{N}_{p+1}\\left(\\mathbf{0},\\,\\sigma^2\\mathbf{I}_{p+1}\\right).\n\\end{aligned}\n\\]\nStarting from these priors, write a for loop that processes the data one observation at a time for each model. Along the way, keep track of the one-step-ahead posterior predictive distributions (non-standard Student’s \\(t\\)), the posterior model probabilities, and the “best” model (the one with highest probability).\nAt the end of the simulation, construct the histogram of PITs for each of three forecasting distributions:\n\n\n\n\n\n\n\n“best” plug-in\n\\(\\text{N}(\\mathbf{x}_t^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{m}_{t-1}, b_{t-1} / a_{t-1})\\)\n\n\n“best” posterior predictive\n\\(p(y_t\\,|\\,y_{1:t-1},\\,\\hat{p})\\)\n\n\nmodel-averaged\n\\(\\sum_{p=1}^3\\text{Pr}(p\\,|\\,y_{1:t-1})\\times p(y_t\\,|\\,y_{1:t-1},\\,p)\\)\n\n\n\nComment on the calibration of each, and don’t forget that the model we are fitting is fundamentally misspecified."
  },
  {
    "objectID": "problems/bank/autoregression/match-autocorr.html",
    "href": "problems/bank/autoregression/match-autocorr.html",
    "title": "STA 542 Fall 2025",
    "section": "",
    "text": "The ARMAacf function in base R computes the autocorrelation function \\(\\rho(h)=\\gamma(h)/\\gamma(0)\\) for the ARMA(p, q) model. Here I’ve wrapped it in a helper that creates a plot:\n\n# plot the autocorrelation function of an ARMA\nplot_arma_acf &lt;- function(ar = numeric(), ma = numeric(), lag.max = 10){\n  y_vals &lt;- ARMAacf(ar = ar, ma = ma, lag.max = lag.max)\n  plot(0:lag.max, y_vals, pch = 19, ylim = c(-1, 1),\n       xlab = \"h\", ylab = expression(rho~\"(h)\"))\n  segments(0:lag.max, 0, 0:lag.max, y_vals)\n  abline(h = 0, col = \"lightgrey\")\n}\n\nSo there you go. John Zito did something nice for once. But unfortunately, his evil twin Zohn Jito is back making ridiculous demands. He visits your office in an unhinged frenzy. Much like Klytaemnestra in Strauss’s opera Elektra, his sleep has been troubled (“Ich habe keine guten Nächte”). He has been haunted by feverish dreams of autocorrelation functions, but he knows not from whence they came. Dripping with sweat, he pants:\n\n“The autocorrelations start high, but then shrink smoothly and monotonically toward zero, without changing sign. It feels like each step of the series only remembers the last one, nothing more complicated.”\n“The correlations seem to alternate signs: positive, then negative, then positive again. They get smaller each time, kind of like a ringing bell that fades away.”\n“The correlations are definitely there at lag 1, a little bit at lag 2, but after that it looks like nothing. Totally flat.”\n“At lag 1 the correlation is negative, but then it’s pretty much gone. Looks like it bounces once and stops.”\n“The autocorrelations stick around forever. They go down, but it’s painfully slow, like the series doesn’t want to forget the past. It looks borderline nonstationary.”\n“There’s a big spike at lag 1, smaller at lag 2, then it tails off smoothly. It’s not a clean cut-off, but it doesn’t oscillate either.”\n“Lag 1 correlation is very strong, lag 2 is still positive but smaller, and then they slowly decay. No sign changes. The series is persistent, but it looks like the short-term correlation is extra strong compared to the long-term tail.”\n“At lag 1 it’s negative, lag 2 bounces back positive, then it decays but stays positive after that.”\n“It’s not a smooth oscillation like a sine wave, but the correlations jump positive, negative, positive for a few lags before dying off.”\n“There’s one big spike at lag 1, maybe a tiny one at lag 2, and then nothing.”\n“First correlation positive, second negative, third positive but small, then basically gone.”\n“The decay is slower than I’d expect for an MA, but the lag-1 correlation isn’t as huge as in an AR(1). It’s like a compromise between short-term and long-term memory.”\n“The first few correlations are really high and fluctuate a little in sign, then they slowly die away like a bell fading in a canyon. It’s more complicated than the simple sine-wave oscillation.”\n“Correlations die very quickly after lag 3, but the first three lags don’t decrease monotonically—they bounce around a bit.”\n“The series looks nearly like white noise. Most of the correlations are tiny, but if you squint you can see a barely-there persistence beyond lag 5 or 6.”\n“Some of the lags alternate in sign, others don’t. It’s not a simple sine wave, more like a complicated dance that fades gradually.”\n“Lag 1 correlation is negative and quite strong, then lag 2 is barely positive, then the series remembers the past moderately for several lags.”\n\nHelp bring this poor man some relief! Play around with different values of \\(p\\), \\(q\\), and the model parameters until you get an autocorrelation function that matches each description.\n\n\n\n\n\n\nJust do ten\n\n\n\nRespond to ten of the statements for full credit, but play around with all of them to prepare for the midterm."
  },
  {
    "objectID": "lecture-notes/ar-1.html",
    "href": "lecture-notes/ar-1.html",
    "title": "Everything You Always Wanted to Know About the AR(1) But Were Afraid to Ask",
    "section": "",
    "text": "A model is a probability distribution over a sequence..\nIn the spirit of Larry Wasserman’s All of Statistics and All of Nonparametric Statistics, this note introduces “All of Time Series Analysis” for the simplest time series model: the autoregression of order 1, or AR(1) for short. Topics include:\nOnce you’re comfortable with all that, everything else in the course is in some sense just a theme and variations on these main ideas."
  },
  {
    "objectID": "lecture-notes/ar-1.html#what-is-a-time-series-model",
    "href": "lecture-notes/ar-1.html#what-is-a-time-series-model",
    "title": "Everything You Always Wanted to Know About the AR(1) But Were Afraid to Ask",
    "section": "What is a time series model?",
    "text": "What is a time series model?\nIn some sense, all we are doing is manipulating joint distributions: computing their marginals, conditionals, means, and covariances. If you can do that, you can “do” time series. Of course, TA has its own special features\nBut dont lose te forest for the trees\nIf you can manipulate joint distributions, then you can do time series analysis is a generic sense. Of course, TS will pose their own special problems that require new techniques, but don’t lost the forest for the trees.\n\n\n\n\n\n\nA quick word on notation\n\n\n\nTwo things you have to get used to:\n\nWe will not use uppercase \\(Y_t\\) versus lowercase \\(y_t\\) to distinguish random variables versus fixed realizations. Everything will just be \\(y_t\\), and context will make clear if something functions as a random variable or a constant;\nThe symbol \\(p\\) will be aggressively abused and overloaded to represent any probability distribution, sometimes several in the same sentence. So \\(p(x)\\) is the density of the random variable \\(x\\), and \\(p(y,\\,z)\\) is the joint density of the random pair \\((y,\\,z)\\), and \\(p(\\boldsymbol{\\beta}\\,|\\,\\sigma^2)\\) is the conditional density of…you get the idea. CHANGE THIS TO REFER TO THE EQUATION\nAlso the \\(y_{i:j}\\) notation"
  },
  {
    "objectID": "lecture-notes/ar-1.html#the-simplest-time-series-model",
    "href": "lecture-notes/ar-1.html#the-simplest-time-series-model",
    "title": "Everything You Always Wanted to Know About the AR(1) But Were Afraid to Ask",
    "section": "The simplest time series model",
    "text": "The simplest time series model\nIn truth, the “simplest” time series model is the one where there is no dependence at all: \\(y_t\\overset{\\text{iid}}{\\sim}F\\). But in that case, why bother with STA 542? Things get interesting when there is dependence, and in particular the kind of dependence where you think the past can help to predict the future. The simplest model that captures this basic idea is the autoregression of order 1, or AR(1) for short:\n\\[\n\\begin{aligned}\ny_t&=\\beta_0+\\beta_1y_{t-1}+\\varepsilon_t,\\quad \\varepsilon_t\\overset{\\text{iid}}{\\sim}\\text{N}(0,\\,\\sigma^2)\\\\\ny_0&\\sim\\text{N}(\\mu_0,\\,\\gamma_0).\n\\end{aligned}\n\\]\nThis takes the form of a simple linear regression where \\(y_t\\) is the response and its first lagged value \\(y_{t-1}\\) is the predictor, hence the name\n\\(y_0\\) independent of the errors.\nI don’t like the notation for the initial condition variance."
  },
  {
    "objectID": "lecture-notes/ar-1.html#what-is-the-joint-distribution",
    "href": "lecture-notes/ar-1.html#what-is-the-joint-distribution",
    "title": "Everything You Always Wanted to Know About the AR(1) But Were Afraid to Ask",
    "section": "What is the joint distribution?",
    "text": "What is the joint distribution?\n\\(y_t\\,|\\,y_{t-1}\\sim\\text{N}(\\beta_0+\\beta_1y_{t-1},\\,\\sigma^2)\\)\n\\[\np(y_{0:T}) = p(y_0)\\prod_{t=1}^Tp(y_t\\,|\\,y_{t-1})\n\\]\nto understand the joint distribution\n\\[\n\\begin{aligned}\ny_0\n&=\ny_0\n\\\\\ny_1\n&=\n\\beta_0+\\beta_1y_0+\\varepsilon_1\n\\\\\ny_2\n&=\n\\beta_0+\\beta_1\n\\\\\ny_3\n&=\n\\\\\n&\\vdots\n\\\\\ny_t\n&=\n\\end{aligned}\n\\]\nsummarized\n\\[\n\\begin{bmatrix}\ny_0\\\\\ny_1\\\\\ny_2\\\\\ny_3\\\\\n\\vdots \\\\\ny_T\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nbloop\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n1 & 0 & 0 & 0 & \\cdots & 0\n\\end{bmatrix}\n\\begin{bmatrix}\ny_0\\\\\n\\varepsilon_1\\\\\n\\varepsilon_2\\\\\n\\varepsilon_3\\\\\n\\vdots \\\\\n\\varepsilon_T\n\\end{bmatrix}\n\\]\nSo \\(\\sim\\text{N}_{T+1}()\\) and \\(\\mathbf{y}\\) is a linear transformation of BLAH, so it is also normal, with mean and covariance. What are these?"
  },
  {
    "objectID": "lecture-notes/ar-1.html#stationarity",
    "href": "lecture-notes/ar-1.html#stationarity",
    "title": "Everything You Always Wanted to Know About the AR(1) But Were Afraid to Ask",
    "section": "Stationarity",
    "text": "Stationarity\nMost interesting time series are not stationary. This comes down to us as an historical artifact because certain kinds of (classical) statistical theory require it as an assumption in order to prove theorems, but it’s never true in practice.\nweird concept. dependent but not too dependent. necessary for stable prediction and inference, but do we really care? Probably false in practice, but so what. I just want a model that generates good and useful predictions.\nImportant historically and theoretically, and it simplifies the model tremendously."
  },
  {
    "objectID": "lecture-notes/ar-1.html#classical-inference",
    "href": "lecture-notes/ar-1.html#classical-inference",
    "title": "Everything You Always Wanted to Know About the AR(1) But Were Afraid to Ask",
    "section": "Classical inference",
    "text": "Classical inference\n\nMethod of moments\n\n\nMaximum likelihood estimaton"
  },
  {
    "objectID": "lecture-notes/ar-1.html#bayesian-inference",
    "href": "lecture-notes/ar-1.html#bayesian-inference",
    "title": "Everything You Always Wanted to Know About the AR(1) But Were Afraid to Ask",
    "section": "Bayesian inference",
    "text": "Bayesian inference\n\\[\n\\begin{aligned}\n\\sigma^2\n&\\sim\n\\text{IG}(a_0,\\,b_0)\n\\\\\n\\boldsymbol{\\beta}\n\\,|\\,\n\\sigma^2\n&\\sim\n\\text{N}_2(\\bar{\\boldsymbol{\\beta}}_0,\\,)\n\\\\\ny_t\\,|\\,y_{t-1},\\,\\boldsymbol{\\beta},\\,\\sigma^2\n&\\sim\n\\end{aligned}\n\\]\nSo we want this:\n\\[\np(\\boldsymbol{\\beta},\\,\\sigma^2\\,|\\,y_{0:T})\n=\n\\frac\n{p(y_{1:T}\\,|\\,\\boldsymbol{\\beta},\\,\\sigma^2,\\,y_0)p(\\boldsymbol{\\beta},\\,\\sigma^2)}\n{p(y_{1:T}\\,|\\,y_0)}\n\\]\nFor simplicity, we’re just going to condition on \\(y_0\\).\nAlso, we do not have to assume stationarity to do this.\nOur prior is conjugate, so we can compute the exact posterior.\n\n\n\n\n\n\nTip\n\n\n\nconjugate updates\n\n\nOther priors: normal - IG that isn’t conjugate, whatever else is in West’s book, the prior that enforces stationarity, either with accept-reject or the Heaps stuff, maybe that West Huerta thing idk."
  },
  {
    "objectID": "lecture-notes/ar-1.html#forecasting",
    "href": "lecture-notes/ar-1.html#forecasting",
    "title": "Everything You Always Wanted to Know About the AR(1) But Were Afraid to Ask",
    "section": "Forecasting",
    "text": "Forecasting\n\nClassical\ntypically ignores estimation uncertainty from coefficients.\nshow a picture comparing\n\n\n\n\n\n\nTip\n\n\n\nsieve bootstrap Buhlmann bernoulli\n\n\nthese draws are a discrete approximation.\npoint forecast: sample mean or median interval forecast: quantiles, HPD, whatever\n\n\nBayes\nProbabilistic prediction is automatic, and it’s easier (not easy, but easier) to augment your inferences with more sources of uncertainty: missing data, model uncertainty, etc."
  },
  {
    "objectID": "lecture-notes/ar-1.html#forecast-evaluation",
    "href": "lecture-notes/ar-1.html#forecast-evaluation",
    "title": "Everything You Always Wanted to Know About the AR(1) But Were Afraid to Ask",
    "section": "Forecast evaluation",
    "text": "Forecast evaluation\nmarginal distributions\njoint distributions\nconditional (forecast) distributions\nstationarity\ndependence structure\nmethod of moments (Yule-Walker)\nmaximum likelihood (unconditional and conditional)\nemphasize sequential recursions\nbayesian inference\nprobabilistic prediction\nparametric bootstrap\npoint prediction\ninterval prediction\ndensity prediction"
  },
  {
    "objectID": "problems/bank/autoregression/dependent-lln.html",
    "href": "problems/bank/autoregression/dependent-lln.html",
    "title": "STA 542 Fall 2025",
    "section": "",
    "text": "Recall the Yule-Walker estimators for the parameters of a stationary AR(1). They are based on these estimating equations\n\\[\n\\begin{aligned}\n\\gamma(1)&=\\beta_1\\gamma(0) &&\\implies\\beta_1=\\frac{\\gamma(1)}{\\gamma(0)}\\\\\n\\mu&=\\frac{\\beta_0}{1-\\beta_1}&&\\implies\\beta_0=(1-\\beta_1)\\mu\\\\\n\\gamma(0)&=\\frac{\\sigma^2}{1-\\beta_1^2}&&\\implies \\sigma^2=(1-\\beta_1^2)\\gamma(0),\n\\end{aligned}\n\\]\nwhere \\(\\mu=E(y_t)\\) is the time-invariant expected value, \\(\\gamma(0)=\\text{var}(y_t)\\) is the time-invariant variance, and so on. The Yule-Walker approach is an example of method of moments, where we replace all of the “population” expected values with sample averages. In order for this method to work, the sample averages must actually be good estimates of their corresponding expected values. If the data are iid, then we have classical laws of large numbers that guarantee this. But in our case the data are not iid. So how do we know this is going to work?\n\n\n\n\n\n\nTheorem (law of large numbers for dependent data)\n\n\n\nLet \\(y_t\\) follow a process with time-invariant mean and shift-invariant covariance:\n\\[\n\\begin{aligned}\n\\mu\n&=\nE(y_t)\n&&\n\\forall\\,t\n\\\\\n\\gamma(h)\n&=\n\\text{cov}\n(y_{t+h},\\,y_t)\n&&\\forall\\,t.\n\\end{aligned}\n\\]\nFurthermore, assume that\n\\[\n\\sum\\limits_{h=0}^\\infty |\\gamma(h)|&lt;\\infty.\n\\]\nThen\n\\[\n\\bar{y}_T=\\frac{1}{T}\\sum\\limits_{t=1}^Ty_t\\overset{\\text{prob}}{\\to}\\mu.\n\\]\n\n\n\nProve the theorem;\nShow that the AR(1) with \\(|\\beta_1|&lt;1\\) satisfies the conditions."
  },
  {
    "objectID": "problems/bank/autoregression/forecasting-fred.html",
    "href": "problems/bank/autoregression/forecasting-fred.html",
    "title": "STA 542 Fall 2025",
    "section": "",
    "text": "FRED is a fabulous source of economic time series data, and this problem encourages you to play around with it. Please do the following:\n\nSurf around FRED for a while until you find a time series that interests you, then use the fredr package to pull the data into R;\nUsing your online recursion from the previous problem, write a loop that sequentially fits a Bayesian AR(1) with conjugate prior to the time series you chose;\nIn each step of the loop, compute the PIT value comparing the one-step posterior predictive distribution \\(p(y_t\\,|\\,y_{0:t-1})\\) to the actual, realized value \\(y_t\\);\nAt the end of the loop, plot a histogram of the PITs. Are the forecasts well-calibrated? If not, what diagnostic information does the histogram convey?\n\n\n\n\n\n\n\nHint\n\n\n\nThe extraDistr package provides tools for the non-standard Student’s \\(t\\) distribution."
  },
  {
    "objectID": "problems/bank/autoregression/ar-2-yule-walker.html",
    "href": "problems/bank/autoregression/ar-2-yule-walker.html",
    "title": "STA 542 Fall 2025",
    "section": "",
    "text": "Consider again the mean-zero AR(2):\n\\[\ny_t=\\beta_1y_{t-1}+\\beta_2y_{t-2}+\\varepsilon_t,\\quad\\varepsilon_t\\overset{\\text{iid}}{\\sim}\\text{N}(0,\\,\\sigma^2).\n\\]\nAssuming the process is stationary, derive concrete formulas for the Yule-Walker estimators of \\(\\beta_1\\), \\(\\beta_2\\), and \\(\\sigma^2\\). To keep the formulas clean, you may find it helpful to introduce notation for the autocorrelation function of a stationary process:\n\\[\n\\rho(h)=\\frac{\\gamma(h)}{\\gamma(0)}=\\frac{\\text{cov}(y_{t+h},\\,y_t)}{\\text{var}(y_t)}=\\text{cor}(y_{t+h},\\,y_t).\n\\]"
  },
  {
    "objectID": "problems/bank/autoregression/interpret-kalman-gain.html",
    "href": "problems/bank/autoregression/interpret-kalman-gain.html",
    "title": "STA 542 Fall 2025",
    "section": "",
    "text": "Last time, you derived a recursive updating rule for conjugate Bayesian linear regression. The recursion for the posterior mean of the regression coefficients is\n\\[\n\\newcommand{\\tr}{{\\scriptscriptstyle\\mathsf{T}}}\n\\begin{aligned}\n\\mathbf{k}_t&=\\frac{\\mathbf{H}_{t-1}^{-1}\\mathbf{x}_t}{1+\\mathbf{x}_t^\\tr\\mathbf{H}_{t-1}^{-1}\\mathbf{x}_t}\n\\\\\n\\mathbf{m}_t\n&=\n\\mathbf{m}_{t-1}\n+\n\\mathbf{k}_t(y_t-\\mathbf{x}_t^\\tr\\mathbf{m}_{t-1})\n.\n\\end{aligned}\n\\]\nThat vector \\(\\mathbf{k}_t\\) is sometimes called the Kalman gain. You probably derived this using purely linear algebraic ideas like the Sherman-Morrison formula, but in fact the form of this recursion has important probabilistic meaning as well.\nImagine you are taking a conjugate Bayesian approach to fitting a Gaussian AR(p). After observing the first \\(t-1\\) observations, here is where things stand:\n\\[\n\\begin{aligned}\n\\sigma^2\\,|\\, y_{1:t-1}\n&\\sim\n\\text{IG}(a_{t-1},\\, b_{t-1})\n\\\\\n\\boldsymbol{\\beta}\\,|\\, \\sigma^2,\\, y_{1:t-1}\n&\\sim\n\\text{N}_{p+1}(\\mathbf{m}_{t-1},\\,\\sigma^2\\mathbf{H}^{-1}_{t-1})\\\\\ny_t\\,|\\,\\boldsymbol{\\beta},\\,\\sigma^2,\\, y_{1:t-1}\n&\\sim\\text{N}(\\mathbf{x}_t^{\\scriptscriptstyle\\mathsf{T}}\\boldsymbol{\\beta},\\,\\sigma^2) &&\n\\mathbf{x}_t=\\begin{bmatrix}1&y_{t-1}&\\cdots &y_{t-p}\\end{bmatrix}^{\\scriptscriptstyle\\mathsf{T}}.\n\\end{aligned}\n\\]\n\nUsing results from Problem Set 0, show that the joint distribution of \\(\\boldsymbol{\\beta}\\) and \\(y_t\\) is multivariate normal, and state the moments:\n\n\\[\n\\begin{bmatrix}\n\\boldsymbol{\\beta}\\\\\ny_{t}\n\\end{bmatrix}\n\\,|\\,\\sigma^2,\\,y_{1:t-1}\n\\sim\n\\text{N}_{p+2}\n\\left(?,\\,?\\right)\n\\]\n\nUsing part a and standard results for the conditional distributions of multivariate normals, what is the conditional posterior \\(p(\\boldsymbol{\\beta}\\,|\\,\\sigma^2,\\,y_{1:t})\\)?\nUsing part b, what is the probabilistic interpretation of the Kalman gain \\(\\mathbf{k}_t\\)?"
  },
  {
    "objectID": "problems/bank/autoregression/ar-2-stationary-region.html",
    "href": "problems/bank/autoregression/ar-2-stationary-region.html",
    "title": "STA 542 Fall 2025",
    "section": "",
    "text": "Consider the mean-zero AR(2):\n\\[\ny_t=\\beta_1y_{t-1}+\\beta_2y_{t-2}+\\varepsilon_t,\\quad\\varepsilon_t\\overset{\\text{iid}}{\\sim}\\text{N}(0,\\,\\sigma^2).\n\\]\nWe can rewrite this in state-space form:\n\\[\n\\begin{aligned}\ny_t&=\\begin{bmatrix}1&0\\end{bmatrix}\\begin{bmatrix}y_t\\\\y_{t-1}\\end{bmatrix}\n\\\\\n\\begin{bmatrix}y_t\\\\y_{t-1}\\end{bmatrix}\n&=\n\\begin{bmatrix}\\beta_1&\\beta_2\\\\1&0\\end{bmatrix}\n\\begin{bmatrix}y_{t-1}\\\\y_{t-2}\\end{bmatrix}\n+\n\\begin{bmatrix}\\varepsilon_t\\\\0\\end{bmatrix}\n.\n\\end{aligned}\n\\]\nIn order for the process to be stationary, the eigenvalues of the companion matrix must all lie inside the unit circle. This happens if and only if the pair \\((\\beta_1,\\,\\beta_2)\\) lies in this region:\n\n\n\n\n\n\n\n\nProve it."
  },
  {
    "objectID": "problems/bank/autoregression/ma-as-state-space.html",
    "href": "problems/bank/autoregression/ma-as-state-space.html",
    "title": "STA 542 Fall 2025",
    "section": "",
    "text": "Recall the mean-zero MA(q):\n\\[\ny_t\n=\n\\sum_{i=1}^q\\theta_i\\varepsilon_{t-i}\n+\n\\varepsilon_t,\n\\quad\n\\varepsilon_t\n\\overset{\\text{iid}}{\\sim}\n\\text{N}(0,\\, \\sigma^2)\n.\n\\]\nIt is useful to write this in the form of a linear state-space system:\n\\[\n\\begin{aligned}\ny_t\n&=\n\\mathbf{F}\\mathbf{s}_t\n+\n\\mathbf{Q}\\boldsymbol{\\varepsilon}_t\n\\\\\n\\mathbf{s}_t\n&=\\mathbf{G}\n\\mathbf{s}_{t-1}\n+\n\\mathbf{R}\\boldsymbol{\\eta}_t\n\\end{aligned}\n\\]\nFor better or worse, there are many equivalent ways to do this depending on what you think of as the “state,” and they all have their uses:\n\n\\(\\mathbf{s}_t=\\begin{bmatrix}\\varepsilon_t & \\varepsilon_{t-1}&\\cdots & \\varepsilon_{t-q}\\end{bmatrix}^{\\scriptscriptstyle\\mathsf{T}}\\in\\mathbb{R}^{q+1}\\);\n\\(\\mathbf{s}_t=\\begin{bmatrix}\\sum\\limits_{i=0}^q\\theta_i\\varepsilon_{t-i} & \\sum\\limits_{i=2}^q\\theta_i\\varepsilon_{t-i+2}&\\cdots&\\theta_q\\varepsilon_t\\end{bmatrix}^{\\scriptscriptstyle\\mathsf{T}}\\in\\mathbb{R}^{q+1}\\);\n\\(\\mathbf{s}_t=\\begin{bmatrix}y_t-\\varepsilon_t & y_{t-1}-\\varepsilon_{t-1}&\\cdots & y_{t-q+1}-\\varepsilon_{t-q+1}\\end{bmatrix}^{\\scriptscriptstyle\\mathsf{T}}\\in\\mathbb{R}^q\\);\n\nFor each version of the state vector, figure out what the system components need to be in order for the state-space system to be equivalent to the MA(q). Some of the objects in the system might be unnecessary and you can set them to zero."
  },
  {
    "objectID": "problems/bank/review/mle.html",
    "href": "problems/bank/review/mle.html",
    "title": "STA 542 Fall 2025",
    "section": "",
    "text": "Here is a cdf indexed by a parameter \\(\\theta&gt;0\\):\n\\[\nF(x;\\,\\theta)\n=\n1 - \\frac{1}{(1+x)^\\theta},\\quad x\\geq 0.\n\\]\n\nDescribe an algorithm that will simulate (pseudo)random numbers from this distribution;\nLet \\(x_1,\\,x_2,\\,...,\\,x_n\\overset{\\text{iid}}{\\sim}F(x;\\,\\theta)\\) and derive the maximum likelihood estimator \\(\\hat{\\theta}_n^{(\\text{MLE})}\\) for the parameter \\(\\theta\\);\nDerive the exact sampling distribution of \\(\\hat{\\theta}_n^{(\\text{MLE})}\\);\nImplement the bootstrap to simulate the sampling distribution of \\(\\hat{\\theta}_n^{(\\text{MLE})}\\), and compare the bootstrap distribution to the exact one you derived in part c. In order to do this, you will probably need your algorithm from part a.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nHere is a schematic of how the bootstrap works. We start with some ground truth parameter value \\(\\theta_0&gt;0\\), and then implement this:\n\\[\n\\begin{matrix}\n\\text{0. True distribution} &&& F_{\\theta_0} && \\\\\n&&& \\downarrow && \\\\\n\\text{1. Original data} &&& x_{1:n} && \\\\\n&&& \\downarrow && \\\\\n\\text{2. Empirical distribution} &&& \\hat{F}_{n} && \\\\\n&\\swarrow &\\swarrow& \\cdots &\\searrow&\\searrow \\\\\n\\text{3. Synthetic data}&\\tilde{x}_{1:n}^{(1)} &\\tilde{x}_{1:n}^{(2)}& \\cdots &\\tilde{x}_{1:n}^{(k-1)}&\\tilde{x}_{1:n}^{(k)} \\\\\n&\\downarrow &\\downarrow& \\cdots &\\downarrow&\\downarrow \\\\\n\\text{4. Bootstrap estimates}&\\hat{\\theta}_n^{(1)} &\\hat{\\theta}_n^{(2)}& \\cdots &\\hat{\\theta}_n^{(k-1)}&\\hat{\\theta}_n^{(k)} \\\\\n\\end{matrix}\n\\]\nIf you implemented this correctly, and if the original sample size \\(n\\) is “large enough,” then a histogram of the \\(k\\) estimates \\(\\hat{\\theta}_n^{(1)}\\), \\(\\hat{\\theta}_n^{(2)}\\), …, \\(\\hat{\\theta}_n^{(k)}\\) ought to resemble the exact density you derived in part c."
  },
  {
    "objectID": "problems/bank/review/prediction-interval.html",
    "href": "problems/bank/review/prediction-interval.html",
    "title": "STA 542 Fall 2025",
    "section": "",
    "text": "Consider these data\n\\[\nX_1,\\,X_2,\\,...,\\,X_n\\overset{\\text{iid}}{\\sim}\\text{N}(\\theta,\\,1)\n\\]\nand the usual estimator \\(\\hat{\\theta}_n=\\sum\\limits_{i=1}^nX_i/n\\).\n\nDerive an exact \\(100\\times (1-\\alpha)\\%\\) classical confidence interval for the unknown mean \\(\\theta\\). That is, find a random interval \\((L_n,\\,U_n)\\) satisfying:\n\n\\[\nP\\left(L_n&lt;\\theta&lt;U_n\\right)=1-\\alpha.\n\\]\n\nDerive an exact \\(100\\times (1-\\alpha)\\%\\) classical prediction interval for a new observation \\(X_{n+1}\\). That is, find a random interval \\((L_n',\\,U_n')\\) satisfying\n\n\\[\nP\\left(L_n'&lt;X_{n+1}&lt;U_n'\\right)=1-\\alpha.\n\\]\n\nComment on the difference between the two intervals."
  },
  {
    "objectID": "problems/bank/review/online-least-squares.html",
    "href": "problems/bank/review/online-least-squares.html",
    "title": "STA 542 Fall 2025",
    "section": "",
    "text": "An important consideration in time series analysis is sequential or online inference. In this environment, our data are not available to us all at once in a batch. Instead, the observations are streaming; they are arriving one-after-another in real-time, and we want to come up with a scheme that allows us to recursively update our inferences as new information arrives. Let’s explore this in the context of least squares regression.\n\n\n\n\n\n\nOLS review\n\n\n\nConsider the usual setting where we observe iid pairs \\(y_i\\in\\mathbb{R}\\) and \\(\\mathbf{x}_i\\in\\mathbb{R}^p\\) from the linear model:\n\\[\ny_i= \\mathbf{x}_i^{\\scriptscriptstyle\\mathsf{T}}\\boldsymbol{\\beta}+\\varepsilon_i,\\quad \\varepsilon_i\\overset{\\text{iid}}{\\sim}\\text{N}(0,\\,\\sigma^2).\n\\] Given \\(n\\) observations, we can form the matrices\n\\[\n\\mathbf{y}_n\n=\n\\underbrace{\n\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix}\n}\n_{n\\times 1}\n\\quad\n\\mathbf{X}_n\n=\n\\underbrace{\n\\begin{bmatrix}\n\\mathbf{x}_1^{\\scriptscriptstyle\\mathsf{T}} \\\\\n\\mathbf{x}_2^{\\scriptscriptstyle\\mathsf{T}} \\\\\n\\vdots \\\\\n\\mathbf{x}_n^{\\scriptscriptstyle\\mathsf{T}}\n\\end{bmatrix}\n}_{n\\times p}\n\\quad\n\\boldsymbol{\\varepsilon}_n\n=\n\\underbrace{\n\\begin{bmatrix}\n\\varepsilon_1 \\\\\n\\varepsilon_2 \\\\\n\\vdots \\\\\n\\varepsilon_n\n\\end{bmatrix}\n}\n_{n\\times 1}\n,\n\\] and we can rewrite the model in matrix form:\n\\[\n\\mathbf{y}_n=\\mathbf{X}_n\\boldsymbol{\\beta}+\\boldsymbol{\\varepsilon}_n,\\quad \\boldsymbol{\\varepsilon}_n\\sim\\text{N}_n(\\mathbf{0},\\,\\sigma^2\\mathbf{I}_n).\n\\] The ordinary least squares (OLS) estimator of the regression coefficients is \\(\\hat{\\boldsymbol{\\beta}}_n=(\\mathbf{X}_n^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{X}_n)^{-1}\\mathbf{X}_n^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{y}_n.\\)\n\n\nHopefully that was all review, and it describes how to do batch or offline inference for the linear model. We are assuming that all of our data are available in a single batch, and we only need to compute the estimate once. But in our class, we will often care about streaming or online inference: efficiently computing the sequence of estimates \\(\\hat{\\boldsymbol{\\beta}}_1\\), \\(\\hat{\\boldsymbol{\\beta}}_2\\), \\(\\hat{\\boldsymbol{\\beta}}_3\\), … as the pairs \\((\\mathbf{x}_1,\\,y_1)\\), \\((\\mathbf{x}_2,\\,y_2)\\), \\((\\mathbf{x}_3,\\,y_3)\\), … arrive one-after-another in real-time.\n\nSo, imagine we have observed \\(n-1\\) pairs, and we have computed the estimate \\(\\hat{\\boldsymbol{\\beta}}_{n-1}=(\\mathbf{X}_{n-1}^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{X}_{n-1})^{-1}\\mathbf{X}_{n-1}^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{y}_{n-1}.\\) Then, a new observation \\((\\mathbf{x}_n,\\,y_n)\\) arrives. How can we efficiently update our estimate of \\(\\boldsymbol{\\beta}\\) to incorporate this new information? In other words, how can we quickly compute \\(\\hat{\\boldsymbol{\\beta}}_{n}\\) given only \\(\\hat{\\boldsymbol{\\beta}}_{n-1}\\) and the new \\((\\mathbf{x}_n,\\,y_n)\\)? Of course, we could always just add new rows to \\(\\mathbf{X}_{n-1}\\) and \\(\\mathbf{y}_{n-1}\\) and recompute the whole estimate from scratch, but that is super inefficient and we can do better. To that end, show that the new estimate is related to the old estimate and the new data by the following recursion: \\(\\hat{\\boldsymbol{\\beta}}_n=\\hat{\\boldsymbol{\\beta}}_{n-1}+\\mathbf{k}_n(y_n-\\mathbf{x}_n^{\\scriptscriptstyle\\mathsf{T}}\\hat{\\boldsymbol{\\beta}}_{n-1})\\). What is \\(\\mathbf{k}_n\\)?\nInterpret the recursion you derived in part a. Isn’t \\((y_n-\\mathbf{x}_n^{\\scriptscriptstyle\\mathsf{T}}\\hat{\\boldsymbol{\\beta}}_{n-1})\\) a residual? What is this \\(\\mathbf{k}_n\\) thing doing? What would it mean if we had \\(\\hat{\\boldsymbol{\\beta}}_n=\\hat{\\boldsymbol{\\beta}}_{n-1}\\)?\nWrite a for loop in R that uses your recursion to process a dataset one observation at a time, and verify that at the end of the loop, you get the same estimates that you would have gotten if you had just used lm. Do this for the mtcars dataset and the regression mpg ~ wt.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe Sherman–Morrison formula is a great tool! For invertible \\(\\mathbf{A}\\in\\mathbb{R}^{p\\times p}\\) and \\(\\mathbf{v}\\in\\mathbb{R}^p\\), we have\n\\[\n(\\mathbf{A}+\\mathbf{v}\\mathbf{v}^{\\scriptscriptstyle\\mathsf{T}})^{-1}\n=\n\\mathbf{A}^{-1}\n-\n\\frac{\n\\mathbf{A}^{-1}\n\\mathbf{v}\\mathbf{v}^{\\scriptscriptstyle\\mathsf{T}}\n\\mathbf{A}^{-1}\n}{\n1\n+\n\\mathbf{v}^{\\scriptscriptstyle\\mathsf{T}}\n\\mathbf{A}^{-1}\n\\mathbf{v}\n}\n.\n\\]"
  },
  {
    "objectID": "problems/bank/review/mvnormal-1.html",
    "href": "problems/bank/review/mvnormal-1.html",
    "title": "STA 542 Fall 2025",
    "section": "",
    "text": "Slicing-and-dicing the multivariate normal distribution is a very important skill in time series analysis, so let’s make sure we can do that.\n\nLet \\(\\mathbf{x}\\sim\\text{N}_n(\\boldsymbol{\\mu},\\,\\boldsymbol{\\Sigma})\\), and fix constants \\(\\mathbf{A}\\in\\mathbb{R}^{m\\times n}\\) and \\(\\mathbf{c}\\in\\mathbb{R}^m\\). Show that\n\n\\[\n\\mathbf{y}\n=\n\\mathbf{A}\n\\mathbf{x}\n+\n\\mathbf{c}\n\\sim\n\\text{N}_m\n\\left(\n\\mathbf{A}\n\\boldsymbol{\\mu}\n+\n\\mathbf{c}\n,\\,\n\\mathbf{A}\n\\boldsymbol{\\Sigma}\n\\mathbf{A}^{\\scriptscriptstyle\\mathsf{T}}\n\\right)\n.\n\\]\n\nConsider the joint distribution \\(p(\\mathbf{x},\\,\\mathbf{y})=p(\\mathbf{y}\\,|\\,\\mathbf{x})p(\\mathbf{x})\\) written hierarchically as \\(\\mathbf{x}\\sim\\text{N}_n(\\boldsymbol{\\mu},\\,\\boldsymbol{\\Sigma})\\) and \\(\\mathbf{y}\\,|\\,\\mathbf{x}\\sim\\text{N}_m(\\mathbf{A}\\mathbf{x}+\\mathbf{c},\\,\\mathbf{R})\\). Use the result in part a to show that the joint distribution implied by this hierarchy is\n\n\\[\n\\begin{bmatrix}\n\\mathbf{x}\n\\\\\n\\mathbf{y}\n\\end{bmatrix}\n\\sim\n\\text{N}_{n+m}\n\\left(\n\\begin{bmatrix}\n\\boldsymbol{\\mu}\n\\\\\n\\mathbf{A}\\boldsymbol{\\mu}+\\mathbf{c}\n\\end{bmatrix}\n,\\,\n\\begin{bmatrix}\n\\boldsymbol{\\Sigma}\n&\n\\boldsymbol{\\Sigma}\\mathbf{A}^{\\scriptscriptstyle\\mathsf{T}}\n\\\\\n\\mathbf{A}\n\\boldsymbol{\\Sigma}\n&\n\\mathbf{A}\n\\boldsymbol{\\Sigma}\n\\mathbf{A}^{\\scriptscriptstyle\\mathsf{T}} + \\mathbf{R}\n\\end{bmatrix}\n\\right)\n.\n\\]\n\nUse the result in part a to show that the linear combination of independent normals is normal. That is, if \\(x_i\\sim\\text{N}(\\mu_i,\\,\\sigma^2_i)\\) are independent and \\(a_i\\in\\mathbb{R}\\) are constant, then prove that\n\n\\[\n\\sum\\limits_{i=1}^na_ix_i\\sim\\text{N}\\left(\\sum\\limits_{i=1}^na_i\\mu_i,\\, \\sum\\limits_{i=1}^na_i^2\\sigma_i^2\\right).\n\\]\n\n\n\n\n\n\nHint\n\n\n\n\n\nJZ changed this hint on 8/30/2025. The original hint mentioned using the change-of-variables formula. That’s a fine solution if \\(\\mathbf{A}\\) is invertible, but the result holds even if \\(\\mathbf{A}\\) is not invertible, and we will often use that version (e.g. part c here). If you used change-of-variables, don’t redo it. We’ll award full credit if you did it right. But otherwise, consider using the moment-generating function (mgf) of a random vector \\(\\mathbf{x}\\):\n\\[\nM(\\mathbf{t})=E\\left(e^{\\mathbf{t}^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{x}}\\right).\n\\]\nAs in the univariate case, when it exists, the mgf uniquely characterizes the entire distribution of a random vector, just like the density, cdf, or characteristic function do."
  },
  {
    "objectID": "problems/pset-1.html",
    "href": "problems/pset-1.html",
    "title": "Problem Set 1",
    "section": "",
    "text": "If \\(X\\sim t_\\nu\\) has Student’s \\(t\\)-distribution, then its density is\n\\[\nf_X(x)=\\frac{\\Gamma \\left(\\frac{\\nu + 1}{2}\\right)}{\\sqrt{\\pi\\nu}\\, \\Gamma\\left(\\frac{\\nu}{2}\\right)} \\left(1 + \\frac{x^2}{\\nu}\\right)^{-\\frac{\\nu+1}{2}},\\quad x\\in\\mathbb{R}.\n\\]\nIf you define a location-scale transformation \\(Y=\\mu+\\tau X\\) for constants \\(\\mu\\in\\mathbb{R}\\) and \\(\\tau&gt;0\\), then the new random variable \\(Y\\sim t(\\nu,\\,\\mu,\\,\\tau^2)\\) has a non-standard Student’s \\(t\\)-distribution. If we did this to the Cauchy for example (\\(\\nu=1\\)), then we get:\n\n\n\n\n\n\n\n\nBe careful not to immediately interpret \\(\\mu\\) and \\(\\tau^2\\) as mean and variance. Only if \\(\\nu&gt;1\\) is \\(E(Y)=\\mu\\), and \\(\\tau^2\\) is only proportional to the variance if \\(\\nu&gt;2\\).\nAnyway, consider a bivariate distribution \\(p(y,\\,\\sigma^2)\\) written hierarchically:\n\\[\n\\begin{aligned}\n\\sigma^2\n&\n\\sim\n\\text{IG}(a,\\,b)\n\\\\\ny\n\\,|\\,\n\\sigma^2\n&\n\\sim\n\\text{N}\n(m\n,\\,\n\\sigma^2v^2)\n.\n\\end{aligned}\n\\]\nDerive the marginal density of \\(y\\) and show that it has a non-standard Student’s \\(t\\)-distribution. What are the parameters?",
    "crumbs": [
      "Problem Sets",
      "Probem Set 1"
    ]
  },
  {
    "objectID": "problems/pset-1.html#problem-1",
    "href": "problems/pset-1.html#problem-1",
    "title": "Problem Set 1",
    "section": "",
    "text": "If \\(X\\sim t_\\nu\\) has Student’s \\(t\\)-distribution, then its density is\n\\[\nf_X(x)=\\frac{\\Gamma \\left(\\frac{\\nu + 1}{2}\\right)}{\\sqrt{\\pi\\nu}\\, \\Gamma\\left(\\frac{\\nu}{2}\\right)} \\left(1 + \\frac{x^2}{\\nu}\\right)^{-\\frac{\\nu+1}{2}},\\quad x\\in\\mathbb{R}.\n\\]\nIf you define a location-scale transformation \\(Y=\\mu+\\tau X\\) for constants \\(\\mu\\in\\mathbb{R}\\) and \\(\\tau&gt;0\\), then the new random variable \\(Y\\sim t(\\nu,\\,\\mu,\\,\\tau^2)\\) has a non-standard Student’s \\(t\\)-distribution. If we did this to the Cauchy for example (\\(\\nu=1\\)), then we get:\n\n\n\n\n\n\n\n\nBe careful not to immediately interpret \\(\\mu\\) and \\(\\tau^2\\) as mean and variance. Only if \\(\\nu&gt;1\\) is \\(E(Y)=\\mu\\), and \\(\\tau^2\\) is only proportional to the variance if \\(\\nu&gt;2\\).\nAnyway, consider a bivariate distribution \\(p(y,\\,\\sigma^2)\\) written hierarchically:\n\\[\n\\begin{aligned}\n\\sigma^2\n&\n\\sim\n\\text{IG}(a,\\,b)\n\\\\\ny\n\\,|\\,\n\\sigma^2\n&\n\\sim\n\\text{N}\n(m\n,\\,\n\\sigma^2v^2)\n.\n\\end{aligned}\n\\]\nDerive the marginal density of \\(y\\) and show that it has a non-standard Student’s \\(t\\)-distribution. What are the parameters?",
    "crumbs": [
      "Problem Sets",
      "Probem Set 1"
    ]
  },
  {
    "objectID": "problems/pset-1.html#problem-2",
    "href": "problems/pset-1.html#problem-2",
    "title": "Problem Set 1",
    "section": "Problem 2",
    "text": "Problem 2\nRecall our good ol’ pal the Gaussian AR(1):\n\\[\n\\begin{aligned}\ny_t\n&=\n\\beta_0+\\beta_1y_{t-1}+\\varepsilon_t, && \\varepsilon_t\\overset{\\text{iid}}{\\sim}\\text{N}(0,\\,\\sigma^2)\n\\\\\ny_0\n&\\sim\n\\text{N}(\\mu_0,\\,v_0^2).\n\\end{aligned}\n\\]\nWithout assuming anything about \\(\\beta_1\\in\\mathbb{R}\\), derive the covariance kernel of the joint distribution:\n\\[\n\\gamma(s,\\, t) = \\text{cov}(y_s,\\,y_t),\\quad s,\\,t\\in\\mathbb{N}.\n\\]",
    "crumbs": [
      "Problem Sets",
      "Probem Set 1"
    ]
  },
  {
    "objectID": "problems/pset-1.html#problem-3",
    "href": "problems/pset-1.html#problem-3",
    "title": "Problem Set 1",
    "section": "Problem 3",
    "text": "Problem 3\nConsider this silly time series process:\n\\[\ny_t\n=\n\\alpha\n+\n\\varepsilon_t\n+\n\\theta_1\n\\varepsilon_{t-1}\n+\n\\theta_2\n\\varepsilon_{t-2}\n,\\quad \\varepsilon_t\\overset{\\text{iid}}{\\sim}\\text{N}(0,\\,\\sigma^2).\n\\]\nDerive the joint distribution of \\(\\mathbf{y}=\\begin{bmatrix}y_1&y_2&\\cdots&y_T\\end{bmatrix}^{\\scriptscriptstyle\\mathsf{T}}\\). In particular…\n\nWhat distribution family does the joint distribution belong to? Justify your answer; don’t just assert it.\nDerive the mean function:\n\n\\[\n\\mu(t)=E(y_t).\n\\]\n\nDerive the covariance kernel:\n\n\\[\n\\gamma(s,\\, t) = \\text{cov}(y_s,\\,y_t),\\quad s,\\,t\\in\\mathbb{N}.\n\\]\n\nIs \\(y_t\\) stationary? Why or why not?",
    "crumbs": [
      "Problem Sets",
      "Probem Set 1"
    ]
  },
  {
    "objectID": "problems/pset-1.html#problem-4",
    "href": "problems/pset-1.html#problem-4",
    "title": "Problem Set 1",
    "section": "Problem 4",
    "text": "Problem 4\n\nModels are wrong and parameters don’t exist. All that matters is observables.\n\nThat statement is the essence of the predictive view of inference. In its Bayesian incarnation, this view says that the only objects that truly matter are the predictive distributions:\n\\[\n\\begin{aligned}\np(\\mathbf{y}_{1:T})\n&=\n\\int\np(\\mathbf{y}_{1:T}\\,|\\,\\boldsymbol{\\theta})\np(\\boldsymbol{\\theta})\n\\,\\text{d}\\boldsymbol{\\theta}\n&&\n(\\text{prior predictive})\n\\\\\np(\\mathbf{y}_{T+1:T+H}\\,|\\,\\mathbf{y}_{1:T})\n&=\n\\int\np(\\mathbf{y}_{T+1:T+H}\\,|\\,\\mathbf{y}_{1:T},\\,\\boldsymbol{\\theta})\np(\\boldsymbol{\\theta}\\,|\\,\\mathbf{y}_{1:T})\n\\,\\text{d}\\boldsymbol{\\theta}.\n&&\n(\\text{posterior predictive})\n\\end{aligned}\n\\]\nModels, parameters, likelihoods, priors, even Bayes’ theorem – these are all just a means to the end of specifying and accessing predictive distributions for observable quantities. So, when you write down a model and a prior on the model’s parameters, you should tune the prior on parameters to capture your prior beliefs about how the data will look before you see them. In other words, the goal of prior elicitation should be accurately eliciting the prior predictive distribution, not the prior on parameters per se. Let’s take this seriously in a toy example.\nImagine you are a data scientist at Feta Platforms Inc, and one day your manager Zohn Jito knocks on your office door. Zohn Jito is an irritating fool who knows nothing about statistics, and he is always making ridiculous requests. On this day, he asks you to build a model for the log of daily feta sales, and he describes his beliefs about how the data should look:\n\nThe series should look smoother than white noise, but not as smooth as a random walk. Runs of consecutive positive values are expected, but they should not typically last more than 5 – 6 periods. The series should hover around zero, but occasionally wander away and then return.\n\nYou decide to build an AR(1) model with normal-inverse-gamma prior (maybe conjugate, maybe not?). Your tasks:\n\nchoose a prior and set the hyperparameters so that the prior predictive distribution is consistent with Zohn’s description;\nsimulate prior predictive sample paths of length 100 and visually check whether they look the way they should;\nif not, adjust your prior until they do.",
    "crumbs": [
      "Problem Sets",
      "Probem Set 1"
    ]
  },
  {
    "objectID": "problems/pset-1.html#problem-5",
    "href": "problems/pset-1.html#problem-5",
    "title": "Problem Set 1",
    "section": "Problem 5",
    "text": "Problem 5\nAs we saw in lecture, (conditional) likelihood-based inference for the AR(1) proceeds identically to iid multiple regression. Whether it’s MLE or Bayes, nothing changes. In particular, imagine you place a conjugate normal-inverse-gamma prior on the model parameters:\n\\[\n\\begin{aligned}\n\\sigma^2\n&\\sim\n\\text{IG}(a_0,\\, b_0)\n\\\\\n\\boldsymbol{\\beta}\\,|\\, \\sigma^2\n&\\sim\n\\text{N}_2(\\mathbf{m}_0,\\,\\sigma^2\\mathbf{H}^{-1}_0)\n\\\\\ny_t\n\\,|\\,\ny_{t-1}\n,\\,\n\\boldsymbol{\\beta},\\,\\sigma^2\n&\\sim \\text{N}\n\\left(\n\\mathbf{x}_t^{\\scriptscriptstyle\\mathsf{T}}\\boldsymbol{\\beta},\\,\\sigma^2\n\\right), && \\mathbf{x}_t=\\begin{bmatrix}1 & y_{t-1}\\end{bmatrix}^{\\scriptscriptstyle\\mathsf{T}}.\n\\end{aligned}\n\\]\nThen you get a normal-inverse-gamma posterior:\n\\[\n\\begin{aligned}\n\\sigma^2\\,|\\, y_{0:T}\n&\\sim\n\\text{IG}(a_T,\\, b_T)\n\\\\\n\\boldsymbol{\\beta}\\,|\\, \\sigma^2,\\, y_{0:T}\n&\\sim\n\\text{N}_2(\\mathbf{m}_T,\\,\\sigma^2\\mathbf{H}^{-1}_T)\n\\\\\n\\\\\n\\mathbf{H}_T\n&=\n\\mathbf{X}_T^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{X}_T+\\mathbf{H}_0\n\\\\\n\\mathbf{m}_T\n&=\n\\mathbf{H}_T^{-1}(\\mathbf{X}_T^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{y}_T+\\mathbf{H}_0\\mathbf{m}_0)\n\\\\\na_T\n&=\na_0 + T/2\n\\\\\nb_T\n&=\nb_0\n+\n(\\mathbf{y}_T^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{y}_T+\\mathbf{m}_0^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{H}_0\\mathbf{m}_0-\\mathbf{m}_T^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{H}_T\\mathbf{m}_T)/2.\n\\end{aligned}\n\\]\nThose formulas for updating the hyperparameters are great for batch inference, but inefficient if the data are streaming. So let’s improve them. Imagine we have collected data \\(y_{0:t-1}\\) and characterized the posterior up to that point:\n\\[\n\\begin{aligned}\n\\sigma^2\\,|\\, y_{0:t-1}\n&\\sim\n\\text{IG}(a_{t-1},\\, b_{t-1})\n\\\\\n\\boldsymbol{\\beta}\\,|\\, \\sigma^2,\\, y_{0:t-1}\n&\\sim\n\\text{N}_2(\\mathbf{m}_{t-1},\\,\\sigma^2\\mathbf{H}^{-1}_{t-1}).\n\\end{aligned}\n\\]\nThen a single new observation \\(y_t\\) arrives and you want to characterize the new posterior:\n\\[\n\\begin{aligned}\n\\sigma^2\\,|\\, y_{0:t}\n&\\sim\n\\text{IG}(a_{t},\\, b_{t})\n\\\\\n\\boldsymbol{\\beta}\\,|\\, \\sigma^2,\\, y_{0:t}\n&\\sim\n\\text{N}_2(\\mathbf{m}_{t},\\,\\sigma^2\\mathbf{H}^{-1}_{t}).\n\\end{aligned}\n\\]\n\nDerive a recursion that takes the old hyperparameters and the new data and computes the new hyperparameters without inverting any matrices;\n\nc(sunspots) is a monthly time series of mean relative sunspot numbers from 1749 to 1983. Write a for loop in R that uses your recursion to fit an AR(1) to these data one observation at a time. Verify that at the end of the loop, you get the same estimates that you would have gotten if you had applied the batch update to the entire data set all at once.",
    "crumbs": [
      "Problem Sets",
      "Probem Set 1"
    ]
  },
  {
    "objectID": "problems/pset-1.html#problem-6",
    "href": "problems/pset-1.html#problem-6",
    "title": "Problem Set 1",
    "section": "Problem 6",
    "text": "Problem 6\nFRED is a fabulous source of economic time series data, and this problem encourages you to play around with it. Please do the following:\n\nSurf around FRED for a while until you find a time series that interests you, then use the fredr package to pull the data into R;\nUsing your online recursion from the previous problem, write a loop that sequentially fits a Bayesian AR(1) with conjugate prior to the time series you chose;\nIn each step of the loop, compute the PIT value comparing the one-step posterior predictive distribution \\(p(y_t\\,|\\,y_{0:t-1})\\) to the actual, realized value \\(y_t\\);\nAt the end of the loop, plot a histogram of the PITs. Are the forecasts well-calibrated? If not, what diagnostic information does the histogram convey?\n\n\n\n\n\n\n\nHint\n\n\n\nThe extraDistr package provides tools for the non-standard Student’s \\(t\\) distribution.",
    "crumbs": [
      "Problem Sets",
      "Probem Set 1"
    ]
  },
  {
    "objectID": "problems/pset-1.html#submission",
    "href": "problems/pset-1.html#submission",
    "title": "Problem Set 1",
    "section": "Submission",
    "text": "Submission\nYou are free to compose your solutions for this problem set however you wish (scan or photograph written work, handwriting capture on a tablet device, LaTeX, Quarto, whatever) as long as the final product is a single PDF file. You must upload this to Gradescope and mark the pages associated with each problem.\nDo not forget to include the following:\n\nFor each problem, please acknowledge your collaborators;\nIf a problem required you to code something, please include both the code and the output.",
    "crumbs": [
      "Problem Sets",
      "Probem Set 1"
    ]
  },
  {
    "objectID": "problems/pset-2.html",
    "href": "problems/pset-2.html",
    "title": "Problem Set 2",
    "section": "",
    "text": "Consider the mean-zero AR(2):\n\\[\ny_t=\\beta_1y_{t-1}+\\beta_2y_{t-2}+\\varepsilon_t,\\quad\\varepsilon_t\\overset{\\text{iid}}{\\sim}\\text{N}(0,\\,\\sigma^2).\n\\]\nWe can rewrite this in state-space form:\n\\[\n\\begin{aligned}\ny_t&=\\begin{bmatrix}1&0\\end{bmatrix}\\begin{bmatrix}y_t\\\\y_{t-1}\\end{bmatrix}\n\\\\\n\\begin{bmatrix}y_t\\\\y_{t-1}\\end{bmatrix}\n&=\n\\begin{bmatrix}\\beta_1&\\beta_2\\\\1&0\\end{bmatrix}\n\\begin{bmatrix}y_{t-1}\\\\y_{t-2}\\end{bmatrix}\n+\n\\begin{bmatrix}\\varepsilon_t\\\\0\\end{bmatrix}\n.\n\\end{aligned}\n\\]\nIn order for the process to be stationary, the eigenvalues of the companion matrix must all lie inside the unit circle. This happens if and only if the pair \\((\\beta_1,\\,\\beta_2)\\) lies in this region:\n\n\n\n\n\n\n\n\nProve it."
  },
  {
    "objectID": "problems/pset-2.html#problem-1",
    "href": "problems/pset-2.html#problem-1",
    "title": "Problem Set 2",
    "section": "",
    "text": "Consider the mean-zero AR(2):\n\\[\ny_t=\\beta_1y_{t-1}+\\beta_2y_{t-2}+\\varepsilon_t,\\quad\\varepsilon_t\\overset{\\text{iid}}{\\sim}\\text{N}(0,\\,\\sigma^2).\n\\]\nWe can rewrite this in state-space form:\n\\[\n\\begin{aligned}\ny_t&=\\begin{bmatrix}1&0\\end{bmatrix}\\begin{bmatrix}y_t\\\\y_{t-1}\\end{bmatrix}\n\\\\\n\\begin{bmatrix}y_t\\\\y_{t-1}\\end{bmatrix}\n&=\n\\begin{bmatrix}\\beta_1&\\beta_2\\\\1&0\\end{bmatrix}\n\\begin{bmatrix}y_{t-1}\\\\y_{t-2}\\end{bmatrix}\n+\n\\begin{bmatrix}\\varepsilon_t\\\\0\\end{bmatrix}\n.\n\\end{aligned}\n\\]\nIn order for the process to be stationary, the eigenvalues of the companion matrix must all lie inside the unit circle. This happens if and only if the pair \\((\\beta_1,\\,\\beta_2)\\) lies in this region:\n\n\n\n\n\n\n\n\nProve it."
  },
  {
    "objectID": "problems/pset-2.html#problem-2",
    "href": "problems/pset-2.html#problem-2",
    "title": "Problem Set 2",
    "section": "Problem 2",
    "text": "Problem 2\nConsider again the mean-zero AR(2):\n\\[\ny_t=\\beta_1y_{t-1}+\\beta_2y_{t-2}+\\varepsilon_t,\\quad\\varepsilon_t\\overset{\\text{iid}}{\\sim}\\text{N}(0,\\,\\sigma^2).\n\\]\nAssuming the process is stationary, derive concrete formulas for the Yule-Walker estimators of \\(\\beta_1\\), \\(\\beta_2\\), and \\(\\sigma^2\\). To keep the formulas clean, you may find it helpful to introduce notation for the autocorrelation function of a stationary process:\n\\[\n\\rho(h)=\\frac{\\gamma(h)}{\\gamma(0)}=\\frac{\\text{cov}(y_{t+h},\\,y_t)}{\\text{var}(y_t)}=\\text{cor}(y_{t+h},\\,y_t).\n\\]"
  },
  {
    "objectID": "problems/pset-2.html#problem-3",
    "href": "problems/pset-2.html#problem-3",
    "title": "Problem Set 2",
    "section": "Problem 3",
    "text": "Problem 3\nLast time, you derived a recursive updating rule for conjugate Bayesian linear regression. The recursion for the posterior mean of the regression coefficients is\n\\[\n\\newcommand{\\tr}{{\\scriptscriptstyle\\mathsf{T}}}\n\\begin{aligned}\n\\mathbf{k}_t&=\\frac{\\mathbf{H}_{t-1}^{-1}\\mathbf{x}_t}{1+\\mathbf{x}_t^\\tr\\mathbf{H}_{t-1}^{-1}\\mathbf{x}_t}\n\\\\\n\\mathbf{m}_t\n&=\n\\mathbf{m}_{t-1}\n+\n\\mathbf{k}_t(y_t-\\mathbf{x}_t^\\tr\\mathbf{m}_{t-1})\n.\n\\end{aligned}\n\\]\nThat vector \\(\\mathbf{k}_t\\) is sometimes called the Kalman gain. You probably derived this using purely linear algebraic ideas like the Sherman-Morrison formula, but in fact the form of this recursion has important probabilistic meaning as well.\nImagine you are taking a conjugate Bayesian approach to fitting a Gaussian AR(p). After observing the first \\(t-1\\) observations, here is where things stand:\n\\[\n\\begin{aligned}\n\\sigma^2\\,|\\, y_{1:t-1}\n&\\sim\n\\text{IG}(a_{t-1},\\, b_{t-1})\n\\\\\n\\boldsymbol{\\beta}\\,|\\, \\sigma^2,\\, y_{1:t-1}\n&\\sim\n\\text{N}_{p+1}(\\mathbf{m}_{t-1},\\,\\sigma^2\\mathbf{H}^{-1}_{t-1})\\\\\ny_t\\,|\\,\\boldsymbol{\\beta},\\,\\sigma^2,\\, y_{1:t-1}\n&\\sim\\text{N}(\\mathbf{x}_t^{\\scriptscriptstyle\\mathsf{T}}\\boldsymbol{\\beta},\\,\\sigma^2) &&\n\\mathbf{x}_t=\\begin{bmatrix}1&y_{t-1}&\\cdots &y_{t-p}\\end{bmatrix}^{\\scriptscriptstyle\\mathsf{T}}.\n\\end{aligned}\n\\]\n\nUsing results from Problem Set 0, show that the joint distribution of \\(\\boldsymbol{\\beta}\\) and \\(y_t\\) is multivariate normal, and state the moments:\n\n\\[\n\\begin{bmatrix}\n\\boldsymbol{\\beta}\\\\\ny_{t}\n\\end{bmatrix}\n\\,|\\,\\sigma^2,\\,y_{1:t-1}\n\\sim\n\\text{N}_{p+2}\n\\left(?,\\,?\\right)\n\\]\n\nUsing part a and standard results for the conditional distributions of multivariate normals, what is the conditional posterior \\(p(\\boldsymbol{\\beta}\\,|\\,\\sigma^2,\\,y_{1:t})\\)?\nUsing part b, what is the probabilistic interpretation of the Kalman gain \\(\\mathbf{k}_t\\)?"
  },
  {
    "objectID": "problems/pset-2.html#problem-4",
    "href": "problems/pset-2.html#problem-4",
    "title": "Problem Set 2",
    "section": "Problem 4",
    "text": "Problem 4\nRecall the mean-zero MA(q):\n\\[\ny_t\n=\n\\sum_{i=1}^q\\theta_i\\varepsilon_{t-i}\n+\n\\varepsilon_t,\n\\quad\n\\varepsilon_t\n\\overset{\\text{iid}}{\\sim}\n\\text{N}(0,\\, \\sigma^2)\n.\n\\]\nIt is useful to write this in the form of a linear state-space system:\n\\[\n\\begin{aligned}\ny_t\n&=\n\\mathbf{F}\\mathbf{s}_t\n+\n\\mathbf{Q}\\boldsymbol{\\varepsilon}_t\n\\\\\n\\mathbf{s}_t\n&=\\mathbf{G}\n\\mathbf{s}_{t-1}\n+\n\\mathbf{R}\\boldsymbol{\\eta}_t\n\\end{aligned}\n\\]\nFor better or worse, there are many equivalent ways to do this depending on what you think of as the “state,” and they all have their uses:\n\n\n\\(\\mathbf{s}_t=\\begin{bmatrix}\\varepsilon_t & \\varepsilon_{t-1}&\\cdots & \\varepsilon_{t-q}\\end{bmatrix}^{\\scriptscriptstyle\\mathsf{T}}\\in\\mathbb{R}^{q+1}\\);\n\n\\(\\mathbf{s}_t=\\begin{bmatrix}\\sum\\limits_{i=0}^q\\theta_i\\varepsilon_{t-i} & \\sum\\limits_{i=2}^q\\theta_i\\varepsilon_{t-i+2}&\\cdots&\\theta_q\\varepsilon_t\\end{bmatrix}^{\\scriptscriptstyle\\mathsf{T}}\\in\\mathbb{R}^{q+1}\\);\n\n\\(\\mathbf{s}_t=\\begin{bmatrix}y_t-\\varepsilon_t & y_{t-1}-\\varepsilon_{t-1}&\\cdots & y_{t-q+1}-\\varepsilon_{t-q+1}\\end{bmatrix}^{\\scriptscriptstyle\\mathsf{T}}\\in\\mathbb{R}^q\\);\n\nFor each version of the state vector, figure out what the system components need to be in order for the state-space system to be equivalent to the MA(q). Some of the objects in the system might be unnecessary and you can set them to zero."
  },
  {
    "objectID": "problems/pset-2.html#problem-5",
    "href": "problems/pset-2.html#problem-5",
    "title": "Problem Set 2",
    "section": "Problem 5",
    "text": "Problem 5\nThe ARMAacf function in base R computes the autocorrelation function \\(\\rho(h)=\\gamma(h)/\\gamma(0)\\) for the ARMA(p, q) model. Here I’ve wrapped it in a helper that creates a plot:\n\n# plot the autocorrelation function of an ARMA\nplot_arma_acf &lt;- function(ar = numeric(), ma = numeric(), lag.max = 10){\n  y_vals &lt;- ARMAacf(ar = ar, ma = ma, lag.max = lag.max)\n  plot(0:lag.max, y_vals, pch = 19, ylim = c(-1, 1),\n       xlab = \"h\", ylab = expression(rho~\"(h)\"))\n  segments(0:lag.max, 0, 0:lag.max, y_vals)\n  abline(h = 0, col = \"lightgrey\")\n}\n\nSo there you go. John Zito did something nice for once. But unfortunately, his evil twin Zohn Jito is back making ridiculous demands. He visits your office in an unhinged frenzy. Much like Klytaemnestra in Strauss’s opera Elektra, his sleep has been troubled (“Ich habe keine guten Nächte”). He has been haunted by feverish dreams of autocorrelation functions, but he knows not from whence they came. Dripping with sweat, he pants:\n\n“The autocorrelations start high, but then shrink smoothly and monotonically toward zero, without changing sign. It feels like each step of the series only remembers the last one, nothing more complicated.”\n“The correlations seem to alternate signs: positive, then negative, then positive again. They get smaller each time, kind of like a ringing bell that fades away.”\n“The correlations are definitely there at lag 1, a little bit at lag 2, but after that it looks like nothing. Totally flat.”\n“At lag 1 the correlation is negative, but then it’s pretty much gone. Looks like it bounces once and stops.”\n“The autocorrelations stick around forever. They go down, but it’s painfully slow, like the series doesn’t want to forget the past. It looks borderline nonstationary.”\n“There’s a big spike at lag 1, smaller at lag 2, then it tails off smoothly. It’s not a clean cut-off, but it doesn’t oscillate either.”\n“Lag 1 correlation is very strong, lag 2 is still positive but smaller, and then they slowly decay. No sign changes. The series is persistent, but it looks like the short-term correlation is extra strong compared to the long-term tail.”\n“At lag 1 it’s negative, lag 2 bounces back positive, then it decays but stays positive after that.”\n“It’s not a smooth oscillation like a sine wave, but the correlations jump positive, negative, positive for a few lags before dying off.”\n“There’s one big spike at lag 1, maybe a tiny one at lag 2, and then nothing.”\n“First correlation positive, second negative, third positive but small, then basically gone.”\n“The decay is slower than I’d expect for an MA, but the lag-1 correlation isn’t as huge as in an AR(1). It’s like a compromise between short-term and long-term memory.”\n“The first few correlations are really high and fluctuate a little in sign, then they slowly die away like a bell fading in a canyon. It’s more complicated than the simple sine-wave oscillation.”\n“Correlations die very quickly after lag 3, but the first three lags don’t decrease monotonically—they bounce around a bit.”\n“The series looks nearly like white noise. Most of the correlations are tiny, but if you squint you can see a barely-there persistence beyond lag 5 or 6.”\n“Some of the lags alternate in sign, others don’t. It’s not a simple sine wave, more like a complicated dance that fades gradually.”\n“Lag 1 correlation is negative and quite strong, then lag 2 is barely positive, then the series remembers the past moderately for several lags.”\n\nHelp bring this poor man some relief! Play around with different values of \\(p\\), \\(q\\), and the model parameters until you get an autocorrelation function that matches each description.\n\n\n\n\n\n\nJust do ten\n\n\n\nRespond to ten of the statements for full credit, but play around with all of them to prepare for the midterm."
  },
  {
    "objectID": "problems/pset-2.html#problem-6",
    "href": "problems/pset-2.html#problem-6",
    "title": "Problem Set 2",
    "section": "Problem 6",
    "text": "Problem 6\nThe arima.sim function in base R does exactly what it says it does. Feel free to read the documentation. Furthermore, below the fold are some helper functions that implement basic operations for Bayesian linear regression with conjugate priors:\n\n\n\n\n\n\nFree stuff!\n\n\n\n\n\n\nblr_nig_update &lt;- function(y_t, x_t, a, b, m, invH) {\n  # Given old prior (a, b, m, invH) and new data point (y, x), compute posterior\n  # x_t: column vector (p x 1), m: column vector (p x 1), invH: (p x p)\n  \n  e &lt;- as.numeric(y_t - t(x_t) %*% m)\n  r2 &lt;- as.numeric(1 + t(x_t) %*% invH %*% x_t)\n  k &lt;- invH %*% x_t / r2\n  \n  # update mean\n  m_new &lt;- m + k * e\n  \n  # update covariance\n  invH_new &lt;- invH - k %*% t(x_t) %*% invH\n  \n  # update shape and scale\n  a_new &lt;- a + 0.5\n  b_new &lt;- b + 0.5 * (e^2 / r2)\n  \n  return(list(a = a_new, b = b_new, m = m_new, invH = invH_new))\n}\n\nblr_1step_predictive &lt;- function(x, a, b, m, invH){\n  list(df = 2*a, \n       ybar = c(t(x) %*% m), \n       s2 = (b/a) * c(1 + t(x) %*% invH %*% x))\n}\n\nblr_marginal_likelihood &lt;- function(y, X, mu0, Lambda0, a0, b0, \n                                    mu_n, Lambda_n, a_n, b_n) {\n  n &lt;- length(y)\n  k &lt;- ncol(X)\n  \n  # determinants via Cholesky for stability\n  det_ratio &lt;- determinant(Lambda0, logarithm = TRUE)$modulus -\n    determinant(Lambda_n, logarithm = TRUE)$modulus\n  \n  log_ml &lt;- lgamma(a_n) - lgamma(a0) +\n    a0 * log(b0) - a_n * log(b_n) +\n    0.5 * det_ratio -\n    (n / 2) * log(2 * pi)\n  \n  ml &lt;- exp(log_ml)\n  return(list(log_marginal_lik = as.numeric(log_ml),\n              marginal_lik = as.numeric(ml)))\n}\n\n\n\n\nUsing these raw materials, write a simulation study that generates a synthetic time series from a pure MA(1) with \\(\\theta_1=0.99\\). Then, mistakenly fit Bayesian AR(p) to the simulated data using these initial priors:\n\\[\n\\begin{aligned}\np&\\sim\\text{Unif}(\\{1,\\,2,\\,3\\})\\\\\n\\sigma^2&\\sim\\text{IG}(1,\\,1)\\\\\n\\boldsymbol{\\beta}\\,|\\,\\sigma^2,\\,p&\\sim\\text{N}_{p+1}\\left(\\mathbf{0},\\,\\sigma^2\\mathbf{I}_{p+1}\\right).\n\\end{aligned}\n\\]\nStarting from these priors, write a for loop that processes the data one observation at a time for each model. Along the way, keep track of the one-step-ahead posterior predictive distributions (non-standard Student’s \\(t\\)), the posterior model probabilities, and the “best” model (the one with highest probability).\nAt the end of the simulation, construct the histogram of PITs for each of three forecasting distributions:\n\n\n\n\n\n\n\n“best” plug-in\n\\(\\text{N}(\\mathbf{x}_t^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{m}_{t-1}, b_{t-1} / a_{t-1})\\)\n\n\n“best” posterior predictive\n\\(p(y_t\\,|\\,y_{1:t-1},\\,\\hat{p})\\)\n\n\nmodel-averaged\n\\(\\sum_{p=1}^3\\text{Pr}(p\\,|\\,y_{1:t-1})\\times p(y_t\\,|\\,y_{1:t-1},\\,p)\\)\n\n\n\nComment on the calibration of each, and don’t forget that the model we are fitting is fundamentally misspecified."
  },
  {
    "objectID": "problems/pset-2.html#submission",
    "href": "problems/pset-2.html#submission",
    "title": "Problem Set 2",
    "section": "Submission",
    "text": "Submission\nYou are free to compose your solutions for this problem set however you wish (scan or photograph written work, handwriting capture on a tablet device, LaTeX, Quarto, whatever) as long as the final product is a single PDF file. You must upload this to Gradescope and mark the pages associated with each problem.\nDo not forget to include the following:\n\nFor each problem, please acknowledge your collaborators;\nIf a problem required you to code something, please include both the code and the output."
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#point-forecast",
    "href": "slides/04-forecast-evaluation.html#point-forecast",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Point forecast",
    "text": "Point forecast\nYour single-number best guess at tomorrow’s observation:"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#forecast-interval",
    "href": "slides/04-forecast-evaluation.html#forecast-interval",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Forecast interval",
    "text": "Forecast interval\nA range of likely values for tomorrow’s observation:"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#forecast-density",
    "href": "slides/04-forecast-evaluation.html#forecast-density",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Forecast density",
    "text": "Forecast density\nFull distribution capturing uncertainty about tomorrow:"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#and-then-tomorrow-finally-comes",
    "href": "slides/04-forecast-evaluation.html#and-then-tomorrow-finally-comes",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "And then tomorrow finally comes",
    "text": "And then tomorrow finally comes\nSo…how’d we do?"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#whats-the-point",
    "href": "slides/04-forecast-evaluation.html#whats-the-point",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "What’s the point?",
    "text": "What’s the point?\n\nWe want intervals and densities to communicate uncertainty about the forecast;\n\nWhat sources of uncertainty?\n\nBasic data uncertainty;\nParameter estimation uncertainty;\nHyperparameter uncertainty;\nModel uncertainty;\nUncertainty introduced by missing data.\n\n\nIn the small world of the AR(1), mainly the first two for now."
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#how-do-you-get-full-predictive-distributions",
    "href": "slides/04-forecast-evaluation.html#how-do-you-get-full-predictive-distributions",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "How do you get full predictive distributions?",
    "text": "How do you get full predictive distributions?\n\nIn general, use simulation:\n\nClassical approach: bootstrapping;\nBayesian approach: posterior predictive simulation.\n\n\n\nEither way, you get Monte Carlo draws from a forecast distribution:\n\\[\n\\tilde{y}_{t+h}^{(1)}\\com \\tilde{y}_{t+h}^{(2)}\\com ...\\com \\tilde{y}_{t+h}^{(k)}\n\\sim \\hat{F}_{t+h|t}.\n\\]\n\n\nWhat do you do with them?"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#probabilistic-forecasting-via-monte-carlo",
    "href": "slides/04-forecast-evaluation.html#probabilistic-forecasting-via-monte-carlo",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Probabilistic forecasting via Monte Carlo",
    "text": "Probabilistic forecasting via Monte Carlo\nUse the simulations \\(\\tilde{y}_{t+h}^{(1:k)}=\\left\\{\\tilde{y}_{t+h}^{(1)}\\com \\tilde{y}_{t+h}^{(2)}\\com ...\\com \\tilde{y}_{t+h}^{(k)}\\right\\}\\) to construct whatever forecast object you want:\n\n\\[\n\\hat{y}_{t+h|t}=\\frac{1}{k}\\sum\\limits_{j=1}^k\\tilde{y}_{t+h}^{(j)}. \\quad (\\text{...or median})\n\\]\n\n\nForecast interval:\n\\[\n\\hat{I}_{t+h|t} = \\left[\\hat{Q}_{\\frac{\\alpha}{2}}\\left(\\tilde{y}_{t+h}^{(1:k)}\\right)\\com \\hat{Q}_{1-\\frac{\\alpha}{2}}\\left(\\tilde{y}_{t+h}^{(1:k)}\\right)\\right]. \\quad (...\\text{or hdi})\n\\]\n\n\nForecast distribution:\n\\[\n\\hat{f}_{t+h|t}\n=\n\\text{histogram}\\left(\\tilde{y}_{t+h}^{(1:k)}\\right). \\quad (...\\text{or kde})\n\\]"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#well-behaved-case-iid-normal",
    "href": "slides/04-forecast-evaluation.html#well-behaved-case-iid-normal",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Well-behaved case: iid normal",
    "text": "Well-behaved case: iid normal\n\nAssume\n\n\n\\[\ny_1\\com y_2\\com ...\\com y_n\\com y_{n+1}\\iid\\N(\\mu\\com\\sigma^2).\n\\]\n\n\nWe know\n\\[\n\\frac{\\bar{y}_{n}-y_{n+1}}{\\sigma\\sqrt{1+\\frac{1}{n}}}\\sim\\N(0\\com 1),\n\\]\n\n\nand so plugging in sample standard deviation \\(s_n\\) gives\n\\[\n\\frac{\\bar{y}_{n}-y_{n+1}}{s_n\\sqrt{1+\\frac{1}{n}}}\\sim t_{n-1}.\n\\]\n\n\nThe predictive distribution is non-standard Student’s \\(t\\):\n\\[\n\\hat{y}_{n+1}\\sim t\\left(n-1\\com \\bar{y}_n\\com s_n^2\\left(1+\\frac{1}{n}\\right)\\right).\n\\]"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#well-behaved-case-bayes-with-conjugate-prior",
    "href": "slides/04-forecast-evaluation.html#well-behaved-case-bayes-with-conjugate-prior",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Well-behaved case: Bayes with conjugate prior",
    "text": "Well-behaved case: Bayes with conjugate prior\nA conjugate normal-inverse-gamma prior begets a conjugate posterior:\n\\[\n\\begin{aligned}\n\\sigma^2\\given y_{0:t}\n&\\sim\n\\text{IG}(a_t\\com b_t)\n\\\\\n\\Bbeta\\given \\sigma^2\\com y_{0:t}\n&\\sim\n\\text{N}_2(\\Bm_t\\com\\sigma^2\\BH^{-1}_t)\n\\\\\ny_{t+1}\\given\\Bbeta\\com \\sigma^2\\com y_{0:t}\n&\\sim \\N(\\Bx_{t+1}^\\tr\\Bbeta\\com\\sigma^2).\n\\end{aligned}\n\\]\n\nThe one-step posterior predictive distribution is non-standard Student’s \\(t\\):\n\\[\n\\begin{aligned}\ny_{t+1}\\given y_{0:t}\n&\\sim\nt(\\nu_{t+1|t}\\com\\bar{y}_{t+1|t}\\com s_{t+1|t}^2)\n\\\\\n\\\\\n\\nu_{t+1|t}\n&=\n2a_t\n\\\\\n\\bar{y}_{t+1|t}\n&=\n\\Bx_{t+1}^\\tr\\Bm_t\n\\\\\ns_{t+1|t}^2\n&=\n\\frac{b_t}{a_t}\n(1+\\Bx_{t+1}^\\tr\\BH_t^{-1}\\Bx_{t+1})\n.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#how-do-you-evaluate-the-forecasts",
    "href": "slides/04-forecast-evaluation.html#how-do-you-evaluate-the-forecasts",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "How do you evaluate the forecasts?",
    "text": "How do you evaluate the forecasts?\nYou generate a sequence of one-step-ahead predictions:\n\\[\n\\begin{matrix}\n\\hat{y}_{1|0} & \\hat{y}_{2|1} & \\hat{y}_{3|2} & \\hat{y}_{4|3} & \\hat{y}_{5|4} & \\cdots&\\hat{y}_{t|t-1} & \\cdots\\\\\n\\hat{I}_{1|0} & \\hat{I}_{2|1} & \\hat{I}_{3|2} & \\hat{I}_{4|3} & \\hat{I}_{5|4} & \\cdots&\\hat{I}_{t|t-1} & \\cdots\\\\\n\\hat{f}_{1|0} & \\hat{f}_{2|1} & \\hat{f}_{3|2} & \\hat{f}_{4|3} & \\hat{f}_{5|4} & \\cdots&\\hat{f}_{t|t-1} & \\cdots\n\\end{matrix}\n\\]\n\nBut then the data you were trying to forecast eventually arrive:\n\\[\n\\begin{matrix}\ny_1 & y_2 & y_3 & y_4 & y_5 & \\cdots &y_t & \\cdots\n\\end{matrix}\n\\]\nHow do we score the forecasts and summarize?"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#todays-agenda",
    "href": "slides/04-forecast-evaluation.html#todays-agenda",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Today’s agenda",
    "text": "Today’s agenda\n\nWe will learn how to evaluate probabilistic predictions;\nWe will illustrate by comparing the performance of the two well-behaved methods:\n\nA. classical predictive distribution from iid normal model;\nB. posterior predictive distribution from Gaussian AR(1) with conjugate prior."
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#there-will-be-two-running-examples",
    "href": "slides/04-forecast-evaluation.html#there-will-be-two-running-examples",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "There will be two running examples",
    "text": "There will be two running examples\n\n\nSimulated data from AR(1)\n\nMethod A (iid normal) is wrong by construction;\nMethod B is right by construction;\n\n\n\nApple’s daily stock price from 2000 - this week\n\nboth methods are “wrong,” but is one strictly preferred?\n\n\n\n\nOur forecast metrics will tease all of that out."
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#dataset-1-simulated",
    "href": "slides/04-forecast-evaluation.html#dataset-1-simulated",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Dataset 1: simulated",
    "text": "Dataset 1: simulated"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#forecast-distributions-from-iid-normal-model",
    "href": "slides/04-forecast-evaluation.html#forecast-distributions-from-iid-normal-model",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Forecast distributions from iid normal model",
    "text": "Forecast distributions from iid normal model"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#forecast-distributions-from-bayesian-ar1",
    "href": "slides/04-forecast-evaluation.html#forecast-distributions-from-bayesian-ar1",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Forecast distributions from Bayesian AR(1)",
    "text": "Forecast distributions from Bayesian AR(1)"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#dataset-2-apple-stock-price",
    "href": "slides/04-forecast-evaluation.html#dataset-2-apple-stock-price",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Dataset 2: Apple stock price",
    "text": "Dataset 2: Apple stock price\n\nlibrary(quantmod)\ngetSymbols(\"AAPL\", from = \"2000-01-01\", to = \"2025-09-08\", src = \"yahoo\")\n\n[1] \"AAPL\""
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#forecast-distributions-from-iid-normal-model-1",
    "href": "slides/04-forecast-evaluation.html#forecast-distributions-from-iid-normal-model-1",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Forecast distributions from iid normal model",
    "text": "Forecast distributions from iid normal model"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#forecast-distributions-from-bayesian-ar1-1",
    "href": "slides/04-forecast-evaluation.html#forecast-distributions-from-bayesian-ar1-1",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Forecast distributions from Bayesian AR(1)",
    "text": "Forecast distributions from Bayesian AR(1)"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#any-ideas",
    "href": "slides/04-forecast-evaluation.html#any-ideas",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Any ideas?",
    "text": "Any ideas?"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#point-prediction",
    "href": "slides/04-forecast-evaluation.html#point-prediction",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Point prediction",
    "text": "Point prediction\nWe want the point prediction that minimizes expected loss:\n\n\\[\n\\hat{y}_{t+1|t}\n\\;=\\;\n\\argmin{\\hat{y}\\in\\mathbb{R}}\n\\; E\\big[\\, L\\big(y_{t+1},\\,\\hat{y}\\big) \\,\\big|\\, y_{0:t} \\big].\n\\]\n\n\nThe expectation is taken with respect to the “true” or “idealized” conditional distribution \\(p(y_{t+1}\\given y_{0:t})\\), which we don’t know.\n\n\nWe approximate it with whatever forecast distribution we’ve generated."
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#picking-a-loss-function",
    "href": "slides/04-forecast-evaluation.html#picking-a-loss-function",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Picking a loss function",
    "text": "Picking a loss function\n\nWe have nice results for some loss functions:\n\n\n\\[\n\\begin{array}{rcl}\nL(y_{t+1},\\hat{y}) = (y_{t+1} - \\hat{y})^2\n& \\implies &\n\\hat{y}_{t+1|t} = E[\\,y_{t+1}\\mid y_{0:t}\\,] \\\\[1.2em]\nL(y_{t+1},\\hat{y}) = |y_{t+1} - \\hat{y}|\n& \\implies &\n\\hat{y}_{t+1|t} = \\operatorname{median}(y_{t+1}\\mid y_{0:t}).\n\\end{array}\n\\]\n\n\nAnd there are many more where that came from."
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#in-practice",
    "href": "slides/04-forecast-evaluation.html#in-practice",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "In practice",
    "text": "In practice\nMetrics for scoring the average quality of the point predictions over time:\n\\[\n\\begin{aligned}\n\\text{MSFE}\n&=\n\\frac{1}{T}\n\\sum\\limits_{t=1}^T\n(y_t-\\hat{y}_{t|t-1})^2\n\\\\\n\\text{MAFE}\n&=\n\\frac{1}{T}\n\\sum\\limits_{t=1}^T\n|y_t-\\hat{y}_{t|t-1}|.\n\\end{aligned}\n\\]\nWe want these to be small.\n\n\n\n\n\n\n\nMake sure your loss function and your point prediction play nice\n\n\n\nIf you’re looking at MAFE, use forecast median;\nIf you’re looking at MSFE, use the forecast mean."
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#our-simulated-data",
    "href": "slides/04-forecast-evaluation.html#our-simulated-data",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Our simulated data",
    "text": "Our simulated data\nMSE of forecast mean:\n\nmean((y - pred_params_iid_sim[,\"location\"])^2, na.rm = TRUE)\n\n[1] 162.4966\n\nmean((y - pred_params_ar1_sim[,\"location\"])^2, na.rm = TRUE)\n\n[1] 4.162699\n\n\n\nMAE of forecast median (same as mean for these methods):\n\nmean(abs(y - pred_params_iid_sim[,\"location\"]), na.rm = TRUE)\n\n[1] 10.03766\n\nmean(abs(y - pred_params_ar1_sim[,\"location\"]), na.rm = TRUE)\n\n[1] 1.637622"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#our-real-data",
    "href": "slides/04-forecast-evaluation.html#our-real-data",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Our real data",
    "text": "Our real data\nMSE of forecast mean:\n\nmean((stonks - pred_params_iid_real[,\"location\"])^2, na.rm = TRUE)\n\n[1] 4346.895\n\nmean((stonks - pred_params_ar1_real[,\"location\"])^2, na.rm = TRUE)\n\n[1] 2.111325\n\n\n\nMAE of forecast median (same as mean for these methods):\n\nmean(abs(stonks - pred_params_iid_real[,\"location\"]), na.rm = TRUE)\n\n[1] 37.60877\n\nmean(abs(stonks - pred_params_ar1_real[,\"location\"]), na.rm = TRUE)\n\n[1] 0.612237"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#any-ideas-1",
    "href": "slides/04-forecast-evaluation.html#any-ideas-1",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Any ideas?",
    "text": "Any ideas?"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#interval-width-and-coverage",
    "href": "slides/04-forecast-evaluation.html#interval-width-and-coverage",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Interval width and coverage",
    "text": "Interval width and coverage\n\nYou want intervals that are small enough to be informative, but large enough to swallow the truth often, and there’s a trade-off.\n\n\\(\\hat{I}=(-\\infty\\com \\infty)\\) has perfect coverage but teaches you nothing;\nLook at average size and empirical coverage:\n\n\n\\[\n\\begin{aligned}\n\\overline{\\text{Size}}\n&= \\frac{1}{T} \\sum_{t=1}^{T} \\Big( \\hat{I}_{t\\mid t-1}^{\\text{upper}} - \\hat{I}_{t\\mid t-1}^{\\text{lower}} \\Big), \\\\[0.8em]\n\\overline{\\text{Coverage}}\n&= \\frac{1}{T} \\sum_{t=1}^{T} \\mathbf{1}\\Big\\{ y_t \\in \\hat{I}_{t\\mid t-1} \\Big\\}.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#interval-performance-on-simulated-data",
    "href": "slides/04-forecast-evaluation.html#interval-performance-on-simulated-data",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Interval performance on simulated data",
    "text": "Interval performance on simulated data\n\nSize of 90% intervals:\n\nmean(PI_iid_sim[,1], na.rm = TRUE)   \n\n[1] 37.61362\n\nmean(PI_ar1_sim[,1], na.rm = TRUE)   \n\n[1] 6.660308\n\n\n\n\nCoverage of 90% intervals:\n\nmean(PI_iid_sim[,2], na.rm = TRUE)   \n\n[1] 0.8537415\n\nmean(PI_ar1_sim[,2], na.rm = TRUE)   \n\n[1] 0.8991798"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#interval-performance-on-stock-price-data",
    "href": "slides/04-forecast-evaluation.html#interval-performance-on-stock-price-data",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Interval performance on stock price data",
    "text": "Interval performance on stock price data\n\nSize of 90% intervals:\n\nmean(PI_iid_real[,1], na.rm = TRUE)   \n\n[1] 41.01969\n\nmean(PI_ar1_real[,1], na.rm = TRUE)   \n\n[1] 1.03624\n\n\n\n\nCoverage of 90% intervals:\n\nmean(PI_iid_real[,2], na.rm = TRUE)   \n\n[1] 0.2190211\n\nmean(PI_ar1_real[,2], na.rm = TRUE)   \n\n[1] 0.7071395"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#interval-score",
    "href": "slides/04-forecast-evaluation.html#interval-score",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Interval score",
    "text": "Interval score\nAverage over time for an holistic metric of interval performance:\n\\[\n\\mathrm{IS}_\\alpha(l,u; y)\n=\n(u - l)\n+\n\\frac{2}{\\alpha}\\,(l - y)\\,\\mathbf{1}(y &lt; l)\n+\n\\frac{2}{\\alpha}\\,(y - u)\\,\\mathbf{1}(y &gt; u).\n\\]\nSynthesizes both size and coverage, but in practice, if you want to understand why the score was good or bad, you have to crack it open and look at the size and coverage components separately anyway."
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#any-ideas-2",
    "href": "slides/04-forecast-evaluation.html#any-ideas-2",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Any ideas?",
    "text": "Any ideas?"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#recap-probability-integral-transform-pit",
    "href": "slides/04-forecast-evaluation.html#recap-probability-integral-transform-pit",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Recap: probability integral transform (PIT)",
    "text": "Recap: probability integral transform (PIT)"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#recap-probability-integral-transform-pit-1",
    "href": "slides/04-forecast-evaluation.html#recap-probability-integral-transform-pit-1",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Recap: probability integral transform (PIT)",
    "text": "Recap: probability integral transform (PIT)\nLet \\(Y\\sim F\\) be continuous. If you define a new random variable \\(U=F(Y)\\) by plugging \\(Y\\) into its own cdf, then you get\n\n\\[\nU\\sim \\text{Unif}(0\\com 1).\n\\]\n\n\nFix \\(u\\in(0\\com 1)\\). Then\n\n\n\\[\nP(U\\leq u)=P(F(Y)\\leq u)=P(Y\\leq F^{-1}(u))=F(F^{-1}(u))=u.\n\\]\n\n\nThat’s the cdf of Unif(0, 1)."
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#recap-probability-integral-transform-pit-2",
    "href": "slides/04-forecast-evaluation.html#recap-probability-integral-transform-pit-2",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Recap: probability integral transform (PIT)",
    "text": "Recap: probability integral transform (PIT)\n\nx &lt;- rnorm(10000)\nu &lt;- pnorm(x)\nhist(u, breaks = \"Scott\", freq = FALSE)\nabline(h = 1, col = \"red\")"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#whats-pit-got-to-do-with-it",
    "href": "slides/04-forecast-evaluation.html#whats-pit-got-to-do-with-it",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "What’s PIT got to do with it?",
    "text": "What’s PIT got to do with it?\nLet \\(G_t\\) be the “true” cdf that nature is drawing from to produce \\(y_t\\). By the probability integral transform, we know that:\n\n\\[\nG_1(y_1)\\com G_2(y_2)\\com ...\\com G_t(y_t)\\com ...\\sim\\text{Unif}(0\\com 1).\n\\]\n\n\nIt would be ideal if \\(\\hat{F}_{t|t-1}=G_t\\). We’re probably not so lucky, but if we’re close, then we should see:\n\n\n\\[\n\\hat{F}_{1|0}(y_1)\\com \\hat{F}_{2|1}(y_2)\\com ...\\com \\hat{F}_{t|t-1}(y_t)\\com ...\\sim\\text{Unif}(0\\com 1).\n\\]\n\n\nLet’s check!"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#iid-normal-method-on-simulated-data",
    "href": "slides/04-forecast-evaluation.html#iid-normal-method-on-simulated-data",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "iid normal method on simulated data",
    "text": "iid normal method on simulated data"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#bayesian-ar1-on-simulated-data",
    "href": "slides/04-forecast-evaluation.html#bayesian-ar1-on-simulated-data",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Bayesian AR(1) on simulated data",
    "text": "Bayesian AR(1) on simulated data"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#iid-normal-method-on-stock-price-data",
    "href": "slides/04-forecast-evaluation.html#iid-normal-method-on-stock-price-data",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "iid normal method on stock price data",
    "text": "iid normal method on stock price data\n\n\n\n\n\n\n\n\nWay too many surprises in the right tail (recall waterfall)."
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#bayesian-ar1-on-stock-price-data",
    "href": "slides/04-forecast-evaluation.html#bayesian-ar1-on-stock-price-data",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Bayesian AR(1) on stock price data",
    "text": "Bayesian AR(1) on stock price data"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#diagnosing-underover-dispersion",
    "href": "slides/04-forecast-evaluation.html#diagnosing-underover-dispersion",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Diagnosing under/over-dispersion",
    "text": "Diagnosing under/over-dispersion"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#what-do-the-pit-values-tell-you",
    "href": "slides/04-forecast-evaluation.html#what-do-the-pit-values-tell-you",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "What do the PIT values tell you?",
    "text": "What do the PIT values tell you?"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#what-do-the-pit-values-tell-you-1",
    "href": "slides/04-forecast-evaluation.html#what-do-the-pit-values-tell-you-1",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "What do the PIT values tell you?",
    "text": "What do the PIT values tell you?\n\nIf \\(y\\) tends to surprise in the left tail, we’ll get too many PITs near 0;\nIf \\(y\\) tends to surprise in the right tail, we’ll get too many PITs near 1;\nIf \\(y\\) tends to surprise in the middle, we’ll get too many PITs near 0.5;\nIf the forecast distributions tend to be overdispersed (too much mass in the tails), the histogram is hump-shaped;\nIf the forecast distributions tend to be underdispersed (not enough mass in the tails), the histogram is u-shaped."
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#summary-calibration",
    "href": "slides/04-forecast-evaluation.html#summary-calibration",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Summary: calibration",
    "text": "Summary: calibration\nIf your sequence of forecast distributions is well-calibrated, then the PITs should be approximately uniformly distributed:\n\\[\n\\hat{F}_{1|0}(y_1)\\com \\hat{F}_{2|1}(y_2)\\com ...\\com \\hat{F}_{t|t-1}(y_t)\\com ...\\sim\\text{Unif}(0\\com 1).\n\\]\n\nCheck it with a histogram, QQ-plot, goodness-of-fit test…\n\n\nDeviations from uniformity provide useful diagnostic information.\n\n\n\n\n\n\n\n\nThis is necessary but not sufficient!\n\n\nCalibration alone is not enough to distinguish good/better/best forecasts."
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#maximize-sharpness-subject-to-calibration",
    "href": "slides/04-forecast-evaluation.html#maximize-sharpness-subject-to-calibration",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Maximize sharpness subject to calibration",
    "text": "Maximize sharpness subject to calibration\n\nYou want a forecasting method to be calibrated;\nIf you have many methods to choose from, all of which appear calibrated, select the one that is the sharpest;\n\nSharpness refers to how concentrated the forecast distributions are. Among calibrated distributions, you want the one that is sharpest, most decisive, most concentrated;\nSharpness can be measured by your preferred measure of spread: variance, IQR, etc."
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#sharpness-for-the-simulated-data",
    "href": "slides/04-forecast-evaluation.html#sharpness-for-the-simulated-data",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Sharpness for the simulated data",
    "text": "Sharpness for the simulated data\nCompare the scale parameters of the predictive distributions:"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#sharpness-for-the-stock-price-data",
    "href": "slides/04-forecast-evaluation.html#sharpness-for-the-stock-price-data",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Sharpness for the stock price data",
    "text": "Sharpness for the stock price data"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#log-predictive-score",
    "href": "slides/04-forecast-evaluation.html#log-predictive-score",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Log predictive score",
    "text": "Log predictive score\nEvaluates if the forecast distribution placed high mass/density on the region where \\(y_t\\) actually showed up:\n\\[\n\\overline{\\text{LPS}} = \\frac{1}{T} \\sum_{t=1}^{T} \\ln \\hat{f}_{t|t-1}(y_t).\n\\]\n\nBigger is better;\nRewards both calibration and sharpness;\n\nProper scoring rule: encourages honest probabilistic predictions;\nLocal measure of quality. We will see global measures like the continuous ranked probability score (CRPS) later."
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#lps-rewards-both-calibration-and-sharpness",
    "href": "slides/04-forecast-evaluation.html#lps-rewards-both-calibration-and-sharpness",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "LPS rewards both calibration and sharpness",
    "text": "LPS rewards both calibration and sharpness"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#for-our-examples",
    "href": "slides/04-forecast-evaluation.html#for-our-examples",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "For our examples",
    "text": "For our examples\n\nSimulated data:\n\naverage_log_score(y, pred_params_iid_sim)\n\n[1] -3.965963\n\naverage_log_score(y, pred_params_ar1_sim)\n\n[1] -2.132817\n\n\n\n\nReal data:\n\naverage_log_score(stonks, pred_params_iid_real)\n\n[1] -5.609798\n\naverage_log_score(stonks, pred_params_ar1_real)\n\n[1] -1.79456"
  },
  {
    "objectID": "slides/04-forecast-evaluation.html#authors-and-papers-to-know",
    "href": "slides/04-forecast-evaluation.html#authors-and-papers-to-know",
    "title": "How do you evaluate probabilistic predictions?",
    "section": "Authors and papers to know",
    "text": "Authors and papers to know\n\nGneiting & Raftery (2007): “Strictly Proper Scoring Rules, Prediction, and Estimation,” JASA;\n\nHard to read, but packed with useful info;\n\n\nGneiting, Balabdaoui, & Raftery (2007): “Probabilistic Forecasts, Calibration and Sharpness,” JRSSB;\n\n“We propose a diagnostic approach to the evaluation of predictive performance that is based on the paradigm of maximizing the sharpness of the predictive distributions subject to calibration.” Bada bing."
  },
  {
    "objectID": "slides/05-ar-p-structure.html#time-series",
    "href": "slides/05-ar-p-structure.html#time-series",
    "title": "What is the joint distribution of an AR(p)",
    "section": "Time series",
    "text": "Time series\n\nA time series is a set of measurements collected over time;\nWe model these data as a sequence of dependent random variables:\n\n\\[\n\\By_{0:T} = \\{\\By_0,\\,\\By_1,\\,\\By_2,\\,...,\\,\\By_T\\}.\n\\]\n\nA time series model is “just” their joint probability distribution:\n\n\\[\np(\\By_{0:T}) = p(\\By_0)\\prod_{t=1}^Tp(\\By_t\\,|\\,\\By_{0:t-1}).\n\\]"
  },
  {
    "objectID": "slides/05-ar-p-structure.html#the-simplest-non-trivial-time-series-model",
    "href": "slides/05-ar-p-structure.html#the-simplest-non-trivial-time-series-model",
    "title": "What is the joint distribution of an AR(p)",
    "section": "The simplest non-trivial time series model",
    "text": "The simplest non-trivial time series model\nThe autoregression of order 1, or AR(1):\n\n\\[\n\\begin{aligned}\ny_t\n&=\n\\beta_0\n+\n\\beta_1\ny_{t-1}\n+\n\\varepsilon_t,\n\\quad\n\\varepsilon_t\\iid\\text{N}(0\\com\\sigma^2)\n\\\\\ny_0\n&\\sim\n\\text{N}(\\mu_0\\com \\initvar),\n\\end{aligned}\n\\]\n\n\nThat’s the recursive form. It implies a joint distribution written marginal-conditional style:\n\n\n\\[\n\\begin{aligned}\np(y_{0:T})\n&=\np(y_0)\n\\prod_{t=1}^T\np(y_t\\given y_{0:t-1})\n\\\\\n&=\np(y_0)\n\\prod_{t=1}^T\np(y_t\\given y_{t-1}).\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/05-ar-p-structure.html#all-of-time-series-analysis",
    "href": "slides/05-ar-p-structure.html#all-of-time-series-analysis",
    "title": "What is the joint distribution of an AR(p)",
    "section": "“All of Time Series Analysis”",
    "text": "“All of Time Series Analysis”\nWhat we did for the AR(1):\n\nStructure of the joint distribution: distribution family, mean, covariance, marginals, conditionals;\nStationarity;\nMethod-of-moments estimation (Yule-Walker);\nLikelihood-based inferences;\n\nClassical: MLE/OLS;\nBayesian: conjugate normal-inverse-gamma;\nMake these recursive using Sherman-Morrison;\n\nProbabilistic prediction: points, intervals, and densities speaking to many sources of uncertainty;\n\ngeneric Monte Carlo approach;\nClassical: tough, need bootstrap;\nBayesian: natural, just use posterior predictive;\n\nForecast evaluation:\n\nPoint: loss functions, MSE, MAE, etc;\nInterval: size, coverage, interval score;\nDensity: PIT, calibration, sharpness, log predictive score."
  },
  {
    "objectID": "slides/05-ar-p-structure.html#what-we-havent-talked-about-yet",
    "href": "slides/05-ar-p-structure.html#what-we-havent-talked-about-yet",
    "title": "What is the joint distribution of an AR(p)",
    "section": "What we haven’t talked about yet",
    "text": "What we haven’t talked about yet\nWith only one model on the table, we couldn’t dig into…\n\nmodel selection;\nforecast combination.\n\n\nWe will add those themes in the coming weeks, and then our table is basically set for the rest of the semester."
  },
  {
    "objectID": "slides/05-ar-p-structure.html#arp",
    "href": "slides/05-ar-p-structure.html#arp",
    "title": "What is the joint distribution of an AR(p)",
    "section": "AR(\\(p\\))",
    "text": "AR(\\(p\\))\nThe autoregression of order \\(p\\), or AR(\\(p\\)):\n\n\\[\n\\begin{aligned}\ny_t\n&=\n\\beta_0\n+\n\\beta_1\ny_{t-1}\n+\n\\beta_2\ny_{t-2}\n+\n\\cdots\n+\n\\beta_p\ny_{t-p}\n+\n\\varepsilon_t,\n\\quad\n\\varepsilon_t\\iid\\text{N}(0\\com\\sigma^2)\n\\\\\n\\begin{bmatrix}\ny_0\\\\\ny_{-1}\\\\\n\\vdots\\\\\ny_{1-p}\n\\end{bmatrix}\n&\\sim\n\\text{N}_p(\\Bmu_0\\com \\BV_0),\n\\end{aligned}\n\\]\n\n\nThat’s the recursive form. It implies a joint distribution written marginal-conditional style:\n\n\n\\[\n\\begin{aligned}\np(y_{(1-p):T})\n&=\np(y_{(1-p):0})\n\\prod_{t=1}^T\np(y_t\\given y_{(1-p):t-1})\n\\\\\n&=\np(y_{(1-p):0})\n\\prod_{t=1}^T\np(y_t\\given y_{t-1}\\com y_{t-2}\\com ...\\com y_{t-p}).\n\\end{aligned}\n\\]\n\n\nThere are several ways to rewrite this, each teaching you different things about the model."
  },
  {
    "objectID": "slides/05-ar-p-structure.html#linear-transformation-form",
    "href": "slides/05-ar-p-structure.html#linear-transformation-form",
    "title": "What is the joint distribution of an AR(p)",
    "section": "Linear transformation form",
    "text": "Linear transformation form\nConsider:\n\\[\n\\begin{aligned}\n\\By\n&=\n\\begin{bmatrix}\ny_1 & y_2 & \\cdots & y_T\n\\end{bmatrix}^\\tr\n\\\\\n\\Be\n&=\n\\begin{bmatrix}\ny_{1-p} & \\cdots & y_{-1} & y_0 & \\varepsilon_1 & \\varepsilon_2 & \\cdots & \\varepsilon_{T-1} & \\varepsilon_T\n\\end{bmatrix}^\\tr\n.\n\\end{aligned}\n\\]\n\nYou can show that\n\\[\n\\By\n=\n\\Bc+\\BA\\Be\n,\n\\quad\n\\Be\\sim\n\\N_{p+T}\n\\left(\n\\Bm=\n\\begin{bmatrix}\n\\Bmu_0\\\\\\Bzero\n\\end{bmatrix}\n\\com\n\\BS=\n\\begin{bmatrix}\n\\BV_0& \\Bzero\\\\\n\\Bzero & \\sigma^2\\BI_T\n\\end{bmatrix}\n\\right)\n.\n\\]\n\n\nBy affine transformation,\n\\[\n\\By\n\\sim\n\\text{N}_{T}\n\\left(\n\\Bmu=\\Bc+\\BA\\Bm\n\\com\n\\BSigma\n=\n\\BA\\BS\\BA^\\tr\n\\right)\n.\n\\]\nSo, the AR(p) is just a way of writing down a big, weird multivariate normal."
  },
  {
    "objectID": "slides/05-ar-p-structure.html#multiple-regression-form",
    "href": "slides/05-ar-p-structure.html#multiple-regression-form",
    "title": "What is the joint distribution of an AR(p)",
    "section": "Multiple regression form",
    "text": "Multiple regression form\n\nThe conditionals \\(p(y_t\\given y_{t-1}\\com y_{t-2}\\com ...\\com y_{t-p})\\) have the form:\n\n\n\\[\n\\begin{aligned}\ny_t\n&=\n\\Bx_t^\\tr\\Bbeta+\\varepsilon_t,&&\\varepsilon_t\\iid\\N(0\\com\\sigma^2)\\\\\n\\Bx_t\n&=\n\\begin{bmatrix}\n1 & y_{t-1} & y_{t-2} & \\cdots & y_{t-p}\n\\end{bmatrix}\n.\n\\end{aligned}\n\\]\n\n\n\nOnce the data are observed and treated as fixed, this is “just” multiple linear regression, and the mechanics of (conditional) likelihood-based inference proceed identically to the iid case;\n\n(but recall, it’s not “just regression” when we do prediction!)\n\n\n\n\n\nAll the AR(1) formulas are the same, only the design matrix \\(\\BX_T\\) has more columns."
  },
  {
    "objectID": "slides/05-ar-p-structure.html#state-space-form",
    "href": "slides/05-ar-p-structure.html#state-space-form",
    "title": "What is the joint distribution of an AR(p)",
    "section": "State-space form",
    "text": "State-space form\nStack up some stuff:\n\n\\[\n\\begin{aligned}\n    \\Bs_t&=\\begin{bmatrix}y_t & y_{t-1} & \\cdots & y_{t-(p-1)} \\end{bmatrix}^\\tr\\\\\n    \\Bf&=\\begin{bmatrix}1 & 0 & \\cdots & 0\\end{bmatrix}^\\tr\\\\\n    \\BG&=\\begin{bmatrix}\n    \\beta_1 & \\beta_2 & \\cdots & \\beta_{p-1} & \\beta_p \\\\\n    1       & 0       & \\cdots & 0           & 0 \\\\\n    0       & 1       & \\cdots & 0           & 0 \\\\\n    \\vdots  & \\vdots  & \\ddots & \\vdots      & \\vdots \\\\\n    0       & 0       & \\cdots & 1           & 0 \\\\\n    \\end{bmatrix}\\\\\n    \\Beta_t&=\\begin{bmatrix}\\varepsilon_t & 0 & \\cdots & 0\\end{bmatrix}^\\tr\n    \\\\\n    \\Bb\n    &=\n    \\begin{bmatrix}\\beta_0 & 0 & \\cdots & 0\\end{bmatrix}^\\tr\n\\end{aligned}\n\\]\n\n\nState-space form:\n\\[\n\\begin{aligned}\n    y_t&=\\Bf^\\tr\\Bs_t && \\text{(measurement/observation eq)}\\\\\n    \\Bs_t&=\\BG\\Bs_{t-1}+\\Bb+\\Beta_t && \\text{(state transition/law of motion)}\\\\\n    \\Bs_0&\\sim\\text{N}_p(\\Bmu_0\\com\\BV_0)\\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/05-ar-p-structure.html#the-companion-form-matrix",
    "href": "slides/05-ar-p-structure.html#the-companion-form-matrix",
    "title": "What is the joint distribution of an AR(p)",
    "section": "The companion form matrix",
    "text": "The companion form matrix\nConsider\n\\[\n\\BG=\\begin{bmatrix}\n    \\beta_1 & \\beta_2 & \\cdots & \\beta_{p-1} & \\beta_p \\\\\n    1       & 0       & \\cdots & 0           & 0 \\\\\n    0       & 1       & \\cdots & 0           & 0 \\\\\n    \\vdots  & \\vdots  & \\ddots & \\vdots      & \\vdots \\\\\n    0       & 0       & \\cdots & 1           & 0 \\\\\n    \\end{bmatrix}\n    .\n\\]\n\nBelieve it or not, we will learn something by taking its eigendecomposition:\n\n\n\\[\n\\begin{aligned}\n\\BG\n&=\n\\BE\\BLambda\\BE^{-1}\n\\\\\n\\BLambda&=\\text{diag}(\\lambda_1\\com\\lambda_2\\com...\\com\\lambda_p)\\\\\n        \\BE&=\\begin{bmatrix}\\Be_1 & \\Be_2 & \\cdots & \\Be_p\\end{bmatrix}\\\\\n        \\Be_\\ell&=\\begin{bmatrix}\\lambda_{\\ell}^{p-1} & \\lambda_{\\ell}^{p-2} & \\cdots & 1 \\end{bmatrix}^\\tr,&& \\ell=1\\com 2\\com...\\com p.\n\\end{aligned}\n\\]\n\n\n\\(\\BG\\) is not SPD, so the eigenvalues might be complex numbers. Yuck!"
  },
  {
    "objectID": "slides/05-ar-p-structure.html#the-mean",
    "href": "slides/05-ar-p-structure.html#the-mean",
    "title": "What is the joint distribution of an AR(p)",
    "section": "The mean",
    "text": "The mean\nWe have\n\\[\n\\begin{aligned}\n    \\Bs_t&=\\BG\\Bs_{t-1}+\\Bb+\\Beta_t,\\quad \\Beta_t\\sim\\N_p(\\Bzero\\com\\BW=\\text{diag}(\\sigma^2, 0, ..., 0))\\\\\n    \\Bs_0&\\sim\\text{N}_p(\\Bmu_0\\com\\BV_0).\n\\end{aligned}\n\\]\n\nBy linearity \\(\\Bmu_t=E(\\Bs_t)=\\BG \\Bmu_{t-1}+\\Bb\\), so\n\\[\n\\begin{aligned}\n\\boldsymbol{\\mu}_1 &= \\mathbf{G} \\boldsymbol{\\mu}_0 + \\mathbf{b}, \\\\\n\\boldsymbol{\\mu}_2 &= \\mathbf{G} \\boldsymbol{\\mu}_1 + \\mathbf{b}\n             = \\mathbf{G}^2 \\boldsymbol{\\mu}_0 + \\mathbf{G} \\mathbf{b} + \\mathbf{b}, \\\\\n\\boldsymbol{\\mu}_3 &= \\mathbf{G} \\boldsymbol{\\mu}_2 + \\mathbf{b}\n             = \\mathbf{G}^3 \\boldsymbol{\\mu}_0 + \\mathbf{G}^2 \\mathbf{b} + \\mathbf{G} \\mathbf{b} + \\mathbf{b}, \\\\\n&\\vdots \\\\\n\\boldsymbol{\\mu}_t &= \\mathbf{G}^t \\boldsymbol{\\mu}_0 + \\sum_{k=0}^{t-1} \\mathbf{G}^k \\mathbf{b}.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/05-ar-p-structure.html#the-mean-1",
    "href": "slides/05-ar-p-structure.html#the-mean-1",
    "title": "What is the joint distribution of an AR(p)",
    "section": "The mean",
    "text": "The mean\n\nWe have\n\\[\n\\boldsymbol{\\mu}_t = \\mathbf{G}^t \\boldsymbol{\\mu}_0 + \\sum_{k=0}^{t-1} \\mathbf{G}^k \\mathbf{b},\n\\]\n\n\nand we have\n\\[\n\\BG\n=\n\\BE\\BLambda\\BE^{-1}.\n\\]\n\n\nSo\n\\[\n\\boldsymbol{\\mu}_t = \\BE\\BLambda^t\\BE^{-1} \\boldsymbol{\\mu}_0 + \\sum_{k=0}^{t-1} \\BE\\BLambda^k\\BE^{-1} \\mathbf{b}.\n\\]\n\n\nLong-run behavior is governed by the eigenvalues. Ugh!"
  },
  {
    "objectID": "slides/05-ar-p-structure.html#stationary-region",
    "href": "slides/05-ar-p-structure.html#stationary-region",
    "title": "What is the joint distribution of an AR(p)",
    "section": "Stationary region",
    "text": "Stationary region\nThe subset \\(\\ccal_{p,1}\\subseteq\\RR^p\\) where \\(\\beta_{1:p}\\) must live to ensure the process is stationary:\n\n\nFor \\(p=1\\), \\((-1\\com 1)\\);\n\n\n\n\nFor \\(p=2\\):\n\n\n\n\n\n\n\n\n\n\n\nFor \\(p&gt;2\\): harder to characterize."
  },
  {
    "objectID": "slides/05-ar-p-structure.html#if-the-eigenvalues-have-lambda_ell1",
    "href": "slides/05-ar-p-structure.html#if-the-eigenvalues-have-lambda_ell1",
    "title": "What is the joint distribution of an AR(p)",
    "section": "If the eigenvalues have \\(|\\lambda_\\ell|<1\\)",
    "text": "If the eigenvalues have \\(|\\lambda_\\ell|&lt;1\\)\nThen the time-invariant mean solves\n\\[\n\\Bmu\n=\n\\BG\\Bmu+\\Bb.\n\\]\n\nRearranging,\n\\[\n\\Bmu = (\\BI_p-\\BG)^{-1}\\Bb.\n\\]\n\n\nThe first component of this is\n\\[\nE(y_t)=\\frac{\\beta_0}{1-\\sum_{\\ell=1}^p\\beta_\\ell}\\quad \\forall t.\n\\]"
  },
  {
    "objectID": "slides/05-ar-p-structure.html#the-covariance",
    "href": "slides/05-ar-p-structure.html#the-covariance",
    "title": "What is the joint distribution of an AR(p)",
    "section": "The covariance",
    "text": "The covariance\n\nWe have\n\\[\n\\begin{aligned}\n    \\Bs_t&=\\BG\\Bs_{t-1}+\\Bb+\\Beta_t,\\quad \\Beta_t\\sim\\N_p(\\Bzero\\com\\BW=\\text{diag}(\\sigma^2, 0, ..., 0))\\\\\n    \\Bs_0&\\sim\\text{N}_p(\\Bmu_0\\com\\BV_0).\n\\end{aligned}\n\\]\n\n\nSo\n\\[\n\\BV_t=\\text{cov}(\\Bs_t)=\\BG\\BV_{t-1}\\BG^\\tr+\\BW.\n\\]\n\n\nDo the recursive substitution, and\n\\[\n\\BV_t=\\BG^t\\BV_0(\\BG^\\tr)^t+\\sum\\limits_{k=0}^{t-1}\\BG^k\\BW(\\BG^\\tr)^k.\n\\]\nYou can show\n\\[\n\\text{cov}(\\Bs_t\\com \\Bs_s)=\\BG^{|t-s|}\\BV_{\\min\\{s,t\\}}.\n\\]"
  },
  {
    "objectID": "slides/05-ar-p-structure.html#if-the-eigenvalues-have-lambda_ell1-1",
    "href": "slides/05-ar-p-structure.html#if-the-eigenvalues-have-lambda_ell1-1",
    "title": "What is the joint distribution of an AR(p)",
    "section": "If the eigenvalues have \\(|\\lambda_\\ell|<1\\)",
    "text": "If the eigenvalues have \\(|\\lambda_\\ell|&lt;1\\)\nThe time-invariant covariance satisfies\n\\[\n\\BV=\\BG\\BV\\BG^\\tr+\\BW.\n\\]\n\nSome linear algebra tricks give:\n\\[\n\\text{vec}(\\BV)=(\\BI_{p^2}-\\BG\\otimes\\BG)^{-1}\\text{vec}(\\BW).\n\\]\n\n\n(you don’t have to worry about it.)"
  },
  {
    "objectID": "slides/05-ar-p-structure.html#centered-form",
    "href": "slides/05-ar-p-structure.html#centered-form",
    "title": "What is the joint distribution of an AR(p)",
    "section": "Centered Form",
    "text": "Centered Form\nDefine \\(x_t := y_t - \\mu\\).\nThen the AR(p) in centered form is\n\\[\nx_t = \\sum_{l=1}^p \\beta_l \\, x_{t-l} + \\varepsilon_t.\n\\]"
  },
  {
    "objectID": "slides/05-ar-p-structure.html#autocovariances",
    "href": "slides/05-ar-p-structure.html#autocovariances",
    "title": "What is the joint distribution of an AR(p)",
    "section": "Autocovariances",
    "text": "Autocovariances\nDefine the autocovariance \\(\\gamma(h) = \\operatorname{Cov}(y_t, y_{t-h}) = E[x_t x_{t-h}]\\).\nMultiply the centered equation by \\(x_{t-k}\\) and take expectations:\n\\[\n\\gamma(k) = \\sum_{l=1}^p \\beta_l \\, \\gamma(k-l) + E[\\varepsilon_t x_{t-k}].\n\\]\nFor \\(k \\ge 1\\), \\(\\varepsilon_t\\) is uncorrelated with \\(x_{t-k}\\), so\n\\[\n\\boxed{\\gamma(k) = \\sum_{l=1}^p \\beta_l \\, \\gamma(k-l), \\quad k \\ge 1.}\n\\]"
  },
  {
    "objectID": "slides/05-ar-p-structure.html#variance-equation",
    "href": "slides/05-ar-p-structure.html#variance-equation",
    "title": "What is the joint distribution of an AR(p)",
    "section": "Variance Equation",
    "text": "Variance Equation\nFor \\(k=0\\), note that \\(E[\\varepsilon_t x_t] = \\sigma^2\\), giving\n\\[\n\\gamma(0) = \\sum_{l=1}^p \\beta_l \\, \\gamma(l) + \\sigma^2.\n\\]"
  },
  {
    "objectID": "slides/05-ar-p-structure.html#matrix-form-of-yule-walker-equations",
    "href": "slides/05-ar-p-structure.html#matrix-form-of-yule-walker-equations",
    "title": "What is the joint distribution of an AR(p)",
    "section": "Matrix Form of Yule-Walker Equations",
    "text": "Matrix Form of Yule-Walker Equations\nLet\n\\[\n\\boldsymbol{\\gamma}_p = \\begin{pmatrix} \\gamma(1) \\\\ \\gamma(2) \\\\ \\vdots \\\\ \\gamma(p) \\end{pmatrix}, \\quad\n\\boldsymbol{\\beta} = \\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_p \\end{pmatrix},\n\\]\nand\n\\[\n\\mathbf{\\Gamma}_p =\n\\begin{pmatrix}\n\\gamma(0) & \\gamma(1) & \\cdots & \\gamma(p-1) \\\\\n\\gamma(1) & \\gamma(0) & \\cdots & \\gamma(p-2) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\gamma(p-1) & \\gamma(p-2) & \\cdots & \\gamma(0)\n\\end{pmatrix}.\n\\]\nThen the Yule-Walker equations are\n\\[\n\\boxed{\\boldsymbol{\\gamma}_p = \\mathbf{\\Gamma}_p \\, \\boldsymbol{\\beta}}\n\\quad\\Longrightarrow\\quad\n\\boxed{\\boldsymbol{\\beta} = \\mathbf{\\Gamma}_p^{-1} \\, \\boldsymbol{\\gamma}_p}.\n\\]"
  },
  {
    "objectID": "slides/05-ar-p-structure.html#yule-walker-estimating-equations",
    "href": "slides/05-ar-p-structure.html#yule-walker-estimating-equations",
    "title": "What is the joint distribution of an AR(p)",
    "section": "Yule-Walker estimating equations",
    "text": "Yule-Walker estimating equations\n\\[\n\\begin{aligned}\n\\text{AR coefficients:} \\quad &\n\\begin{pmatrix} \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_p \\end{pmatrix}\n= \\mathbf{\\Gamma}_p^{-1}\n\\begin{pmatrix} \\gamma(1) \\\\ \\gamma(2) \\\\ \\vdots \\\\ \\gamma(p) \\end{pmatrix}, \\\\[1em]\n\\text{Intercept:} \\quad &\n\\beta_0 = \\mu \\left( 1 - \\sum_{l=1}^p \\beta_l \\right), \\\\[0.5em]\n\\text{Noise variance:} \\quad &\n\\sigma^2 = \\gamma(0) - \\sum_{l=1}^p \\beta_l \\, \\gamma(l).\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/05-ar-p-structure.html#yule-walker-estimators",
    "href": "slides/05-ar-p-structure.html#yule-walker-estimators",
    "title": "What is the joint distribution of an AR(p)",
    "section": "Yule-Walker estimators",
    "text": "Yule-Walker estimators\n\\[\n\\begin{aligned}\n\\text{AR coefficients:} \\quad &\n\\begin{pmatrix} \\hat{\\beta}_1 \\\\ \\hat{\\beta}_2 \\\\ \\vdots \\\\ \\hat{\\beta}_p \\end{pmatrix}\n= \\hat{\\mathbf{\\Gamma}}_p^{-1}\n\\begin{pmatrix} \\hat{\\gamma}(1) \\\\ \\hat{\\gamma}(2) \\\\ \\vdots \\\\ \\hat{\\gamma}(p) \\end{pmatrix}, \\\\[1em]\n\\text{Intercept:} \\quad &\n\\hat{\\beta}_0 = \\hat{\\mu} \\left( 1 - \\sum_{l=1}^p \\hat{\\beta}_l \\right), \\\\[0.5em]\n\\text{Noise variance:} \\quad &\n\\hat{\\sigma}^2 = \\hat{\\gamma}(0) - \\sum_{l=1}^p \\hat{\\beta}_l \\, \\hat{\\gamma}(l),\n\\end{aligned}\n\\]\nwhere \\(\\hat{\\mu} = \\frac{1}{T} \\sum_{t=1}^T y_t\\), \\(\\hat{\\gamma}(k) = \\frac{1}{T} \\sum_{t=k+1}^{T} (y_t - \\hat{\\mu})(y_{t-k} - \\hat{\\mu})\\)."
  },
  {
    "objectID": "slides/05-ar-p-structure.html#asymptotics",
    "href": "slides/05-ar-p-structure.html#asymptotics",
    "title": "What is the joint distribution of an AR(p)",
    "section": "Asymptotics",
    "text": "Asymptotics\n\n\n\n\n\n\nShumway and Stoffer (2025) Property 3.7\n\n\nLet \\(\\hat{\\boldsymbol{\\beta}} = (\\hat{\\beta}_1, \\dots, \\hat{\\beta}_p)^\\top\\) be the Yule-Walker estimator of the AR coefficients.\nUnder standard stationarity and moment conditions:\n\\[\n\\sqrt{T} \\, (\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta}) \\;\\;\\;\\overset{d}{\\longrightarrow}\\;\\;\\; \\N_p\\bigl(\\mathbf{0}, \\, \\sigma^2 \\, \\mathbf{\\Gamma}_p^{-1}\\bigr).\n\\]\nwhere \\(\\mathbf{\\Gamma}_p\\) is the \\(p \\times p\\) autocovariance matrix with \\((i,j)\\) entry \\(\\gamma(|i-j|)\\)."
  },
  {
    "objectID": "slides/05-ar-p-structure.html#lets-improve-the-notation",
    "href": "slides/05-ar-p-structure.html#lets-improve-the-notation",
    "title": "What is the joint distribution of an AR(p)",
    "section": "Let’s improve the notation",
    "text": "Let’s improve the notation\nThe autoregression of order p, or AR(p):\n\n\\[\n\\begin{aligned}\ny_t\n&=\n\\beta_0\n+\n\\sum\\limits_{\\ell=1}^p\n\\beta_\\ell\ny_{t-\\ell}\n+\n\\varepsilon_t,\n\\quad\n\\varepsilon_t\\iid\\text{N}(0\\com\\sigma^2).\n\\end{aligned}\n\\]\n\n\nThis implies a joint distribution governed by a finite set of static parameters \\(\\Btheta = \\begin{bmatrix}\\beta_0&\\beta_1&\\cdots &\\beta_p&\\sigma^2\\end{bmatrix}^\\tr\\):\n\n\n\\[\n\\begin{aligned}\np(y_{1:T}\\given y_{1-p:0}\\com\\Btheta)\n&=\n\\prod_{t=1}^T\np(y_t\\given y_{t-p:t-1}\\com\\Btheta).\n\\end{aligned}\n\\]\n\n\nViewed as a function of \\(\\Btheta\\), that’s a (conditional) likelihood!\n\n\nMaximize it, or combine with a prior."
  },
  {
    "objectID": "slides/05-ar-p-structure.html#maximum-likelihood-estimation",
    "href": "slides/05-ar-p-structure.html#maximum-likelihood-estimation",
    "title": "What is the joint distribution of an AR(p)",
    "section": "Maximum likelihood estimation",
    "text": "Maximum likelihood estimation\nTreating the observed data \\(y_{0:T}\\) as fixed, we want:\n\\[\n\\hat{\\Btheta}_T=\\argmax{\\Btheta}\\,p(y_{1:T}\\given lags\\com \\Btheta).\n\\]\nTo do this, it helps to take the view that the AR(1) is “just” a multiple linear regression where \\(\\Bx_t\\) encodes the lagged values of \\(y_t\\)."
  },
  {
    "objectID": "slides/05-ar-p-structure.html#maximum-likelihood-estimation-1",
    "href": "slides/05-ar-p-structure.html#maximum-likelihood-estimation-1",
    "title": "What is the joint distribution of an AR(p)",
    "section": "Maximum likelihood estimation",
    "text": "Maximum likelihood estimation\nSince \\(y_t\\given lags\\com\\Btheta\\sim\\N\\left(\\beta_0+\\sum\\limits_{l=1}^p\\beta_ly_{t-l}\\com\\sigma^2\\right)\\), we have\n\n\\[\n\\begin{aligned}\np(y_{1:T}\\given  y_0\\com \\Btheta)\n&=\n\\prod_{t=1}^T\np(y_t\\given lags\\com\\Btheta)\n\\\\\n&=\n\\prod_{t=1}^T\n\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{1}{2}\\frac{(y_t-\\beta_0-\\sum\\limits_{l=1}^p\\beta_ly_{t-l})^2}{\\sigma^2}\\right)\n\\\\\n&=\n(2\\pi\\sigma^2)^{-T/2}\n\\exp\\left(-\\frac{1}{2\\sigma^2}\\sum\\limits_{t=1}^T(y_t-\\beta_0-\\sum\\limits_{l=1}^p\\beta_ly_{t-l})^2\\right)\n.\n\\end{aligned}\n\\]\n\n\nTo compute the MLE, we just treat the data as fixed.\n\n\nWhere do you think this is headed?"
  },
  {
    "objectID": "slides/05-ar-p-structure.html#stack-em-up",
    "href": "slides/05-ar-p-structure.html#stack-em-up",
    "title": "What is the joint distribution of an AR(p)",
    "section": "Stack ’em up",
    "text": "Stack ’em up\nDefine some things:\n\n\\[\n\\begin{aligned}\n\\By_T\n&=\n\\begin{bmatrix}y_1&y_2 & \\cdots & y_T\\end{bmatrix}^\\tr\n\\\\\n\\BX_T\n&=\n\\begin{bmatrix}\n1 & 1 & \\cdots & 1 \\\\\ny_0 & y_1 & \\cdots & y_{T-1}\\\\\ny_{-1} & y_0 & \\cdots & y_{T-2}\\\\\n\\vdots & \\vdots & \\cdots & \\vdots\\\\\ny_{1-p} & y_{1-p+1} & \\cdots & y_{T-p}\\\\\n\\end{bmatrix}^\\tr\n\\\\\n\\Bbeta\n&=\n\\begin{bmatrix}\\beta_0&\\beta_1 & \\cdots & \\beta_p\\end{bmatrix}^\\tr\n.\n\\end{aligned}\n\\]\n\n\nSo the log-likelihood is:\n\\[\n\\ln\np(y_{1:T}\\given  y_0\\com \\Btheta)\n=\n-\\frac{T}{2}\\ln(2\\pi\\sigma^2)\n-\\frac{1}{2\\sigma^2}\n||\\By_T-\\BX_T\\Bbeta||_2^2\n.\n\\]\n\n\nLook familiar?"
  },
  {
    "objectID": "slides/05-ar-p-structure.html#were-just-back-to-regression-101",
    "href": "slides/05-ar-p-structure.html#were-just-back-to-regression-101",
    "title": "What is the joint distribution of an AR(p)",
    "section": "We’re just back to regression 101",
    "text": "We’re just back to regression 101\nWe have\n\\[\n\\ln\np(y_{1:T}\\given  y_0\\com \\Btheta)\n=\n-\\frac{T}{2}\\ln(2\\pi\\sigma^2)\n-\\frac{1}{2\\sigma^2}\n||\\By_T-\\BX_T\\Bbeta||_2^2\n,\n\\]\n\nand we know that\n\\[\n\\begin{aligned}\n\\hat{\\Btheta}_T\n&=\\argmax{\\Btheta}\\,\\ln p(y_{1:T}\\given y_0\\com \\Btheta)\n\\\\\n&=\\argmin{\\Btheta}\\,-\\ln p(y_{1:T}\\given y_0\\com \\Btheta).\n\\end{aligned}\n\\]\n\n\nThis gives\n\\[\n\\begin{aligned}\n\\hat{\\Bbeta}_T\n&=\n\\argmin{\\Bbeta}\n\\,||\\By_T-\\BX_T\\Bbeta||_2^2\n\\\\\n\\hat{\\sigma^2_T}\n&=\n||\\By_T-\\BX_T\\hat{\\Bbeta}_T||_2^2 / T.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/05-ar-p-structure.html#ols-for-the-ar1",
    "href": "slides/05-ar-p-structure.html#ols-for-the-ar1",
    "title": "What is the joint distribution of an AR(p)",
    "section": "OLS for the AR(1)",
    "text": "OLS for the AR(1)\n\n\nThe maximum (conditional) likelihood estimator in the AR(1) is the same as the ordinary least squares estimator:\n\n\n\n\\[\n\\begin{aligned}\n\\hat{\\Bbeta}_T\n&=\n(\\BX_T^\\tr\\BX_T)^{-1}\\BX_T^\\tr\\By_T\n\\\\\n\\hat{\\sigma^2_T}\n&=\n||\\By_T-\\BX_T\\hat{\\Bbeta}_T||_2^2 / T.\n\\end{aligned}\n\\]\n\n\n\nIt doesn’t matter that there’s time series dependence. Once the data are observed and fixed, everything you know about iid multiple regression applies unmodified."
  },
  {
    "objectID": "slides/05-ar-p-structure.html#if-the-data-are-streaming-apply-pset-0",
    "href": "slides/05-ar-p-structure.html#if-the-data-are-streaming-apply-pset-0",
    "title": "What is the joint distribution of an AR(p)",
    "section": "If the data are streaming, apply PSET 0!",
    "text": "If the data are streaming, apply PSET 0!\nDo not recompute that matrix inverse every period.\n\nInstead:\n\\[\n\\begin{aligned}\n\\hat{\\Bbeta}_t\n&=\n\\hat{\\Bbeta}_{t-1}\n+\n\\Bk_t(y_t-\\Bx_t^\\tr\\hat{\\Bbeta}_{t-1})\n\\\\\n\\Bk_t\n&=\n\\frac{\\BP_{t-1}\\Bx_t}\n{1+\\Bx_t^\\tr\\BP_{t-1}\\Bx_t}\n\\\\\n\\BP_t\n&=\n(\\BX_t^\\tr\\BX_t)^{-1}\n\\\\\n&=\n(\\BX_{t-1}^\\tr\\BX_{t-1}+\\Bx_t\\Bx_t^\\tr)^{-1}\n\\\\\n&=\n(\\BP_{t-1}+\\Bx_t\\Bx_t^\\tr)^{-1}\n\\\\\n&=\n\\BP_{t-1}\n-\n\\Bk_t\n\\Bx_t^\\tr\\BP_{t-1}\n.\n\\end{aligned}\n\\]\n\n\nKeyword: rank-1 update!"
  },
  {
    "objectID": "slides/05-ar-p-structure.html#asymptotics-1",
    "href": "slides/05-ar-p-structure.html#asymptotics-1",
    "title": "What is the joint distribution of an AR(p)",
    "section": "Asymptotics",
    "text": "Asymptotics\n\n\n\n\n\n\nShumway and Stoffer (2025) Property 3.9\n\n\nLet \\(\\hat{\\boldsymbol{\\beta}} = (\\hat{\\beta}_1, \\dots, \\hat{\\beta}_p)^\\top\\) be the MLE/OLS estimator of the AR coefficients. If things are stationary, then:\n\\[\n\\sqrt{T} \\, (\\hat{\\boldsymbol{\\beta}} - \\boldsymbol{\\beta}) \\;\\;\\;\\overset{d}{\\longrightarrow}\\;\\;\\; \\N_p\\bigl(\\mathbf{0}, \\, \\sigma^2 \\, \\mathbf{\\Gamma}_p^{-1}\\bigr).\n\\]\nwhere \\(\\mathbf{\\Gamma}_p\\) is the \\(p \\times p\\) autocovariance matrix with \\((i,j)\\) entry \\(\\gamma(|i-j|)\\).\n\n\n\n\nSame as Yule-Walker!"
  },
  {
    "objectID": "slides/05-ar-p-structure.html#recall",
    "href": "slides/05-ar-p-structure.html#recall",
    "title": "What is the joint distribution of an AR(p)",
    "section": "Recall",
    "text": "Recall\nGiven a prior distribution \\(p(\\Btheta)=p(\\beta_0\\com \\beta_1\\com \\sigma^2)\\) on the parameters and an observed time series \\(y_{0:T}\\), we seek to access the posterior distribution:\n\\[\np(\\Btheta\\given y_{0:T})\n=\n\\frac{p(y_{1:T}\\given y_0\\com \\Btheta)p(\\Btheta)}{p(y_{1:T}\\given y_0)}\n.\n\\]\n\nAs with MLE, everything you know about iid Bayesian regression applies pretty much unmodified."
  },
  {
    "objectID": "slides/05-ar-p-structure.html#conjugate-normal-inverse-gamma-prior",
    "href": "slides/05-ar-p-structure.html#conjugate-normal-inverse-gamma-prior",
    "title": "What is the joint distribution of an AR(p)",
    "section": "Conjugate normal-inverse-gamma prior",
    "text": "Conjugate normal-inverse-gamma prior\nBayesian model with a conjugate prior:\n\\[\n\\begin{aligned}\n\\sigma^2\n&\\sim\n\\text{IG}(a_0\\com b_0)\n\\\\\n\\Bbeta\\given \\sigma^2\n&\\sim\n\\text{N}_2(\\Bm_0\\com\\sigma^2\\BH^{-1}_0)\n\\\\\ny_t\n\\given\ny_{t-1}\n\\com\n\\Bbeta\\com\\sigma^2\n&\\sim \\text{N}\n\\left(\n\\Bx_t^\\tr\\Bbeta\\com\\sigma^2\n\\right), && \\Bx_t=\\begin{bmatrix}1 & lags\\end{bmatrix}^\\tr.\n\\end{aligned}\n\\]\n\nThe posterior is available in closed-form:\n\\[\n\\begin{aligned}\n\\sigma^2\\given y_{0:T}\n&\\sim\n\\text{IG}(a_T\\com b_T)\n\\\\\n\\Bbeta\\given \\sigma^2\\com y_{0:T}\n&\\sim\n\\text{N}_2(\\Bm_T\\com\\sigma^2\\BH^{-1}_T)\n\\\\\n\\\\\n\\BH_T\n&=\n\\BX_T^\\tr\\BX_T+\\BH_0\n\\\\\n\\Bm_T\n&=\n\\BH_T^{-1}(\\BX_T^\\tr\\By_T+\\BH_0\\Bm_0)\n\\\\\na_T\n&=\na_0 + T/2\n\\\\\nb_T\n&=\nb_0\n+\n(\\By_T^\\tr\\By_T+\\Bm_0^\\tr\\BH_0\\Bm_0-\\Bm_T^\\tr\\BH_T\\Bm_T)/2.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/05-ar-p-structure.html#but-wait",
    "href": "slides/05-ar-p-structure.html#but-wait",
    "title": "What is the joint distribution of an AR(p)",
    "section": "But wait!",
    "text": "But wait!\nThis entire time we assumed that the lag order \\(p\\) was fixed and known. In practice, we do not know it. It is a hyperparameter that needs to be tuned using the data. How do we do that?"
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#the-simplest-non-trivial-time-series-model",
    "href": "slides/03-ar-1-forecasting.html#the-simplest-non-trivial-time-series-model",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "The simplest non-trivial time series model",
    "text": "The simplest non-trivial time series model\nThe autoregression of order 1, or AR(1):\n\n\\[\n\\begin{aligned}\ny_t\n&=\n\\beta_0\n+\n\\beta_1\ny_{t-1}\n+\n\\varepsilon_t,\n\\quad\n\\varepsilon_t\\iid\\text{N}(0\\com\\sigma^2)\n\\\\\ny_0\n&\\sim\n\\text{N}(\\mu_0\\com \\initvar),\n\\end{aligned}\n\\]\n\n\nThis implies a joint distribution governed by a finite set of static parameters \\(\\Btheta = \\begin{bmatrix}\\beta_0&\\beta_1&\\sigma^2\\end{bmatrix}^\\tr\\):\n\n\n\\[\n\\begin{aligned}\np(y_{0:T}\\given \\Btheta)\n&=\np(y_0)\n\\prod_{t=1}^T\np(y_t\\given y_{t-1}\\com\\Btheta).\n\\end{aligned}\n\\]\n\n\nViewed as a function of \\(\\Btheta\\), that’s a likelihood!"
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#the-ar1-joint-distribution",
    "href": "slides/03-ar-1-forecasting.html#the-ar1-joint-distribution",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "The AR(1) joint distribution",
    "text": "The AR(1) joint distribution\nJoint distribution:\n\\[\n\\begin{bmatrix}\ny_0 & y_1 & \\cdots & y_T\n\\end{bmatrix}^\\tr\n\\sim\\text{N}_{T+1}\\left(\\Bmu\\com \\BSigma\\right).\n\\]\n\nMoments:\n\\[\n\\begin{aligned}\nE(y_t)\n&=\n\\beta_0\\sum\\limits_{i=0}^{t-1}\\beta_1^i\n+\n\\beta_1^t\\mu_0\n\\\\\n\\var(y_t)\n&=\n\\sigma^2\n\\sum\\limits_{i=0}^{t-1}\\beta_1^{2i}+\n\\beta_1^{2t}\\initvar\n\\\\\n\\cov(y_t\\com y_s)\n&=\n\\begin{cases}\n\\beta_1^{s-t}\\var(y_t) & t\\leq s\\\\\n\\beta_1^{t-s}\\var(y_s) & s &lt; t.\n\\end{cases}\n.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#the-special-case-when-beta_11",
    "href": "slides/03-ar-1-forecasting.html#the-special-case-when-beta_11",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "The special case when \\(|\\beta_1|<1\\)\n",
    "text": "The special case when \\(|\\beta_1|&lt;1\\)\n\nThe mean and variance are time-invariant, and the covariance structure is shift-invariant:\n\n\\[\n\\begin{aligned}\nE(y_t)\n&=\n\\frac{\\beta_0}{1-\\beta_1}\n\\\\\n\\var(y_t)\n&=\n\\frac{\\sigma^2}{1-\\beta_1^2}\n\\\\\n\\gamma(h)\n&=\n\\cov(y_{t+h}\\com y_t)\n=\n\\beta_1^{h}\\frac{\\sigma^2}{1-\\beta_1^2}.\n\\end{aligned}\n\\]\n\n\nThe common marginal shared by all \\(y_t\\) is called the stationary distribution:\n\n\n\\[\ny_t\\sim\\text{N}\\left(\\frac{\\beta_0}{1-\\beta_1}\\com \\frac{\\sigma^2}{1-\\beta_1^2}\\right).\n\\]\n\n\nSo “did: dependent but identically distributed.”"
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#method-of-moments",
    "href": "slides/03-ar-1-forecasting.html#method-of-moments",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Method of moments",
    "text": "Method of moments\nThe Yule-Walker estimators are\n\\[\n\\begin{aligned}\n\\hat{\\beta}_1&=\\frac{\\hat{\\gamma}_T(1)}{\\hat{\\gamma}_T(0)}\n\\\\\n\\hat{\\beta}_0&=(1-\\hat{\\beta}_1)\\hat{\\mu}_T\\\\\n\\hat{\\sigma^2_T}&=(1-\\hat{\\beta}_1^2)\\hat{\\gamma}_T(0),\n\\end{aligned}\n\\]\nwhere \\(\\hat{\\mu}_T\\), \\(\\hat{\\gamma}_T(0)\\), and \\(\\hat{\\gamma}_T(1)\\) are simple sample averages.\n\nThese have good statistical properties, but only if the true process is stationary."
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#likelihood-based-inference",
    "href": "slides/03-ar-1-forecasting.html#likelihood-based-inference",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Likelihood-based inference",
    "text": "Likelihood-based inference\nIn practice it is easiest to work with the conditional likelihood:\n\\[\np(y_{1:T}\\given  y_0\\com \\Btheta)\n=\n\\prod_{t=1}^T\np(y_t\\given y_{t-1}\\com\\Btheta).\n\\]\n\nClassical route:\n\\[\n\\hat{\\Btheta}_T=\\argmax{\\Btheta}\\,p(y_{1:T}\\given y_0\\com \\Btheta).\n\\]\n\n\nBayesian route:\n\\[\np(\\Btheta\\given y_{0:T})\n=\n\\frac{p(y_{1:T}\\given y_0\\com \\Btheta)p(\\Btheta)}{p(y_{1:T}\\given y_0)}\n.\n\\]\n\n\nEither way, everything you know about inference for iid regression goes through unmodified."
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#maximum-likelihood",
    "href": "slides/03-ar-1-forecasting.html#maximum-likelihood",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Maximum likelihood",
    "text": "Maximum likelihood\nIt’s just ordinary least squares (OLS):\n\n\\[\n\\begin{aligned}\n\\By_T\n&=\n\\begin{bmatrix}y_1&y_2 & \\cdots & y_T\\end{bmatrix}^\\tr\n\\\\\n\\BX_T\n&=\n\\begin{bmatrix}\n1 & 1 & \\cdots & 1 \\\\\ny_0 & y_1 & \\cdots & y_{T-1}\n\\end{bmatrix}^\\tr\n\\\\\n\\Bbeta\n&=\n\\begin{bmatrix}\\beta_0&\\beta_1\\end{bmatrix}^\\tr\n\\\\\n\\\\\n\\hat{\\Bbeta}_T\n&=\n(\\BX_T^\\tr\\BX_T)^{-1}\\BX_T^\\tr\\By_T\n\\\\\n\\hat{\\sigma^2_T}\n&=\n||\\By_T-\\BX_T\\hat{\\Bbeta}_T||_2^2 / T.\n\\end{aligned}\n\\]\n\n\nThings can get weird, but we have asymptotic theory for this regardless whether or not things are stationary."
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#recursive-version",
    "href": "slides/03-ar-1-forecasting.html#recursive-version",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Recursive version",
    "text": "Recursive version\nIf the data are streaming, we have some bad ass rank-1 updates:\n\n\\[\n\\begin{aligned}\n\\hat{\\Bbeta}_t\n&=\n\\hat{\\Bbeta}_{t-1}\n+\n\\Bk_t(y_t-\\Bx_t^\\tr\\hat{\\Bbeta}_{t-1})\n\\\\\n\\Bk_t\n&=\n\\frac{\\BP_{t-1}\\Bx_t}\n{1+\\Bx_t^\\tr\\BP_{t-1}\\Bx_t}\n\\\\\n\\BP_t\n&=\n(\\BX_t^\\tr\\BX_t)^{-1}\n\\\\\n&=\n(\\BX_{t-1}^\\tr\\BX_{t-1}+\\Bx_t\\Bx_t^\\tr)^{-1}\n\\\\\n&=\n(\\BP_{t-1}+\\Bx_t\\Bx_t^\\tr)^{-1}\n\\\\\n&=\n\\BP_{t-1}\n-\n\\Bk_t\n\\Bx_t^\\tr\\BP_{t-1}\n.\n\\end{aligned}\n\\]\n\n\nThanks Sherman-Morrison!"
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#conjugate-bayes",
    "href": "slides/03-ar-1-forecasting.html#conjugate-bayes",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Conjugate Bayes",
    "text": "Conjugate Bayes\nTake a conjugate normal-inverse-gamma prior:\n\n\\[\n\\begin{aligned}\n\\sigma^2\n&\\sim\n\\text{IG}(a_0\\com b_0)\n\\\\\n\\Bbeta\\given \\sigma^2\n&\\sim\n\\text{N}_2(\\Bm_0\\com\\sigma^2\\BH^{-1}_0)\n\\\\\ny_t\n\\given\ny_{t-1}\n\\com\n\\Bbeta\\com\\sigma^2\n&\\sim \\text{N}\n\\left(\n\\Bx_t^\\tr\\Bbeta\\com\\sigma^2\n\\right), && \\Bx_t=\\begin{bmatrix}1 & y_{t-1}\\end{bmatrix}^\\tr.\n\\end{aligned}\n\\]\n\n\nThe posterior is available in closed-form:\n\\[\n\\begin{aligned}\n\\sigma^2\\given y_{0:T}\n&\\sim\n\\text{IG}(a_T\\com b_T)\n\\\\\n\\Bbeta\\given \\sigma^2\\com y_{0:T}\n&\\sim\n\\text{N}_2(\\Bm_T\\com\\sigma^2\\BH^{-1}_T)\n\\\\\n\\\\\n\\BH_T\n&=\n\\BX_T^\\tr\\BX_T+\\BH_0\n\\\\\n\\Bm_T\n&=\n\\BH_T^{-1}(\\BX_T^\\tr\\By_T+\\BH_0\\Bm_0)\n\\\\\na_T\n&=\na_0 + T/2\n\\\\\nb_T\n&=\nb_0\n+\n(\\By_T^\\tr\\By_T+\\Bm_0^\\tr\\BH_0\\Bm_0-\\Bm_T^\\tr\\BH_T\\Bm_T)/2.\n\\end{aligned}\n\\]\n\n\nMake it recursive on Problem Set 1!"
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#a-classic-example",
    "href": "slides/03-ar-1-forecasting.html#a-classic-example",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "A classic example",
    "text": "A classic example"
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#point-forecast",
    "href": "slides/03-ar-1-forecasting.html#point-forecast",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Point forecast",
    "text": "Point forecast\nYour single-number best guess at tomorrow’s observation:"
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#forecast-interval",
    "href": "slides/03-ar-1-forecasting.html#forecast-interval",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Forecast interval",
    "text": "Forecast interval\nA range of likely values for tomorrow’s observation:"
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#forecast-density",
    "href": "slides/03-ar-1-forecasting.html#forecast-density",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Forecast density",
    "text": "Forecast density\nFull distribution capturing uncertainty about tomorrow:"
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#and-then-tomorrow-finally-comes",
    "href": "slides/03-ar-1-forecasting.html#and-then-tomorrow-finally-comes",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "And then tomorrow finally comes",
    "text": "And then tomorrow finally comes\nSo…how’d we do?"
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#whats-the-point",
    "href": "slides/03-ar-1-forecasting.html#whats-the-point",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "What’s the point?",
    "text": "What’s the point?\n\nWe want intervals and densities to communicate uncertainty about the forecast;\n\nWhat sources of uncertainty?\n\nBasic data uncertainty;\nParameter estimation uncertainty;\nHyperparameter uncertainty;\nModel uncertainty;\nUncertainty introduced by missing data.\n\n\nIn the small world of the AR(1), mainly the first two for now."
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#forecast-distribution",
    "href": "slides/03-ar-1-forecasting.html#forecast-distribution",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Forecast distribution",
    "text": "Forecast distribution\nGiven the data we’ve seen, we want a distribution for the data we haven’t seen.\n\nIn other words:\n\\[\np(y_{t+1:t+H}\\given y_{0:t})\n\\]\n\n\nAssume for now the parameters are fixed and known (red flag!)."
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#clumsy-but-true",
    "href": "slides/03-ar-1-forecasting.html#clumsy-but-true",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Clumsy, but true",
    "text": "Clumsy, but true\n\nWe know\n\\[\n\\begin{bmatrix}\n\\mathbf{y}_t \\\\\n\\mathbf{y}_{t+H}\n\\end{bmatrix}\n\\sim\n\\text{N}_{1+t+H}\n\\left(\n\\begin{bmatrix}\n\\boldsymbol{\\mu}_t \\\\\n\\boldsymbol{\\mu}_{t+H}\n\\end{bmatrix}\n,\\,\n\\begin{bmatrix}\n\\boldsymbol{\\Sigma}_t & \\boldsymbol{\\Sigma}_{t,t+H}\\\\\n\\boldsymbol{\\Sigma}_{t,t+H}^{\\scriptscriptstyle\\mathsf{T}} & \\boldsymbol{\\Sigma}_{t+H}\n\\end{bmatrix}\n\\right),\n\\]\n\n\nand so strictly speaking:\n\\[\n\\mathbf{y}_{t+H} \\,|\\,\\mathbf{y}_t\n\\sim\n\\text{N}_H\n\\left(\n\\boldsymbol{\\mu}_{t+H} + \\boldsymbol{\\Sigma}_{t,t+H}^{\\scriptscriptstyle\\mathsf{T}}\\boldsymbol{\\Sigma}_t^{-1}(\\mathbf{y}_t-\\boldsymbol{\\mu}_t)\n,\\,\n\\boldsymbol{\\Sigma}_{t+H} - \\boldsymbol{\\Sigma}_{t,t+H}^{\\scriptscriptstyle\\mathsf{T}}\n\\boldsymbol{\\Sigma}_t^{-1}\n\\boldsymbol{\\Sigma}_{t,t+H}\n\\right).\n\\]\n\n\nBut let’s make this…useful.\n\n\n(Also, ignore all this ridiculous notation)"
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#lets-give-it-some-thought",
    "href": "slides/03-ar-1-forecasting.html#lets-give-it-some-thought",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Let’s give it some thought",
    "text": "Let’s give it some thought\n\nFirst, the AR(1) is first order Markov, so\n\n\n\\[\np(y_{t+1:t+H}\\given y_{0:t})=p(y_{t+1:t+H}\\given y_{t}).\n\\]\n\n\nThe future data behaves just like the past, with a fixed initial condition:\n\n\n\\[\n\\begin{aligned}\ny_{t+h}\n&=\n\\beta_0\n+\n\\beta_1\ny_{t+h-1}\n+\n\\varepsilon_{t+h},\n\\quad\n\\varepsilon_t\\iid\\text{N}(0\\com\\sigma^2)\n\\\\\ny_t\n&\\text{ fixed}.\n\\end{aligned}\n\\]\nSo the conditional distribution has the same structure as the unconditional joint that we have already studied. Just starts in a different spot."
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#forecast-distributions",
    "href": "slides/03-ar-1-forecasting.html#forecast-distributions",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Forecast distributions",
    "text": "Forecast distributions\n\\[\n\\begin{aligned}\ny_{t+h}\\given y_{0:t}\n&\\sim\\N\\left(\\mu_{t+h|t}\\com \\tau^2_{t+h|t}\\right)\n\\\\\n\\\\\n\\mu_{t+h|t}\n&=\n\\beta_0\\sum\\limits_{i=0}^{h-1}\\beta_1^i\n+\n\\beta_1^hy_t\n\\\\\n&=\n\\beta_0\n+\n\\beta_1\n\\mu_{t+h-1|t}\n&& \\mu_{t|t}=y_t\n\\\\\n\\tau^2_{t+h|t}\n&=\n\\sigma^2\n\\sum\\limits_{i=0}^{h-1}\\beta_1^{2i}\n\\\\\n&=\n\\sigma^2\n+\n\\beta_1^2\n\\tau^2_{t+h-1|t}\n&&\n\\tau^2_{t|t}=0.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#prediction-intervals",
    "href": "slides/03-ar-1-forecasting.html#prediction-intervals",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Prediction intervals",
    "text": "Prediction intervals\nFull distribution:\n\\[\ny_{t+h}\\given y_{0:t}\n\\sim\\N\\left(\\mu_{t+h|t}\\com \\tau^2_{t+h|t}\\right)\n\\]\nPrediction interval:\n\\[\n\\begin{aligned}\n\\mu_{t+h-1|t}\n&\\pm\nz_{1-\\frac{\\alpha}{2}}\n\\tau_{t+h|t}\n\\\\\n\\beta_0\\sum\\limits_{i=0}^{h-1}\\beta_1^i\n&\\pm\nz_{1-\\frac{\\alpha}{2}}\n\\sqrt{\\sigma^2\n\\sum\\limits_{i=0}^{h-1}\\beta_1^{2i}}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#summary",
    "href": "slides/03-ar-1-forecasting.html#summary",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Summary",
    "text": "Summary\nDistribution forecasts:\n\\[\n\\begin{aligned}\ny_{t+h}\\given y_{0:t}\n&\\sim\\N\\left(\\mu_{t+h|t}\\com \\tau^2_{t+h|t}\\right)\n\\\\\n\\\\\n\\mu_{t+h|t}\n&=\n\\beta_0\\sum\\limits_{i=0}^{h-1}\\beta_1^i\n+\n\\beta_1^hy_t\n\\\\\n&=\n\\beta_0\n+\n\\beta_1\n\\mu_{t+h-1|t}\n&& \\mu_{t|t}=y_t\n\\\\\n\\tau^2_{t+h|t}\n&=\n\\sigma^2\n\\sum\\limits_{i=0}^{h-1}\\beta_1^{2i}\n\\\\\n&=\n\\sigma^2\n+\n\\beta_1^2\n\\tau^2_{t+h-1|t}\n&&\n\\tau^2_{t|t}=0.\n\\end{aligned}\n\\]\nPrediction intervals:\n\\[\n\\mu_{t+h-1|t}\n\\pm\nz_{1-\\frac{\\alpha}{2}}\n\\tau_{t+h|t}\n.\n\\]"
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#pics-or-it-didnt-happen",
    "href": "slides/03-ar-1-forecasting.html#pics-or-it-didnt-happen",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Pics or it didn’t happen",
    "text": "Pics or it didn’t happen\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\nlibrary(shiny)\n\nsimulate_ar_1 &lt;- function(T, b0, b1, s, y0){\n  y &lt;- numeric(T)\n  y[1] &lt;- y0\n  for(t in 2:T){\n    y[t] &lt;- b0 + b1 * y[t - 1] + rnorm(1, 0, s)\n  }\n  return(y)\n}\n\nar_1_mean &lt;- function(h, b0, b1, yT){\n  if(h == 0){\n    return(yT)\n  } else {\n    return(b0 * sum(b1 ^ (0:(h-1))) + yT * (b1^h)) \n  }\n}\n\nar_1_var &lt;- function(h, b1, s){\n  if(h == 0){\n    return(0)\n  } else {\n    return((s^2) * sum(b1 ^ (2*(0:(h-1)))))\n  }\n}\n\nui &lt;- fluidPage(\n  titlePanel(\"Forecast distribution of a Gaussian AR(1)\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"b0\", \"β₀\", min = -5, max = 5, value = 0, step = 0.1),\n      sliderInput(\"b1\", \"β₁\", min = -2, max = 2, value = 0, step = 0.1),\n      sliderInput(\"s\", \"σ\", min = 0, max = 2, value = 1, step = 0.1)\n    ),\n    mainPanel(\n      plotOutput(\"distPlot\", height = \"600px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  \n  # fixed observed data\n  set.seed(123)\n  y_obs &lt;- simulate_ar_1(10, 0, 0, 1, 0)\n  \n  output$distPlot &lt;- renderPlot({\n    b0 &lt;- input$b0\n    b1 &lt;- input$b1\n    s &lt;- input$s\n    \n    T_obs &lt;- length(y_obs)\n    H &lt;- 20  # forecast horizon\n    range &lt;- 1:(T_obs + H)\n    \n    # plot window\n    plot(range, c(y_obs, rep(NA,H)), type=\"n\",\n         xlab = \"t\", ylab = expression(y[t]),\n         ylim = c(-20,20), bty=\"n\")\n    \n    # grey forecast region\n    rect(T_obs+0.5, -20, T_obs+H+0.5, 20, col=rgb(0.8,0.8,0.8,0.5), border=NA)\n    \n    # observed data\n    lines(1:T_obs, y_obs, col=\"black\", lwd=2)\n    \n    # forecast distribution intervals\n    alpha = c(0.01, seq(0.1,0.9,0.1))\n    middle &lt;- sapply(0:H, ar_1_mean, b0, b1, y_obs[T_obs])\n    sds &lt;- sqrt(sapply(0:H, ar_1_var, b1, s))\n    f_range &lt;- T_obs:(T_obs+H)\n    \n    for(a in alpha){\n      U = qnorm(1 - a/2, mean = middle, sd = sds)\n      L = qnorm(a/2, mean = middle, sd = sds)\n      polygon(c(f_range, rev(f_range)),\n              c(U, rev(L)),\n              col = rgb(1,0,0,0.15), border=NA)\n    }\n    \n    # add mean forecast line\n    lines(f_range, middle, col=\"red\", lwd=2, lty=2)\n  })\n}\n\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#but-wait",
    "href": "slides/03-ar-1-forecasting.html#but-wait",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "But wait!",
    "text": "But wait!\nThe parameters are not known. They have to be estimated.\n\n\\[\n\\mu_{t+h-1|t}\n\\pm\nz_{1-\\frac{\\alpha}{2}}\n\\tau_{t+h|t}\n.\n\\]\n\n\nCan we plug-in estimates and replace \\(z\\) with \\(t\\)?\n\n\nNot quite."
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#bootstrapping-for-dependent-data-with-iid-errors",
    "href": "slides/03-ar-1-forecasting.html#bootstrapping-for-dependent-data-with-iid-errors",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Bootstrapping for dependent data with iid errors",
    "text": "Bootstrapping for dependent data with iid errors\n\n\nData come from an AR(1) with mean zero iid errors (may not be normal!):\n\n\n\n\\[\ny_t\n=\n\\beta_0\n+\n\\beta_1\ny_{t-1}\n+\n\\varepsilon_t,\n\\quad\n\\varepsilon_t\n\\overset{\\text{iid}}{\\sim}F;\n\\]\n\n\n\nUse observed \\(y_{0:T}\\) to compute OLS estimate \\(\\hat{\\Bbeta}_T\\);\n\n\n\n\nEstimate residuals and center them:\n\n\\[\n\\hat{\\varepsilon}_t=y_t-\\hat{\\beta}_0-\\hat{\\beta}_1y_{t-1}\\quad \\to\\quad e_t=\\hat{\\varepsilon}_t-\\sum\\limits_{j=1}^T\\hat{\\varepsilon}_j/T.\n\\]\n\n\n\nConstruct alternative time series by resampling residuals:\n\n\\[\n\\begin{aligned}\n\\tilde{y}_0&=y_0\\\\\n\\tilde{y}_t&=\\hat{\\beta}_0+\\hat{\\beta}_1\\tilde{y}_{t-1}+\\tilde{e}_{t},&&\\tilde{e}_{t}\\overset{\\text{iid}}{\\sim} \\hat{F}_T.\n\\end{aligned}\n\\]\n\n\nRepeats the last step many times, and from then on it’s bootstrap like normal."
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#bootstrapping-the-residuals",
    "href": "slides/03-ar-1-forecasting.html#bootstrapping-the-residuals",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Bootstrapping the residuals",
    "text": "Bootstrapping the residuals\nGird your loins:\n\n\\[\n\\begin{matrix}\n\\text{1. Original data} &&& y_{0:n} && \\\\\n&&& \\downarrow && \\\\\n\\text{2. OLS} &&& \\hat{\\Btheta}_{T} && \\\\\n&&& \\downarrow && \\\\\n\\text{3. Estimate (centered) residuals} &&& e_{1:T} && \\\\\n&\\swarrow &\\swarrow& \\cdots &\\searrow&\\searrow \\\\\n\\text{4. Resample residuals}&\\tilde{e}_{1:T}^{(1)} &\\tilde{e}_{1:T}^{(2)}& \\cdots &\\tilde{e}_{1:T}^{(k-1)}&\\tilde{e}_{1:T}^{(k)} \\\\\n&\\downarrow &\\downarrow& \\cdots &\\downarrow&\\downarrow \\\\\n\\text{5. Bootstrap time series}&\\tilde{y}_{0:T}^{(1)} &\\tilde{y}_{0:T}^{(2)}& \\cdots &\\tilde{y}_{0:T}^{(k-1)}&\\tilde{y}_{0:T}^{(k)} \\\\\n&\\downarrow &\\downarrow& \\cdots &\\downarrow&\\downarrow \\\\\n\\text{6. Bootstrap estimates}&\\tilde{\\Btheta}_{T}^{(1)} &\\tilde{\\Btheta}_{T}^{(2)}& \\cdots &\\tilde{\\Btheta}_{T}^{(k-1)}&\\tilde{\\Btheta}_{T}^{(k)} \\\\\n&\\downarrow &\\downarrow& \\cdots &\\downarrow&\\downarrow \\\\\n\\text{7. Draw more residuals}&\\tilde{e}_{T+1:T+h}^{(1)} &\\tilde{e}_{T+1:T+h}^{(2)}& \\cdots &\\tilde{e}_{T+1:T+h}^{(k-1)}&\\tilde{e}_{T+1:T+h}^{(k)} \\\\\n&\\downarrow &\\downarrow& \\cdots &\\downarrow&\\downarrow \\\\\n\\text{8. Simulate forecasts}&\\tilde{y}_{T+1:T+h}^{(1)} &\\tilde{y}_{T+1:T+h}^{(2)}& \\cdots &\\tilde{y}_{T+1:T+h}^{(k-1)}&\\tilde{y}_{T+1:T+h}^{(k)} \\\\\n\\end{matrix}\n\\]\n\n\nIn other words, it’s awful."
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#probabilistic-forecasting-via-monte-carlo",
    "href": "slides/03-ar-1-forecasting.html#probabilistic-forecasting-via-monte-carlo",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Probabilistic forecasting via Monte Carlo",
    "text": "Probabilistic forecasting via Monte Carlo\nUse the simulations \\(\\tilde{y}_{t+h}^{(1:k)}=\\left\\{\\tilde{y}_{t+h}^{(1)}\\com \\tilde{y}_{t+h}^{(2)}\\com ...\\com \\tilde{y}_{t+h}^{(k)}\\right\\}\\) to construct whatever forecast object you want:\n\n\\[\n\\hat{y}_{t+h|t}=\\frac{1}{k}\\sum\\limits_{j=1}^k\\tilde{y}_{t+h}^{(j)}. \\quad (\\text{...or median})\n\\]\n\n\nForecast interval:\n\\[\n\\hat{I}_{t+h|t} = \\left[\\hat{Q}_{\\frac{\\alpha}{2}}\\left(\\tilde{y}_{t+h}^{(1:k)}\\right)\\com \\hat{Q}_{1-\\frac{\\alpha}{2}}\\left(\\tilde{y}_{t+h}^{(1:k)}\\right)\\right]. \\quad (...\\text{or hdi})\n\\]\n\n\nForecast distribution:\n\\[\n\\hat{f}_{t+h|t}\n=\n\\text{histogram}\\left(\\tilde{y}_{t+h}^{(1:k)}\\right). \\quad (...\\text{or kde})\n\\]"
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#compare-intervals",
    "href": "slides/03-ar-1-forecasting.html#compare-intervals",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Compare intervals",
    "text": "Compare intervals\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  titlePanel(\"AR(1) Forecast: Plug-in vs Residual Bootstrap\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"true_b0\", \"True β₀:\", min = -1, max = 1, value = 0.5, step = 0.1),\n      sliderInput(\"true_b1\", \"True β₁:\", min = -0.95, max = 0.95, value = 0.7, step = 0.05),\n      sliderInput(\"true_sigma\", \"True σ:\", min = 0.1, max = 2, value = 1, step = 0.1),\n      sliderInput(\"n_obs\", \"Sample size:\", min = 40, max = 500, value = 100, step = 10),\n      actionButton(\"rerun\", \"Re-run simulation\"),\n      checkboxInput(\"show_red\", \"Show plug-in fan (red)\", TRUE),\n      checkboxInput(\"show_blue\", \"Show bootstrap fan (blue)\", TRUE)\n    ),\n    \n    mainPanel(\n      plotOutput(\"fanPlot\", height = \"600px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  \n  simulate_ar_1 &lt;- function(T, b0, b1, s, y1 = 0){\n    y &lt;- numeric(T)\n    y[1] &lt;- y1\n    for(t in 2:T){\n      y[t] &lt;- b0 + b1 * y[t-1] + rnorm(1, 0, s)\n    }\n    return(y)\n  }\n  \n  ar_1_mean_h &lt;- function(h, b0, b1, yT){\n    if(h == 0) return(yT)\n    b0 * sum(b1^(0:(h-1))) + yT * (b1^h)\n  }\n  \n  ar_1_var_h &lt;- function(h, b1, sigma){\n    if(h == 0) return(0)\n    sigma^2 * sum(b1^(2*(0:(h-1))))\n  }\n  \n  sim_data &lt;- reactiveVal(NULL)\n  \n  observeEvent(input$rerun, {\n    set.seed(123)  # keep deterministic for reproducibility\n    n_obs &lt;- input$n_obs\n    H &lt;- 20\n    B &lt;- 7500\n    \n    y_obs &lt;- simulate_ar_1(n_obs, input$true_b0, input$true_b1, input$true_sigma, y1 = 0)\n    \n    # OLS fit\n    Y &lt;- y_obs[2:n_obs]\n    X &lt;- cbind(1, y_obs[1:(n_obs-1)])\n    ols_fit &lt;- lm(Y ~ X - 1)\n    coef_hat &lt;- coef(ols_fit)\n    b0_hat &lt;- coef_hat[1]\n    b1_hat &lt;- coef_hat[2]\n    resid_hat &lt;- resid(ols_fit)\n    sigma_hat &lt;- sqrt(sum(resid_hat^2) / (length(resid_hat) - 1))\n    \n    h_seq &lt;- 0:H\n    plug_mean &lt;- sapply(h_seq, ar_1_mean_h, b0 = b0_hat, b1 = b1_hat, yT = y_obs[n_obs])\n    plug_sd   &lt;- sqrt(sapply(h_seq, ar_1_var_h, b1 = b1_hat, sigma = sigma_hat))\n    \n    # bootstrap\n    bootstrap_forecasts &lt;- matrix(NA, nrow = B, ncol = H + 1)\n    resid_centered &lt;- resid_hat - mean(resid_hat)\n    \n    for(b in 1:B){\n      e_star &lt;- sample(resid_centered, size = n_obs - 1, replace = TRUE)\n      y_star &lt;- numeric(n_obs)\n      y_star[1] &lt;- y_obs[1]\n      for(t in 2:n_obs){\n        y_star[t] &lt;- b0_hat + b1_hat * y_star[t-1] + e_star[t-1]\n      }\n      Ys &lt;- y_star[2:n_obs]\n      Xs &lt;- cbind(1, y_star[1:(n_obs-1)])\n      fit_star &lt;- lm(Ys ~ Xs - 1)\n      coef_star &lt;- coef(fit_star)\n      b0_star &lt;- coef_star[1]\n      b1_star &lt;- coef_star[2]\n      resid_star &lt;- resid(fit_star)\n      resid_star_centered &lt;- resid_star - mean(resid_star)\n      \n      y_fut &lt;- numeric(H + 1)\n      y_fut[1] &lt;- y_obs[n_obs]\n      future_shocks &lt;- sample(resid_star_centered, size = H, replace = TRUE)\n      for(h in 1:H){\n        y_fut[h + 1] &lt;- b0_star + b1_star * y_fut[h] + future_shocks[h]\n      }\n      bootstrap_forecasts[b, ] &lt;- y_fut\n    }\n    \n    boot_mean &lt;- colMeans(bootstrap_forecasts)\n    \n    sim_data(list(\n      y_obs = y_obs,\n      plug_mean = plug_mean,\n      plug_sd = plug_sd,\n      bootstrap_forecasts = bootstrap_forecasts,\n      boot_mean = boot_mean,\n      H = H\n    ))\n  }, ignoreNULL = FALSE)\n  \n  output$fanPlot &lt;- renderPlot({\n    dat &lt;- sim_data()\n    if(is.null(dat)) return(NULL)\n    \n    y_obs &lt;- dat$y_obs\n    plug_mean &lt;- dat$plug_mean\n    plug_sd &lt;- dat$plug_sd\n    bootstrap_forecasts &lt;- dat$bootstrap_forecasts\n    boot_mean &lt;- dat$boot_mean\n    H &lt;- dat$H\n    \n    n_obs &lt;- length(y_obs)\n    \n    # Window: last 20 obs and 20 forecasts\n    obs_window &lt;- (n_obs-19):n_obs\n    f_range &lt;- (n_obs):(n_obs + H)\n    plot_range &lt;- c((n_obs-19):(n_obs+H))\n    \n    y_min &lt;- min(c(y_obs[obs_window], plug_mean - 4 * plug_sd, bootstrap_forecasts))\n    y_max &lt;- max(c(y_obs[obs_window], plug_mean + 4 * plug_sd, bootstrap_forecasts))\n    \n    plot(plot_range, rep(NA, length(plot_range)), type = \"n\",\n         xlab = \"t\", ylab = expression(y[t]),\n         ylim = c(y_min, y_max), bty = \"n\",\n         main = \"AR(1) forecast: plug-in (red) vs residual-bootstrap (blue)\")\n    \n    rect(n_obs + 0.5, y_min, n_obs + H + 0.5, y_max,\n         col = rgb(0.85,0.85,0.85,0.5), border = NA)\n    \n    lines(obs_window, y_obs[obs_window], col = \"black\", lwd = 2)\n    \n    # bootstrap fan\n    if(input$show_blue){\n      prob_levels &lt;- c(0.001, 0.005, 0.01, seq(0.02, 0.48, by = 0.02))\n      lower_probs &lt;- prob_levels\n      upper_probs &lt;- 1 - prob_levels\n      boot_fan_lower &lt;- apply(bootstrap_forecasts, 2, quantile, probs = lower_probs)\n      boot_fan_upper &lt;- apply(bootstrap_forecasts, 2, quantile, probs = upper_probs)\n      for(i in seq_len(nrow(boot_fan_lower))){\n        polygon(c(f_range, rev(f_range)),\n                c(boot_fan_upper[i, ], rev(boot_fan_lower[i, ])),\n                col = rgb(0,0,1,0.08), border = NA)\n      }\n      lines(f_range, apply(bootstrap_forecasts, 2, median),\n            col = rgb(0,0,1,0.8), lty = 2, lwd = 1.5)\n      lines(f_range, boot_mean, col = rgb(0,0,1,0.9), lty = 1, lwd = 1)\n    }\n    \n    # plug-in fan\n    if(input$show_red){\n      alpha_vec &lt;- c(0.01, seq(0.1,0.9,by=0.1))\n      for(a in rev(alpha_vec)){\n        U &lt;- qnorm(1 - a/2, mean = plug_mean, sd = plug_sd)\n        L &lt;- qnorm(a/2, mean = plug_mean, sd = plug_sd)\n        polygon(c(f_range, rev(f_range)),\n                c(U, rev(L)),\n                col = rgb(1, 0, 0, 0.15), border = NA)\n      }\n      lines(f_range, plug_mean, col = \"red\", lty = 2, lwd = 2)\n    }\n    \n    abline(v = n_obs + 0.5, lty = 3)\n  })\n}\n\nshinyApp(ui, server)"
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#thats-just-the-tip-of-the-iceberg",
    "href": "slides/03-ar-1-forecasting.html#thats-just-the-tip-of-the-iceberg",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "That’s just the tip of the iceberg",
    "text": "That’s just the tip of the iceberg\n\n\n\n\nIf you really want to know,\nLahiri is your man."
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#posterior-predictive-distribution",
    "href": "slides/03-ar-1-forecasting.html#posterior-predictive-distribution",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Posterior predictive distribution",
    "text": "Posterior predictive distribution\nAfter computing the posterior\n\\[\np(\\Btheta\\given y_{0:T})\n=\n\\frac{p(y_{1:T}\\given y_0\\com \\Btheta)p(\\Btheta)}{p(y_{1:T}\\given y_0)}\n,\n\\]\n\nyou base forecasts on the posterior predictive distribution:\n\\[\np(y_{T+1:T+H}\\,|\\,y_{1:T})\n=\n\\int\np(y_{T+1:T+H}\\,|\\,y_{1:T},\\,\\boldsymbol{\\theta})\np(\\boldsymbol{\\theta}\\,|\\,y_{1:T})\n\\,\\text{d}\\boldsymbol{\\theta}.\n\\]\n\n\nImmediately incorporates both data and parameter uncertainty by construction.\n\n\nThere’s…nothing else to say about this."
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#probabilistic-prediction-is-a-natural-byproduct-of-a-bayesian-approach",
    "href": "slides/03-ar-1-forecasting.html#probabilistic-prediction-is-a-natural-byproduct-of-a-bayesian-approach",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Probabilistic prediction is a natural byproduct of a Bayesian approach",
    "text": "Probabilistic prediction is a natural byproduct of a Bayesian approach\nIt falls out basically for free:\n\n\n\n\n\n\nBjørnstad, Jan (1990): “Predictive likelihood: a review,” Statistical Science\n\n\n“Prediction of the value of an unobserved or future random variable is a fundamental problem in statistics. From a Bayesian point of view, it is solved in a straightforward manner by finding the posterior predictive density of the unobserved random variable given the data. If one does not want to pay the Bayesian price of having to determine a prior, no unifying basis for prediction has existed until recently.”\n\n\n\n\nIt’s a price I’m willing to pay, dude."
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#in-general-just-simulate",
    "href": "slides/03-ar-1-forecasting.html#in-general-just-simulate",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "In general, just simulate",
    "text": "In general, just simulate\nSimulate the posterior somehow (iid, MCMC, HMC, SMC, whatever), and then for each draw, simulate the model forward:\n\\[\n\\begin{aligned}\n\\Btheta^{(j)} &\\sim p(\\boldsymbol{\\theta}\\,|\\,y_{0:T})\\\\\n\\tilde{y}^{(j)}_{T+1:T+H}\n&\\sim\np(y_{T+1:T+H}\\,|\\,y_{0:T},\\,\\Btheta^{(j)}), &&j = 1\\com 2\\com ...\\com k.\n\\end{aligned}\n\\]\nThe retained sample \\(\\tilde{y}^{(1:k)}_{T+1:T+H}\\) is drawn from the posterior predictive."
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#same-slide-as-before",
    "href": "slides/03-ar-1-forecasting.html#same-slide-as-before",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Same slide as before",
    "text": "Same slide as before\nUse the simulations \\(\\tilde{y}_{t+h}^{(1:k)}=\\left\\{\\tilde{y}_{t+h}^{(1)}\\com \\tilde{y}_{t+h}^{(2)}\\com ...\\com \\tilde{y}_{t+h}^{(k)}\\right\\}\\) to construct whatever forecast object you want:\n\n\\[\n\\hat{y}_{t+h|t}=\\frac{1}{k}\\sum\\limits_{j=1}^k\\tilde{y}_{t+h}^{(j)}. \\quad (\\text{...or median})\n\\]\n\n\nForecast interval:\n\\[\n\\hat{I}_{t+h|t} = \\left[\\hat{Q}_{\\frac{\\alpha}{2}}\\left(\\tilde{y}_{t+h}^{(1:k)}\\right)\\com \\hat{Q}_{1-\\frac{\\alpha}{2}}\\left(\\tilde{y}_{t+h}^{(1:k)}\\right)\\right]. \\quad (...\\text{or hdi})\n\\]\n\n\nForecast distribution:\n\\[\n\\hat{f}_{t+h|t}\n=\n\\text{histogram}\\left(\\tilde{y}_{t+h}^{(1:k)}\\right). \\quad (...\\text{or kde})\n\\]"
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#one-step-ahead-in-the-conjugate-case",
    "href": "slides/03-ar-1-forecasting.html#one-step-ahead-in-the-conjugate-case",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "One-step-ahead in the conjugate case",
    "text": "One-step-ahead in the conjugate case\nWe want\n\n\\[\np(y_{t+1}\\given y_{0:t})\n=\n\\int\n\\int\np(y_{t+1}\\given\\Bbeta\\com\\sigma^2\\com y_{0:t})p(\\Bbeta\\com\\sigma^2\\given y_{0:t})\\,\\dd\\Bbeta\\,\\dd\\sigma^2\n,\n\\]\n\n\nand we know that\n\\[\n\\begin{aligned}\n\\sigma^2\\given y_{0:t}\n&\\sim\n\\text{IG}(a_t\\com b_t)\n\\\\\n\\Bbeta\\given \\sigma^2\\com y_{0:t}\n&\\sim\n\\text{N}_2(\\Bm_t\\com\\sigma^2\\BH^{-1}_t)\n\\\\\ny_{t+1}\\given\\Bbeta\\com \\sigma^2\\com y_{0:t}\n&\\sim \\N(\\Bx_{t+1}^\\tr\\Bbeta\\com\\sigma^2).\n\\end{aligned}\n\\]\n\n\nThere is actually a closed-form solution."
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#marginalize-out-bbeta",
    "href": "slides/03-ar-1-forecasting.html#marginalize-out-bbeta",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Marginalize out \\(\\Bbeta\\)\n",
    "text": "Marginalize out \\(\\Bbeta\\)\n\nWe know that\n\\[\n\\begin{aligned}\n\\Bbeta\\given \\sigma^2\\com y_{0:t}\n&\\sim\n\\text{N}_2(\\Bm_t\\com\\sigma^2\\BH^{-1}_t)\n\\\\\ny_{t+1}\n&=\n\\Bx_{t+1}^\\tr\\Bbeta\n+\n\\varepsilon_{t+1}\n,\n&&\n\\varepsilon_{t+1}\\sim\\N(0\\com \\sigma^2).\n\\end{aligned}\n\\]\n\nBy affine transformation:\n\\[\n\\Bx_{t+1}^\\tr\\Bbeta\n\\given \\sigma^2\\com y_{0:t}\n\\sim\n\\N(\n\\Bx_{t+1}^\\tr\\Bm_t\n\\com\n\\sigma^2\\Bx_{t+1}^\\tr\\BH_t^{-1}\\Bx_{t+1}\n).\n\\]\n\n\nBy linear combination of independent normals:\n\\[\n\\begin{aligned}\ny_{t+1}\\given \\sigma^2\\com y_{0:t}\n&\\sim \\N\\left(\\Bx_{t+1}^\\tr\\Bm_t\\com\\sigma^2(1+\\Bx_{t+1}^\\tr\\BH_t^{-1}\\Bx_{t+1})\\right).\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#marginalize-out-sigma2",
    "href": "slides/03-ar-1-forecasting.html#marginalize-out-sigma2",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Marginalize out \\(\\sigma^2\\)\n",
    "text": "Marginalize out \\(\\sigma^2\\)\n\nWe know that\n\\[\n\\begin{aligned}\n\\sigma^2\\given y_{0:t}\n&\\sim\n\\text{IG}(a_t\\com b_t)\n\\\\\ny_{t+1}\\given \\sigma^2\\com y_{0:t}\n&\\sim \\N\\left(\\Bx_{t+1}^\\tr\\Bm_t\\com\\sigma^2(1+\\Bx_{t+1}^\\tr\\BH_t^{-1}\\Bx_{t+1})\\right).\n\\end{aligned}\n\\]\n\nMarginalizing \\(\\sigma^2\\) out of this hierarchy is essentially the definition of Student’s \\(t\\):\n\n\n\\[\n\\begin{aligned}\ny_{t+1}\\given y_{0:t}\n&\\sim\nt(\\nu_{t+1|t}\\com\\bar{y}_{t+1|t}\\com s_{t+1|t}^2)\n\\\\\n\\\\\n\\nu_{t+1|t}\n&=\n2a_t\n\\\\\n\\bar{y}_{t+1|t}\n&=\n\\Bx_{t+1}^\\tr\\Bm_t\n\\\\\ns_{t+1|t}^2\n&=\n\\frac{b_t}{a_t}\n(1+\\Bx_{t+1}^\\tr\\BH_t^{-1}\\Bx_{t+1})\n.\n\\end{aligned}\n\\]\n\n\nSo, Student’s \\(t\\) with location-scale."
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#one-step-ahead-probabilistic-prediction",
    "href": "slides/03-ar-1-forecasting.html#one-step-ahead-probabilistic-prediction",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "One-step-ahead probabilistic prediction",
    "text": "One-step-ahead probabilistic prediction\n\nOur density forecast is:\n\\[\ny_{t+1}\\given y_{0:t}\n\\sim\nt(\\nu_{t+1|t}\\com\\bar{y}_{t+1|t}\\com s_{t+1|t}^2).\n\\]\n\n\nThis gives a \\(100\\times(1-\\alpha)\\%\\) credible interval:\n\\[\n\\bar{y}_{t+1|t}\n\\pm\n\\underbrace{t_{\\nu_{t+1|t}}\n\\left(\n1-\\frac{\\alpha}{2}\n\\right)}_{\\text{quantile of }t(\\nu_{t+1|t}\\com 0\\com 1)}\ns_{t+1|t}\n.\n\\]\n\n\nAnd the moments are\n\\[\n\\begin{aligned}\nE(y_{t+1}\\given y_{0:t})\n&=\n\\bar{y}_{t+1|t}\n,\n&&\n\\nu_{t+1|t}&gt;1\n\\\\\n\\text{var}(y_{t+1}\\given y_{0:t})\n&=\n\\frac{\\nu_{t+1|t}}{\\nu_{t+1|t}-2}\ns_{t+1|t}^2\n,\n&&\n\\nu_{t+1|t}&gt;2.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/03-ar-1-forecasting.html#things-to-notice",
    "href": "slides/03-ar-1-forecasting.html#things-to-notice",
    "title": "How do you compute probabilistic forecasts from the AR(1)?",
    "section": "Things to notice",
    "text": "Things to notice\n\nGoal: generate probabilistic predictions incorporating many sources of uncertainty. In the small world of the AR(1), these are mainly uncertainty from parameter estimation and the inherent randomness of future observations.\n\n\n\n\nClassical approach: pretty unnatural. Can’t do anything analytically. Whole books have been written about how the bootstrapping ought to go;\n\n\n\n\n\nBayesian approach: totally natural. Everything is probabilistic already. Posterior predictive simulation is conceptually straightforward. We can even do the math in a special case.\n\n\n\nBut in fairness, the Bayes stuff leans hard into normality. The bootstrap stuff less so."
  },
  {
    "objectID": "slides/06-ar-p-inference.html#arp",
    "href": "slides/06-ar-p-inference.html#arp",
    "title": "How do you estimate and tune the AR(p)?",
    "section": "AR(p)",
    "text": "AR(p)\nThe autoregression of order \\(p\\), or AR(\\(p\\)):\n\n\\[\n\\begin{aligned}\ny_t\n&=\n\\beta_0\n+\n\\beta_1\ny_{t-1}\n+\n\\beta_2\ny_{t-2}\n+\n\\cdots\n+\n\\beta_p\ny_{t-p}\n+\n\\varepsilon_t,\n\\quad\n\\varepsilon_t\\iid\\text{N}(0\\com\\sigma^2)\n\\\\\n\\begin{bmatrix}\ny_0\n\\\\\ny_{-1}\n\\\\\n\\vdots\n\\\\\ny_{1-p}\n\\end{bmatrix}\n&\\sim\n\\text{N}_p(\\Bmu_0\\com \\BP_0).\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/06-ar-p-inference.html#state-space-form",
    "href": "slides/06-ar-p-inference.html#state-space-form",
    "title": "How do you estimate and tune the AR(p)?",
    "section": "State-space form",
    "text": "State-space form\nEquivalently:\n\\[\n\\begin{aligned}\ny_t\n&=\n\\Bf^\\tr\\Bs_t\\\\\n\\Bs_t\n&=\n\\Bb\n+\n\\BG\n\\Bs_{t-1}\n+\n\\Beta_t\n,\\quad\n\\Beta_t\n\\sim\\N_p(\\Bzero\\com\\BW)\n\\\\\n\\Bs_0&\\sim\n\\text{N}_p(\\Bmu_0\\com \\BP_0)\n,\n\\end{aligned}\n\\] where\n\\[\n\\begin{aligned}\n    \\Bs_t&=\\begin{bmatrix}y_t & y_{t-1} & \\cdots & y_{t-(p-1)} \\end{bmatrix}^\\tr\\\\\n    \\Bf&=\\begin{bmatrix}1 & 0 & \\cdots & 0\\end{bmatrix}^\\tr\\\\\n    \\BG&=\\begin{bmatrix}\n    \\beta_1 & \\beta_2 & \\cdots & \\beta_{p-1} & \\beta_p \\\\\n    1       & 0       & \\cdots & 0           & 0 \\\\\n    0       & 1       & \\cdots & 0           & 0 \\\\\n    \\vdots  & \\vdots  & \\ddots & \\vdots      & \\vdots \\\\\n    0       & 0       & \\cdots & 1           & 0 \\\\\n    \\end{bmatrix}\\\\\n    \\Beta_t&=\\begin{bmatrix}\\varepsilon_t & 0 & \\cdots & 0\\end{bmatrix}^\\tr\n    \\\\\n    \\Bb\n    &=\n    \\begin{bmatrix}\\beta_0 & 0 & \\cdots & 0\\end{bmatrix}^\\tr\n    \\\\\n    \\BW\n    &=\n    \\text{diag}(\\sigma^2\\com 0\\com ...\\com 0).\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/06-ar-p-inference.html#moments",
    "href": "slides/06-ar-p-inference.html#moments",
    "title": "How do you estimate and tune the AR(p)?",
    "section": "Moments",
    "text": "Moments\n\nBy substitution, \\(E(y_t)\\), \\(\\var(y_t)\\), and \\(\\cov(y_t\\com y_s)\\) are the (1, 1) elements of\n\n\n\\[\n\\begin{aligned}\nE(\\Bs_t)\n&=\n\\mathbf{G}^t \\boldsymbol{\\mu}_0 + \\sum_{k=0}^{t-1} \\mathbf{G}^k \\mathbf{b}\n\\\\\n\\cov(\\Bs_t)\n&=\n\\BG^t\\BP_0(\\BG^\\tr)^t+\\sum\\limits_{k=0}^{t-1}\\BG^k\\BW(\\BG^\\tr)^k\n\\\\\n\\cov(\\Bs_t\\com\\Bs_s)\n&=\n\\BG^{|t-s|}\\cov(\\Bs_{\\min\\{s\\com t\\}}).\n\\end{aligned}\n\\]\n\nTrue and computable, but not super enlightening;\nThese will be more informative when we study vector autoregressions. \\(\\Bs_t\\) follows a VAR(1) here;\nEigenvalues of companion matrix \\(\\BG\\) govern the dynamics."
  },
  {
    "objectID": "slides/06-ar-p-inference.html#assume-the-arp-is-stationary",
    "href": "slides/06-ar-p-inference.html#assume-the-arp-is-stationary",
    "title": "How do you estimate and tune the AR(p)?",
    "section": "Assume the AR(p) is stationary",
    "text": "Assume the AR(p) is stationary\nIf the eigenvalues of \\(\\BG\\) are all in the unit circle, then the mean is time invariant, and the covariance is shift-invariant:\n\\[\n\\begin{aligned}\n\\mu\n&=\nE(y_t)\n=\n\\frac{\\beta_0}{1-\\sum\\limits_{l=1}^p\\beta_l} && \\forall t\n\\\\\n\\gamma(h)\n&=\n\\cov(y_{t+h}\\com y_{t})&& \\forall t.\n\\end{aligned}\n\\]\n\nAnd notice that:\n\\[\n\\gamma(-h)=\\cov(y_{t-h}\\com y_{t})=\\cov(y_{t}\\com y_{t-h})=\\gamma(h).\n\\]"
  },
  {
    "objectID": "slides/06-ar-p-inference.html#sample-averages",
    "href": "slides/06-ar-p-inference.html#sample-averages",
    "title": "How do you estimate and tune the AR(p)?",
    "section": "Sample averages",
    "text": "Sample averages\nThe “population” moments can be estimated with sample averages:\n\\[\n\\begin{aligned}\n\\hat{\\mu}_T\n&=\n\\frac{1}{T}\\sum\\limits_{t=1}^Ty_t\n\\\\\n\\hat{\\gamma}_T(h)\n&=\n\\frac{1}{T}\\sum\\limits_{t=1}^{T-h}(y_{t+h}-\\hat{\\mu}_T)(y_t-\\hat{\\mu}_T).\n\\end{aligned}\n\\]\nTo show that \\(\\hat{\\mu}_T\\) and \\(\\hat{\\gamma}_T(h)\\) actually estimate \\(\\mu\\) and \\(\\gamma(h)\\), you need new LLNs and CLTs for dependent data."
  },
  {
    "objectID": "slides/06-ar-p-inference.html#estimating-equations",
    "href": "slides/06-ar-p-inference.html#estimating-equations",
    "title": "How do you estimate and tune the AR(p)?",
    "section": "Estimating equations",
    "text": "Estimating equations\nExpress the expected values as a function of the parameters:\n\\[\n\\begin{aligned}\n\\mu\n&=\\beta_0/\\left(1-\\sum\\limits_{l=1}^p\\beta_l\\right)\n\\\\\n\\gamma(0)\n&=\ng_0(\\beta_0\\com \\beta_1\\com ...\\com \\beta_p\\com \\sigma^2)\n\\\\\n\\gamma(1)\n&=\ng_1(\\beta_0\\com \\beta_1\\com ...\\com \\beta_p\\com \\sigma^2)\n\\\\\n\\gamma(2)\n&=\ng_2(\\beta_0\\com \\beta_1\\com ...\\com \\beta_p\\com \\sigma^2)\n\\\\\n&\\vdots\n\\\\\n\\gamma(p)\n&=\ng_p(\\beta_0\\com \\beta_1\\com ...\\com \\beta_p\\com \\sigma^2)\n.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/06-ar-p-inference.html#method-of-moments-estimator",
    "href": "slides/06-ar-p-inference.html#method-of-moments-estimator",
    "title": "How do you estimate and tune the AR(p)?",
    "section": "Method of moments estimator",
    "text": "Method of moments estimator\n“Plug-in” the sample averages and solve the system:\n\\[\n\\begin{aligned}\n\\hat{\\mu}_T\n&=\\hat{\\beta}_0/\\left(1-\\sum\\limits_{l=1}^p\\hat{\\beta}_l\\right)\n\\\\\n\\hat{\\gamma}_T(0)\n&=\ng_0(\\hat{\\beta}_0\\com \\hat{\\beta}_1\\com ...\\com \\hat{\\beta}_p\\com \\hat{\\sigma^2})\n\\\\\n\\hat{\\gamma}_T(1)\n&=\ng_1(\\hat{\\beta}_0\\com \\hat{\\beta}_1\\com ...\\com \\hat{\\beta}_p\\com \\hat{\\sigma^2})\n\\\\\n\\hat{\\gamma}_T(2)\n&=\ng_2(\\hat{\\beta}_0\\com \\hat{\\beta}_1\\com ...\\com \\hat{\\beta}_p\\com \\hat{\\sigma^2})\n\\\\\n&\\vdots\n\\\\\n\\hat{\\gamma}_T(p)\n&=\ng_p(\\hat{\\beta}_0\\com \\hat{\\beta}_1\\com ...\\com \\hat{\\beta}_p\\com \\hat{\\sigma^2})\n.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/06-ar-p-inference.html#setting-up-the-system",
    "href": "slides/06-ar-p-inference.html#setting-up-the-system",
    "title": "How do you estimate and tune the AR(p)?",
    "section": "Setting up the system",
    "text": "Setting up the system\nAssume that \\(\\beta_0=0\\), or that you’ve otherwise centered the variables with \\(z_t=y_t-\\mu\\), and consider:\n\\[\nz_t = \\sum_{l=1}^p \\beta_l \\, z_{t-l} + \\varepsilon_t.\n\\]\nBy centering, we have that \\(\\gamma(h) = \\operatorname{Cov}(y_t, y_{t-h}) = E(z_t z_{t-h})\\)."
  },
  {
    "objectID": "slides/06-ar-p-inference.html#setting-up-the-system-1",
    "href": "slides/06-ar-p-inference.html#setting-up-the-system-1",
    "title": "How do you estimate and tune the AR(p)?",
    "section": "Setting up the system",
    "text": "Setting up the system\nStart here:\n\\[\nz_t = \\sum_{l=1}^p \\beta_l \\, z_{t-l} + \\varepsilon_t.\n\\]\n\nMultiply both sides by \\(z_{t-k}\\):\n\\[\nz_tz_{t-k} = \\sum_{l=1}^p \\beta_l \\, z_{t-l}z_{t-k} + \\varepsilon_tz_{t-k}.\n\\]\n\n\nTake expected value of both sides:\n\\[\nE(z_tz_{t-k}) = \\sum_{l=1}^p \\beta_l \\, E(z_{t-l}z_{t-k}) + E(\\varepsilon_tz_{t-k}).\n\\]"
  },
  {
    "objectID": "slides/06-ar-p-inference.html#setting-up-the-system-2",
    "href": "slides/06-ar-p-inference.html#setting-up-the-system-2",
    "title": "How do you estimate and tune the AR(p)?",
    "section": "Setting up the system",
    "text": "Setting up the system\nStare at this:\n\\[\nE(z_tz_{t-k}) = \\sum_{l=1}^p \\beta_l \\, E(z_{t-l}z_{t-k}) + E(\\varepsilon_tz_{t-k}).\n\\]\nNotice:\n\n\\(E(z_tz_{t-k})=\\cov(z_t\\com z_{t-k})=\\gamma(k)\\);\n\\(E(z_{t-l}z_{t-k})=\\cov(z_{t-l}\\com z_{t-k})=\\gamma(k-l)\\);\n\\(E(\\varepsilon_tz_{t-k})=\\cov(\\varepsilon_t\\com z_{t-k})=0\\) if \\(k\\geq 1\\) by independence;\n\\(E(\\varepsilon_tz_{t-k})=\\cov(\\varepsilon_t\\com z_t)=\\sigma^2\\) if \\(k=0\\)."
  },
  {
    "objectID": "slides/06-ar-p-inference.html#setting-up-the-system-3",
    "href": "slides/06-ar-p-inference.html#setting-up-the-system-3",
    "title": "How do you estimate and tune the AR(p)?",
    "section": "Setting up the system",
    "text": "Setting up the system\nSumming up, we have\n\\[\n\\begin{aligned}\n\\gamma(k) &= \\sum_{l=1}^p \\beta_l \\, \\gamma(k-l) && k\\geq 1.\n\\\\\n\\gamma(0) &= \\sum_{l=1}^p \\beta_l \\, \\gamma(l) + \\sigma^2\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/06-ar-p-inference.html#example-the-ar1",
    "href": "slides/06-ar-p-inference.html#example-the-ar1",
    "title": "How do you estimate and tune the AR(p)?",
    "section": "Example: the AR(1)",
    "text": "Example: the AR(1)\nIf \\(p=1\\), we saw\n\\[\n\\begin{aligned}\n\\gamma(1)&=\\beta_1\\gamma(0)\\\\\n\\gamma(0)&=\\beta_1\\gamma(1)+\\sigma^2.\n\\end{aligned}\n\\]\nImplying that\n\\[\n\\begin{aligned}\n\\gamma(0)&=\\beta_1\\gamma(1)+\\sigma^2\\\\\n\\gamma(0)&=\\beta_1^2\\gamma(0)+\\sigma^2\\\\\n\\gamma(0)-\\beta_1^2\\gamma(0)&=\\sigma^2\\\\\n(1-\\beta_1^2)\\gamma(0)&=\\sigma^2\\\\\n\\gamma(0)&=\\frac{\\sigma^2}{1-\\beta_1^2}.\\\\\n\\end{aligned}\n\\]\nBeen there; done that;"
  },
  {
    "objectID": "slides/06-ar-p-inference.html#lets-be-super-pedantic",
    "href": "slides/06-ar-p-inference.html#lets-be-super-pedantic",
    "title": "How do you estimate and tune the AR(p)?",
    "section": "Let’s be super pedantic",
    "text": "Let’s be super pedantic\nWe saw:\n\\[\n\\begin{aligned}\n\\gamma(k) &= \\sum_{l=1}^p \\beta_l \\, \\gamma(k-l) && k\\geq 1.\n\\end{aligned}\n\\]\nIn other words:\n\\[\n\\begin{matrix}\n\\gamma(1)\n&=&\n\\beta_1\\gamma(0)\n&+&\n\\beta_2\\gamma(1)\n&+&\n&\\cdots\n&+ &\n\\beta_p\\gamma(p-1)\n\\\\\n\\gamma(2)\n&=&\n\\beta_1\\gamma(1)\n&+&\n\\beta_2\\gamma(0)\n&+&\n&\\cdots\n&+ &\n\\beta_p\\gamma(p-2)\n\\\\\n&\\vdots\n\\\\\n\\gamma(p)\n&=&\n\\beta_1\\gamma(p-1)\n&+&\n\\beta_2\\gamma(p-1)\n&+&\n&\\cdots\n&+ &\n\\beta_p\\gamma(0)\n\\end{matrix}\n\\]\nA system of \\(p\\) linear equations in \\(p\\) unknowns."
  },
  {
    "objectID": "slides/06-ar-p-inference.html#the-yule-walker-equations",
    "href": "slides/06-ar-p-inference.html#the-yule-walker-equations",
    "title": "How do you estimate and tune the AR(p)?",
    "section": "The Yule-Walker equations",
    "text": "The Yule-Walker equations\nRewriting as a matrix equation:\n\\[\n\\begin{aligned}\n\\begin{bmatrix}\n\\gamma(0)\n&\n\\gamma(1)\n&\n\\gamma(2)\n&\n\\cdots\n&\n\\gamma(p-2)\n&\n\\gamma(p-1)\n\\\\\n\\gamma(1)\n&\n\\gamma(0)\n&\n\\gamma(1)\n&\n\\cdots\n&\n\\gamma(p-3)\n&\n\\gamma(p-2)\n\\\\\n\\gamma(2)\n&\n\\gamma(1)\n&\n\\gamma(0)\n&\n\\cdots\n&\n\\gamma(p-4)\n&\n\\gamma(p-3)\n\\\\\n\\vdots\n&\n\\vdots\n&\n\\vdots\n&\n\\ddots\n&\n\\vdots\n&\n\\vdots\n\\\\\n\\gamma(p-2)\n&\n\\gamma(p-3)\n&\n\\gamma(p-4)\n&\n\\cdots\n&\n\\gamma(0)\n&\n\\gamma(1)\n\\\\\n\\gamma(p-1)\n&\n\\gamma(p-2)\n&\n\\gamma(p-3)\n&\n\\cdots\n&\n\\gamma(1)\n&\n\\gamma(0)\n\\end{bmatrix}\n\\begin{bmatrix}\n\\beta_1\n\\\\\n\\beta_2\n\\\\\n\\beta_3\n\\\\\n\\vdots\n\\\\\n\\beta_{p-1}\n\\\\\n\\beta_p\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\gamma(1)\n\\\\\n\\gamma(2)\n\\\\\n\\gamma(3)\n\\\\\n\\vdots\n\\\\\n\\gamma(p-1)\n\\\\\n\\gamma(p)\n\\end{bmatrix}.\n\\end{aligned}\n\\]\nSo\n\\[\n\\BGamma_p\n\\Bbeta_{-0}\n=\n\\boldsymbol{\\gamma}_p\n.\n\\]"
  },
  {
    "objectID": "slides/06-ar-p-inference.html#the-estimating-equations",
    "href": "slides/06-ar-p-inference.html#the-estimating-equations",
    "title": "How do you estimate and tune the AR(p)?",
    "section": "The estimating equations",
    "text": "The estimating equations\nWe know:\n\\[\n\\begin{aligned}\n\\boldsymbol{\\gamma}_p\n&=\n\\BGamma_p\n\\Bbeta_{-0}\n\\\\\n\\gamma(0) &= \\Bbeta_{-0}^\\tr\\boldsymbol{\\gamma}_p+\\sigma^2\n\\\\\n\\mu\n&=\n\\beta_0/\\left(1-\\sum_{l=1}^p\\beta_l\\right).\n\\end{aligned}\n\\]\n\nSolving for the model parameters:\n\\[\n\\begin{aligned}\n\\Bbeta_{-0}\n&=\n\\BGamma_p^{-1}\n\\boldsymbol{\\gamma}_p\n\\\\\n\\sigma^2 &= \\gamma(0)-\\Bbeta_{-0}^\\tr\\boldsymbol{\\gamma}_p\n\\\\\n\\beta_0\n&=\n\\left(1-\\sum_{l=1}^p\\beta_l\\right)\\mu.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/06-ar-p-inference.html#example-the-ar1-1",
    "href": "slides/06-ar-p-inference.html#example-the-ar1-1",
    "title": "How do you estimate and tune the AR(p)?",
    "section": "Example: the AR(1)",
    "text": "Example: the AR(1)\nIf \\(p=1\\), then \\(\\BGamma_p=[\\gamma(0)]\\), \\(\\boldsymbol{\\gamma}_p=[\\gamma(1)]\\), and \\(\\Bbeta_{-0}=[\\beta_1]\\), so we’re just saying that\n\\[\n\\begin{aligned}\n\\beta_1\n&=\n\\gamma(1)/\\gamma(0)\n\\\\\n\\sigma^2\n&=\n\\gamma(0)\n-\n\\beta_1\n\\gamma(1)\n\\\\\n&=\n(1-\\beta_1^2)\\gamma(0)\n\\\\\n\\beta_0\n&=\n(1-\\beta_1)\\mu.\n\\end{aligned}\n\\]\n\nBeen there; done that."
  },
  {
    "objectID": "slides/06-ar-p-inference.html#the-yule-walker-estimators",
    "href": "slides/06-ar-p-inference.html#the-yule-walker-estimators",
    "title": "How do you estimate and tune the AR(p)?",
    "section": "The Yule-Walker estimators",
    "text": "The Yule-Walker estimators\nPlug-in sample averages for all expected values:\n\\[\n\\begin{aligned}\n\\hat{\\Bbeta}_{-0}\n&=\n\\hat{\\BGamma}_p^{-1}\n\\hat{\\boldsymbol{\\gamma}}_p\n\\\\\n\\hat{\\sigma^2} &= \\hat{\\gamma}_T(0)-\\hat{\\Bbeta}_{-0}^\\tr\\hat{\\boldsymbol{\\gamma}}_p\n\\\\\n\\hat{\\beta}_0\n&=\n\\left(1-\\sum_{l=1}^p\\hat{\\beta}_l\\right)\\hat{\\mu}_T.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/06-ar-p-inference.html#asymptotics",
    "href": "slides/06-ar-p-inference.html#asymptotics",
    "title": "How do you estimate and tune the AR(p)?",
    "section": "Asymptotics",
    "text": "Asymptotics\n\n\n\n\n\n\nShumway and Stoffer (2025) Property 3.7\n\n\nIf the AR(p) is stationary, then\n\\[\n\\sqrt{T} \\, (\\hat{\\boldsymbol{\\beta}}_{-0} - \\boldsymbol{\\beta}_{-0})\\cd\\N_p\\bigl(\\mathbf{0}, \\, \\sigma^2 \\, \\mathbf{\\Gamma}_p^{-1}\\bigr).\n\\]\nwhere \\(\\mathbf{\\Gamma}_p\\) is the \\(p \\times p\\) autocovariance matrix with \\((i,j)\\) entry \\(\\gamma(|i-j|)\\)."
  },
  {
    "objectID": "slides/06-ar-p-inference.html#lets-improve-the-notation",
    "href": "slides/06-ar-p-inference.html#lets-improve-the-notation",
    "title": "How do you estimate and tune the AR(p)?",
    "section": "Let’s improve the notation",
    "text": "Let’s improve the notation\nThe autoregression of order p, or AR(p):\n\n\\[\n\\begin{aligned}\ny_t\n&=\n\\beta_0\n+\n\\sum\\limits_{\\ell=1}^p\n\\beta_\\ell\ny_{t-\\ell}\n+\n\\varepsilon_t,\n\\quad\n\\varepsilon_t\\iid\\text{N}(0\\com\\sigma^2).\n\\end{aligned}\n\\]\n\n\nThis implies a joint distribution governed by a finite set of static parameters \\(\\Btheta = \\begin{bmatrix}\\beta_0&\\beta_1&\\cdots &\\beta_p&\\sigma^2\\end{bmatrix}^\\tr\\):\n\n\n\\[\n\\begin{aligned}\np(y_{1:T}\\given y_{1-p:0}\\com\\Btheta)\n&=\n\\prod_{t=1}^T\np(y_t\\given y_{t-p:t-1}\\com\\Btheta).\n\\end{aligned}\n\\]\n\n\nViewed as a function of \\(\\Btheta\\), that’s a (conditional) likelihood!\n\n\nMaximize it, or combine with a prior."
  },
  {
    "objectID": "slides/06-ar-p-inference.html#maximum-likelihood-estimation-1",
    "href": "slides/06-ar-p-inference.html#maximum-likelihood-estimation-1",
    "title": "How do you estimate and tune the AR(p)?",
    "section": "Maximum likelihood estimation",
    "text": "Maximum likelihood estimation\nTreating the observed data \\(y_{1-p:T}\\) as fixed, we want:\n\\[\n\\hat{\\Btheta}_T=\\argmax{\\Btheta}\\,p(y_{1:T}\\given y_{1-p:0}\\com \\Btheta).\n\\]\nTo do this, it helps to view the AR(p) is “just” a multiple linear regression where \\(\\Bx_t\\) encodes the lagged values of \\(y_t\\):\n\\[\n\\begin{aligned}\ny_t&=\\Bx_t^\\tr\\Bbeta+\\varepsilon_t,&&\\varepsilon_t\\iid\\N(0\\com\\sigma^2)\n\\\\\n\\Bx_t&=\\begin{bmatrix}\n1 & y_{t-1} & y_{t-2} & \\cdots & y_{t-p}\n\\end{bmatrix}^\\tr\\\\\n\\Bbeta&=\\begin{bmatrix}\n\\beta_0 & \\beta_1 & \\beta_2 & \\cdots & \\beta_p\n\\end{bmatrix}^\\tr.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/06-ar-p-inference.html#maximum-likelihood-estimation-2",
    "href": "slides/06-ar-p-inference.html#maximum-likelihood-estimation-2",
    "title": "How do you estimate and tune the AR(p)?",
    "section": "Maximum likelihood estimation",
    "text": "Maximum likelihood estimation\nSince \\(y_t\\given \\Bx_t\\com\\Btheta\\sim\\N\\left(\\Bx_t^\\tr\\Bbeta\\com\\sigma^2\\right)\\), we have\n\n\\[\n\\begin{aligned}\np(y_{1:T}\\given  \\Bx_0\\com \\Btheta)\n&=\n\\prod_{t=1}^T\np(y_t\\given \\Bx_t\\com\\Btheta)\n\\\\\n&=\n\\prod_{t=1}^T\n\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(-\\frac{1}{2}\\frac{(y_t-\\Bx_t^\\tr\\Bbeta)^2}{\\sigma^2}\\right)\n\\\\\n&=\n(2\\pi\\sigma^2)^{-T/2}\n\\exp\\left(-\\frac{1}{2\\sigma^2}\\sum\\limits_{t=1}^T(y_t-\\Bx_t^\\tr\\Bbeta)^2\\right)\n.\n\\end{aligned}\n\\]\n\n\nTo compute the MLE, we treat this as a function of \\(\\Btheta\\) with the data fixed."
  },
  {
    "objectID": "slides/06-ar-p-inference.html#stack-em-up",
    "href": "slides/06-ar-p-inference.html#stack-em-up",
    "title": "How do you estimate and tune the AR(p)?",
    "section": "Stack ’em up",
    "text": "Stack ’em up\nConstruct the response vector and design matrix:\n\n\\[\n\\begin{aligned}\n\\underbrace{\\By_T}_{T\\times 1}\n&=\n\\begin{bmatrix}y_1&y_2 & \\cdots & y_T\\end{bmatrix}^\\tr\n\\\\\n\\underbrace{\\BX_T}_{T\\times (p+1)}\n&=\n\\begin{bmatrix}\n1 & y_0 & y_{-1} & \\cdots & y_{1-p} \\\\\n1 & y_1 & y_0 & \\cdots & y_{2-p} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & y_{T-1} & y_{T-2} & \\cdots & y_{T-p}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\Bx_1^\\tr\\\\\n\\Bx_2^\\tr\\\\\n\\vdots\\\\\n\\Bx_T^\\tr\n\\end{bmatrix}\n.\n\\end{aligned}\n\\]\n\n\nSo the likelihood is:\n\\[\n\\begin{aligned}\np(y_{1:T}\\given  \\Bx_0\\com \\Btheta)\n&=\n(2\\pi\\sigma^2)^{-T/2}\n\\exp\\left(-\\frac{1}{2\\sigma^2}\\sum\\limits_{t=1}^T(y_t-\\Bx_t^\\tr\\Bbeta)^2\\right)\n\\\\\n&=\n(2\\pi\\sigma^2)^{-T/2}\n\\exp\\left(-\\frac{1}{2\\sigma^2}||\\By_T-\\BX_T\\Bbeta||_2^2\\right)\n.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/06-ar-p-inference.html#what-are-we-doing",
    "href": "slides/06-ar-p-inference.html#what-are-we-doing",
    "title": "How do you estimate and tune the AR(p)?",
    "section": "What are we doing?",
    "text": "What are we doing?\nLikelihood:\n\\[\n\\begin{aligned}\nL(\\Bbeta\\com\\sigma^2)\n&=\n(2\\pi\\sigma^2)^{-T/2}\n\\exp\\left(-\\frac{1}{2\\sigma^2}||\\By_T-\\BX_T\\Bbeta||_2^2\\right)\n.\n\\end{aligned}\n\\]\nLog-likelihood:\n\\[\n\\ell(\\Bbeta\\com\\sigma^2)\n=\n-\\frac{T}{2}\\ln(2\\pi\\sigma^2)\n-\\frac{1}{2\\sigma^2}\n||\\By_T-\\BX_T\\Bbeta||_2^2\n,\n\\]\nWe want:\n\\[\n\\begin{aligned}\n\\hat{\\Bbeta}_T\\com \\hat{\\sigma_T^2}\n&=\\argmax{\\Bbeta\\com\\sigma^2}\\,\\ell(\\Bbeta\\com\\sigma^2).\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/06-ar-p-inference.html#ols-for-the-arp",
    "href": "slides/06-ar-p-inference.html#ols-for-the-arp",
    "title": "How do you estimate and tune the AR(p)?",
    "section": "OLS for the AR(p)",
    "text": "OLS for the AR(p)\n\n\nThe maximum (conditional) likelihood estimator in the AR(p) is the same as the ordinary least squares estimator:\n\n\n\n\\[\n\\begin{aligned}\n\\hat{\\Bbeta}_T\n&=\n(\\BX_T^\\tr\\BX_T)^{-1}\\BX_T^\\tr\\By_T\n\\\\\n\\hat{\\sigma^2_T}\n&=\n||\\By_T-\\BX_T\\hat{\\Bbeta}_T||_2^2 / T.\n\\end{aligned}\n\\]\n\n\n\nIt doesn’t matter that there’s time series dependence. Once the data are observed and fixed, the mechanics are identical to iid multiple regression;\nFor things like prediction, dependence matters, and the correspondence with iid regression breaks down."
  },
  {
    "objectID": "slides/06-ar-p-inference.html#recursive-form",
    "href": "slides/06-ar-p-inference.html#recursive-form",
    "title": "How do you estimate and tune the AR(p)?",
    "section": "Recursive form",
    "text": "Recursive form\nDo not incur the \\(O(t)\\) cost of re-computing \\((\\BX_t^\\tr\\BX_t)^{-1}\\) every period.\n\nInstead:\n\\[\n\\begin{aligned}\n\\hat{\\Bbeta}_t\n&=\n\\hat{\\Bbeta}_{t-1}\n+\n\\Bk_t(y_t-\\Bx_t^\\tr\\hat{\\Bbeta}_{t-1})\n\\\\\n\\Bk_t\n&=\n\\frac{\\BP_{t-1}\\Bx_t}\n{1+\\Bx_t^\\tr\\BP_{t-1}\\Bx_t}\n\\\\\n\\BP_t\n&=\n\\BP_{t-1}\n-\n\\Bk_t\n\\Bx_t^\\tr\\BP_{t-1}\n.\n\\end{aligned}\n\\]\n\n\nKeyword: rank-1 update!"
  },
  {
    "objectID": "slides/06-ar-p-inference.html#asymptotics-1",
    "href": "slides/06-ar-p-inference.html#asymptotics-1",
    "title": "How do you estimate and tune the AR(p)?",
    "section": "Asymptotics",
    "text": "Asymptotics\n\n\n\n\n\n\nShumway and Stoffer (2025) Property 3.9\n\n\nIf the AR(p) is stationary, then\n\\[\n\\sqrt{T} \\, (\\hat{\\boldsymbol{\\beta}}_{-0} - \\boldsymbol{\\beta}_{-0})\\cd\\N_p\\bigl(\\mathbf{0}, \\, \\sigma^2 \\, \\mathbf{\\Gamma}_p^{-1}\\bigr).\n\\]\nwhere \\(\\mathbf{\\Gamma}_p\\) is the \\(p \\times p\\) autocovariance matrix with \\((i,j)\\) entry \\(\\gamma(|i-j|)\\).\n\n\n\n\nSame as Yule-Walker!"
  },
  {
    "objectID": "slides/06-ar-p-inference.html#batch-form",
    "href": "slides/06-ar-p-inference.html#batch-form",
    "title": "How do you estimate and tune the AR(p)?",
    "section": "Batch form",
    "text": "Batch form\nBayesian model with a conjugate prior:\n\\[\n\\begin{aligned}\n\\sigma^2\n&\\sim\n\\text{IG}(a_0\\com b_0)\n\\\\\n\\Bbeta\\given \\sigma^2\n&\\sim\n\\text{N}_{p+1}(\\Bm_0\\com\\sigma^2\\BH^{-1}_0)\n\\\\\ny_t\n\\given\n\\Bx_t\n\\com\n\\Bbeta\\com\\sigma^2\n&\\sim \\text{N}\n\\left(\n\\Bx_t^\\tr\\Bbeta\\com\\sigma^2\n\\right).\n\\end{aligned}\n\\]\n\nThe posterior is available in closed-form:\n\\[\n\\begin{aligned}\n\\sigma^2\\given y_{0:T}\n&\\sim\n\\text{IG}(a_T\\com b_T)\n\\\\\n\\Bbeta\\given \\sigma^2\\com y_{0:T}\n&\\sim\n\\text{N}_{p+2}(\\Bm_T\\com\\sigma^2\\BH^{-1}_T)\n\\\\\n\\\\\n\\BH_T\n&=\n\\BX_T^\\tr\\BX_T+\\BH_0\n\\\\\n\\Bm_T\n&=\n\\BH_T^{-1}(\\BX_T^\\tr\\By_T+\\BH_0\\Bm_0)\n\\\\\na_T\n&=\na_0 + T/2\n\\\\\nb_T\n&=\nb_0\n+\n(\\By_T^\\tr\\By_T+\\Bm_0^\\tr\\BH_0\\Bm_0-\\Bm_T^\\tr\\BH_T\\Bm_T)/2.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/06-ar-p-inference.html#recursive-form-1",
    "href": "slides/06-ar-p-inference.html#recursive-form-1",
    "title": "How do you estimate and tune the AR(p)?",
    "section": "Recursive form",
    "text": "Recursive form\nDefine:\n\\[\n\\begin{aligned}\ne_{t|t-1}&=y_t-\\Bx_t^\\tr\\Bm_{t-1}\\\\\nr_{t|t-1}^2&=1+\\Bx_t^\\tr\\BH_{t-1}^{-1}\\Bx_t\\\\\n\\Bk_t&=\\BH_{t-1}^{-1}\\Bx_t/r_{t|t-1}^2.\n\\end{aligned}\n\\]\nThen:\n\\[\n\\begin{aligned}\n\\Bm_t\n&=\n\\Bm_{t-1}\n+\n\\Bk_te_{t|t-1}\n\\\\\n\\BH_t^{-1}\n&=\n\\BH_{t-1}^{-1}-\\Bk_t\\Bx_t^\\tr\\BH_{t-1}^{-1}\n\\\\\na_t\n&=a_{t-1}+1/2\n\\\\\nb_t\n&=\nb_{t-1}\n+\n\\frac{1}{2}\n\\frac{e_{t|t-1}^2}{r_{t|t-1}^2}.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/06-ar-p-inference.html#one-step-posterior-predictive-distribution",
    "href": "slides/06-ar-p-inference.html#one-step-posterior-predictive-distribution",
    "title": "How do you estimate and tune the AR(p)?",
    "section": "One-step posterior predictive distribution",
    "text": "One-step posterior predictive distribution\nWe want\n\n\\[\np(y_{t+1}\\given y_{0:t})\n=\n\\int\n\\int\np(y_{t+1}\\given\\Bbeta\\com\\sigma^2\\com y_{0:t})p(\\Bbeta\\com\\sigma^2\\given y_{0:t})\\,\\dd\\Bbeta\\,\\dd\\sigma^2\n,\n\\]\n\n\nand we know that\n\\[\n\\begin{aligned}\n\\sigma^2\\given y_{0:t}\n&\\sim\n\\text{IG}(a_t\\com b_t)\n\\\\\n\\Bbeta\\given \\sigma^2\\com y_{0:t}\n&\\sim\n\\text{N}_{p+1}(\\Bm_t\\com\\sigma^2\\BH^{-1}_t)\n\\\\\ny_{t+1}\\given\\Bbeta\\com \\sigma^2\\com y_{0:t}\n&\\sim \\N(\\Bx_{t+1}^\\tr\\Bbeta\\com\\sigma^2).\n\\end{aligned}\n\\]\n\n\nThere is actually a closed-form solution."
  },
  {
    "objectID": "slides/06-ar-p-inference.html#marginalize-out-bbeta",
    "href": "slides/06-ar-p-inference.html#marginalize-out-bbeta",
    "title": "How do you estimate and tune the AR(p)?",
    "section": "Marginalize out \\(\\Bbeta\\)",
    "text": "Marginalize out \\(\\Bbeta\\)\nWe know that\n\\[\n\\begin{aligned}\n\\Bbeta\\given \\sigma^2\\com y_{0:t}\n&\\sim\n\\text{N}_{p+1}(\\Bm_t\\com\\sigma^2\\BH^{-1}_t)\n\\\\\ny_{t+1}\n&=\n\\Bx_{t+1}^\\tr\\Bbeta\n+\n\\varepsilon_{t+1}\n,\n&&\n\\varepsilon_{t+1}\\sim\\N(0\\com \\sigma^2).\n\\end{aligned}\n\\]\n\nBy affine transformation:\n\\[\n\\Bx_{t+1}^\\tr\\Bbeta\n\\given \\sigma^2\\com y_{0:t}\n\\sim\n\\N(\n\\Bx_{t+1}^\\tr\\Bm_t\n\\com\n\\sigma^2\\Bx_{t+1}^\\tr\\BH_t^{-1}\\Bx_{t+1}\n).\n\\]\n\n\nBy linear combination of independent normals:\n\\[\n\\begin{aligned}\ny_{t+1}\\given \\sigma^2\\com y_{0:t}\n&\\sim \\N\\left(\\Bx_{t+1}^\\tr\\Bm_t\\com\\sigma^2(1+\\Bx_{t+1}^\\tr\\BH_t^{-1}\\Bx_{t+1})\\right).\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/06-ar-p-inference.html#marginalize-out-sigma2",
    "href": "slides/06-ar-p-inference.html#marginalize-out-sigma2",
    "title": "How do you estimate and tune the AR(p)?",
    "section": "Marginalize out \\(\\sigma^2\\)",
    "text": "Marginalize out \\(\\sigma^2\\)\nWe know that\n\\[\n\\begin{aligned}\n\\sigma^2\\given y_{0:t}\n&\\sim\n\\text{IG}(a_t\\com b_t)\n\\\\\ny_{t+1}\\given \\sigma^2\\com y_{0:t}\n&\\sim \\N\\left(\\Bx_{t+1}^\\tr\\Bm_t\\com\\sigma^2(1+\\Bx_{t+1}^\\tr\\BH_t^{-1}\\Bx_{t+1})\\right).\n\\end{aligned}\n\\]\n\nMarginalizing \\(\\sigma^2\\) out of this hierarchy is essentially the definition of Student’s \\(t\\):\n\n\n\\[\n\\begin{aligned}\ny_{t+1}\\given y_{0:t}\n&\\sim\nt(\\nu_{t+1|t}\\com\\bar{y}_{t+1|t}\\com s_{t+1|t}^2)\n\\\\\n\\\\\n\\nu_{t+1|t}\n&=\n2a_t\n\\\\\n\\bar{y}_{t+1|t}\n&=\n\\Bx_{t+1}^\\tr\\Bm_t\n\\\\\ns_{t+1|t}^2\n&=\n\\frac{b_t}{a_t}\n(1+\\Bx_{t+1}^\\tr\\BH_t^{-1}\\Bx_{t+1})\n.\n\\end{aligned}\n\\]\n\n\nSo, Student’s \\(t\\) with location-scale."
  },
  {
    "objectID": "slides/06-ar-p-inference.html#rules-of-thumb",
    "href": "slides/06-ar-p-inference.html#rules-of-thumb",
    "title": "How do you estimate and tune the AR(p)?",
    "section": "Rules of thumb",
    "text": "Rules of thumb\nPick \\(p\\) = number of observations in one natural cycle of the data:\n\n\n\n\nData frequency\nTypical lag length (p)\n\n\n\n\nHourly\n24\n\n\nDaily\n7\n\n\nMonthly\n12\n\n\nQuarterly\n4\n\n\nAnnual\n1\n\n\n\n\n\nFine as a quick-and-dirty solution. Guidance on this will be domain-specific."
  },
  {
    "objectID": "slides/06-ar-p-inference.html#minimize-an-information-criterion",
    "href": "slides/06-ar-p-inference.html#minimize-an-information-criterion",
    "title": "How do you estimate and tune the AR(p)?",
    "section": "Minimize an information criterion",
    "text": "Minimize an information criterion\nTake:\n\\[\n\\hat{p}=\\argmin{p=1\\com2\\com...\\com\\bar{p}}\\,\\text{IC}(p;\\,\\By_T\\com\\BX_T).\n\\]\n\nCommon choices:\n\nAkaike information criterion: \\(\\text{AIC}(p)=2p+T\\ln(\\hat{\\sigma^2_p})\\);\nBayesian information criterion: \\(\\text{BIC}(p)=p\\ln T+T\\ln(\\hat{\\sigma^2_p})\\).\n\n\n\nThese measure in-sample fit, but out-of-sample performance is more important."
  },
  {
    "objectID": "slides/06-ar-p-inference.html#cross-validation",
    "href": "slides/06-ar-p-inference.html#cross-validation",
    "title": "How do you estimate and tune the AR(p)?",
    "section": "Cross-validation",
    "text": "Cross-validation\n\nDirectly target out-of-sample predictive performance, and tailor selection to the specific predictive task and measure of performance that you care about most in your application;\nIn exchange for being more relevant than the automated in-sample measures, it’s more computationally intensive to implement."
  },
  {
    "objectID": "slides/06-ar-p-inference.html#recall-loo-cv-for-iid-regression",
    "href": "slides/06-ar-p-inference.html#recall-loo-cv-for-iid-regression",
    "title": "How do you estimate and tune the AR(p)?",
    "section": "Recall: LOO-CV for iid regression",
    "text": "Recall: LOO-CV for iid regression\nLeave-one-out cross-validation:\n\n\nFor each model configuration, train on all data except observation \\(i\\), and then test on held-out \\((\\Bx_i\\com y_i)\\);\n\n\n\n\nAverage prediction error over all training/test splits, and pick the best model:\n\n\n\n\\[\n\\hat{\\lambda}=\\argmin{\\lambda\\geq0}\\,\\sum\\limits_{i=1}^n\\left(y_i-\\Bx_i^\\tr\\hat{\\Bbeta}_\\lambda^{(-i)}\\right)^2\n\\]\n\n\n\nOptimizes out-of-sample point prediction accuracy\n\n\n\n\nFine for iid, but with serially dependent time series data, it’s not appropriate to randomly rip observations out of the middle and fit a model to what’s left before and after."
  },
  {
    "objectID": "slides/06-ar-p-inference.html#time-series-cross-validation",
    "href": "slides/06-ar-p-inference.html#time-series-cross-validation",
    "title": "How do you estimate and tune the AR(p)?",
    "section": "Time series cross-validation",
    "text": "Time series cross-validation\nLeave-future-out cross-validation (LFO-CV):\n\\[\n\\begin{aligned}\n&{\\color{blue}\\bullet\\,\\text{training}}\\quad{\\color{red}\\bullet\\,\\text{test}}\\\\\n&\\begin{matrix}\n\\color{blue}{y_1} & \\color{lightgray}{y_2} & \\color{lightgray}{y_3} & \\color{red}{y_4} & \\color{lightgray}{y_5} & \\color{lightgray}{y_6} & \\color{lightgray}{y_7} & \\color{lightgray}{y_8} & \\color{lightgray}{y_9} & \\color{lightgray}{y_{10}}& \\cdots & \\longrightarrow & \\text{error}_1^{(p)} \\\\\n\\color{blue}{y_1} & \\color{blue}{y_2} & \\color{lightgray}{y_3} & \\color{lightgray}{y_4} & \\color{red}{y_5} & \\color{lightgray}{y_6} & \\color{lightgray}{y_7} & \\color{lightgray}{y_8} & \\color{lightgray}{y_9} & \\color{lightgray}{y_{10}}& \\cdots & \\longrightarrow & \\text{error}_2^{(p)} \\\\\n\\color{blue}{y_1} & \\color{blue}{y_2} & \\color{blue}{y_3} & \\color{lightgray}{y_4} & \\color{lightgray}{y_5} & \\color{red}{y_6} & \\color{lightgray}{y_7} & \\color{lightgray}{y_8} & \\color{lightgray}{y_9} & \\color{lightgray}{y_{10}}& \\cdots & \\longrightarrow & \\text{error}_3^{(p)} \\\\\n\\color{blue}{y_1} & \\color{blue}{y_2} & \\color{blue}{y_3} & \\color{blue}{y_4} & \\color{lightgray}{y_5} & \\color{lightgray}{y_6} & \\color{red}{y_7} & \\color{lightgray}{y_8} & \\color{lightgray}{y_9} & \\color{lightgray}{y_{10}}& \\cdots & \\longrightarrow & \\text{error}_4^{(p)} \\\\\n\\color{blue}{y_1} & \\color{blue}{y_2} & \\color{blue}{y_3} & \\color{blue}{y_4} & \\color{blue}{y_5} & \\color{lightgray}{y_6} & \\color{lightgray}{y_7} & \\color{red}{y_8} & \\color{lightgray}{y_9} & \\color{lightgray}{y_{10}}& \\cdots & \\longrightarrow & \\text{error}_5^{(p)} \\\\\n\\color{blue}{y_1} & \\color{blue}{y_2} & \\color{blue}{y_3} & \\color{blue}{y_4} & \\color{blue}{y_5} & \\color{blue}{y_6} & \\color{lightgray}{y_7} & \\color{lightgray}{y_8} & \\color{red}{y_9} & \\color{lightgray}{y_{10}}& \\cdots & \\longrightarrow & \\text{error}_6^{(p)} \\\\\n\\color{blue}{y_1} & \\color{blue}{y_2} & \\color{blue}{y_3} & \\color{blue}{y_4} & \\color{blue}{y_5} & \\color{blue}{y_6} & \\color{blue}{y_7} & \\color{lightgray}{y_8} & \\color{lightgray}{y_9} & \\color{red}{y_{10}}& \\cdots & \\longrightarrow & \\text{error}_7^{(p)}\\\\\n&&&&&\\vdots&&&&&&&\\vdots\\\\\n\\end{matrix}\n\\end{aligned}\n\\]\n\nSelect:\n\\[\n\\hat{p}=\\argmin{p=1\\com2\\com...\\com\\bar{p}}\\,\\sum\\limits_{t=1}^T\\text{error}_t^{(p)}\n\\]"
  },
  {
    "objectID": "slides/06-ar-p-inference.html#tailor-this-to-your-preferred-task",
    "href": "slides/06-ar-p-inference.html#tailor-this-to-your-preferred-task",
    "title": "How do you estimate and tune the AR(p)?",
    "section": "Tailor this to your preferred task",
    "text": "Tailor this to your preferred task\nIf 3-step-ahead point prediction is most important to you:\n\\[\n\\hat{p}=\\argmin{p=1\\com2\\com...\\com\\bar{p}}\\,\\sum\\limits_{t=1}^{T-3}(y_{t+3} - \\hat{y}_{t+3|t}^{(p)})^2\n\\]\nIf 1-step-ahead density prediction is most important to you:\n\\[\n\\hat{p}=\\argmin{p=1\\com2\\com...\\com\\bar{p}}\\,\\sum\\limits_{t=1}^{T-1}-\\ln \\hat{f}_{t+1|t}^{(p)}(y_{t+1})\n\\]\nAnd so on. We are optimizing out-of-sample performance."
  },
  {
    "objectID": "slides/06-ar-p-inference.html#uncertainty",
    "href": "slides/06-ar-p-inference.html#uncertainty",
    "title": "How do you estimate and tune the AR(p)?",
    "section": "Uncertainty?",
    "text": "Uncertainty?\n\n“Hyperparameter tuning” is just a euphemism for estimation;\nThe same data used to estimate \\(\\Bbeta\\) and \\(\\sigma^2\\) are also being used to select \\(p\\);\nThere is estimation uncertainty associated with lag order selection, and ideally this is propagated to the forecast distribution;\n\nkey word: post-selection inference;\n\nIn practice, this is a massive pain, and people seldom attempt it;\nThey select \\(\\hat{p}\\), proceed as if it were fixed and known, accept that their uncertainty quantification is “wrong,” and pray that it’s not too wrong.\n\n\nIf you really want to do this properly, try Bayesian model averaging."
  },
  {
    "objectID": "slides/09-arma-structure.html#exam-1-on-wednesday-october-8",
    "href": "slides/09-arma-structure.html#exam-1-on-wednesday-october-8",
    "title": "ARMA models I",
    "section": "Exam 1 on Wednesday October 8",
    "text": "Exam 1 on Wednesday October 8\n\nyou get both sides of one 8.5” x 11” sheet of notes;\ngrinding through calculations is not the focus;\nfocus on our main themes and visual intuition;\n\nsample paths;\nautocov/cor functions;\nsampling distributions;\nforecast distributions;\nPIT plots."
  },
  {
    "objectID": "slides/09-arma-structure.html#the-pure-autoregression",
    "href": "slides/09-arma-structure.html#the-pure-autoregression",
    "title": "ARMA models I",
    "section": "The pure autoregression",
    "text": "The pure autoregression\n\\[\ny_t\n=\n\\beta_0\n+\n\\sum\\limits_{l=1}^p\\beta_ly_{t-l}\n+\n\\varepsilon_t,\n\\quad\n\\varepsilon_t\n\\iid\n\\N(0\\com \\sigma^2)\n.\n\\]"
  },
  {
    "objectID": "slides/09-arma-structure.html#state-space-form",
    "href": "slides/09-arma-structure.html#state-space-form",
    "title": "ARMA models I",
    "section": "State-space form",
    "text": "State-space form\n\n\n\\[\n\\begin{aligned}\n    y_t&=\\Bf^\\tr\\Bs_t \\\\\n    \\Bs_t&=\\BG\\Bs_{t-1}+\\Bb+\\Beta_t\n\\end{aligned}\n\\]\n\n\\[\n\\begin{aligned}\n    \\Bs_t&=\\begin{bmatrix}y_t & y_{t-1} & \\cdots & y_{t-(p-1)} \\end{bmatrix}^\\tr\\\\\n    \\Bf&=\\begin{bmatrix}1 & 0 & \\cdots & 0\\end{bmatrix}^\\tr\\\\\n    \\BG&=\\begin{bmatrix}\n    \\beta_1 & \\beta_2 & \\cdots & \\beta_{p-1} & \\beta_p \\\\\n    1       & 0       & \\cdots & 0           & 0 \\\\\n    0       & 1       & \\cdots & 0           & 0 \\\\\n    \\vdots  & \\vdots  & \\ddots & \\vdots      & \\vdots \\\\\n    0       & 0       & \\cdots & 1           & 0 \\\\\n    \\end{bmatrix}\\\\\n    \\Beta_t&=\\begin{bmatrix}\\varepsilon_t & 0 & \\cdots & 0\\end{bmatrix}^\\tr\n    \\\\\n    \\Bb\n    &=\n    \\begin{bmatrix}\\beta_0 & 0 & \\cdots & 0\\end{bmatrix}^\\tr\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/09-arma-structure.html#ar2-example",
    "href": "slides/09-arma-structure.html#ar2-example",
    "title": "ARMA models I",
    "section": "AR(2) example",
    "text": "AR(2) example\n\\[\n\\begin{aligned}\ny_t\n&=\n\\begin{bmatrix}\n1 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\ny_t\\\\\ny_{t-1}\n\\end{bmatrix}\n\\\\\n\\begin{bmatrix}\ny_t\\\\\ny_{t-1}\n\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\beta_1 & \\beta_1\\\\\n1 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\ny_{t-1}\\\\\ny_{t-2}\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n\\beta_0\\\\0\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n\\varepsilon_{t}\\\\\n0\n\\end{bmatrix}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/09-arma-structure.html#arp-structure",
    "href": "slides/09-arma-structure.html#arp-structure",
    "title": "ARMA models I",
    "section": "AR(p) structure",
    "text": "AR(p) structure\nThe AR(p) is a Gaussian process, and the moments are given by the [1, 1] elements of\n\\[\n\\begin{aligned}\nE(\\Bs_t)\n&=\n\\mathbf{G}^t \\boldsymbol{\\mu}_0 + \\sum_{k=0}^{t-1} \\mathbf{G}^k \\mathbf{b}\n\\\\\n\\cov(\\Bs_t)\n&=\n\\BG^t\\BP_0(\\BG^\\tr)^t+\\sum\\limits_{k=0}^{t-1}\\BG^k\\BW(\\BG^\\tr)^k\n\\\\\n\\cov(\\Bs_t\\com\\Bs_s)\n&=\n\\BG^{|t-s|}\\cov(\\Bs_{\\min\\{s\\com t\\}}).\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/09-arma-structure.html#special-case-ar1",
    "href": "slides/09-arma-structure.html#special-case-ar1",
    "title": "ARMA models I",
    "section": "Special case: AR(1)",
    "text": "Special case: AR(1)\n\\[\n\\begin{aligned}\nE(y_t)\n&=\n\\beta_0\\sum\\limits_{k=0}^{t-1}\\beta_1^k\n+\n\\beta_1^t\\mu_0\n\\\\\n\\var(y_t)\n&=\n\\sigma^2\n\\sum\\limits_{k=0}^{t-1}\\beta_1^{2k}+\n\\beta_1^{2t}\\initvar\n\\\\\n\\cov(y_t\\com y_s)\n&=\n\\beta_1^{|s-t|}\\var(y_{\\min\\{s\\com t\\}})\n.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/09-arma-structure.html#stationarity",
    "href": "slides/09-arma-structure.html#stationarity",
    "title": "ARMA models I",
    "section": "Stationarity",
    "text": "Stationarity\nIf \\(\\BG\\) has eigenvalues inside the unit circle, then the AR(p) is strictly stationary. This means\n\\[\n\\{y_{t_1}\\com y_{t_2}\\com ...\\com y_{t_n}\\}\\overset{d}{=}\\{y_{t_1+h}\\com y_{t_2+h}\\com ...\\com y_{t_n+h}\\}.\n\\]\nConsequences:\n\n\n\\(E(y_t)=\\mu\\) for all \\(t\\);\n\n\\(\\var(y_t)=\\gamma(0)\\) for all \\(t\\)\n\n\n\\(\\cov(y_{t+h}\\com y_t)=\\gamma(h)\\) for all \\(t\\)."
  },
  {
    "objectID": "slides/09-arma-structure.html#autocovariance",
    "href": "slides/09-arma-structure.html#autocovariance",
    "title": "ARMA models I",
    "section": "Autocovariance",
    "text": "Autocovariance\nFor the AR(1), we know:\n\\[\n\\gamma(h)=\\beta_1^{|h|}\\gamma(0).\n\\]\nFor the general AR(p), we have the Yule-Walker equations:\n\\[\n\\begin{aligned}\n\\gamma(k) &= \\sum_{l=1}^p \\beta_l \\, \\gamma(k-l) && k\\geq 1.\n\\\\\n\\gamma(0) &= \\sum_{l=1}^p \\beta_l \\, \\gamma(l) + \\sigma^2\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/09-arma-structure.html#autocorrelation",
    "href": "slides/09-arma-structure.html#autocorrelation",
    "title": "ARMA models I",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\\[\n\\begin{aligned}\n\\rho(h)\n=\\frac{\\gamma(h)}{\\gamma(0)}\n&=\n\\frac{\\cov(y_{t+h}\\com y_t)}{\\var(y_t)}\n\\\\\n&=\n\\frac{\\cov(y_{t+h}\\com y_t)}{\\text{sd}(y_t)\\text{sd}(y_t)}\n\\\\\n&=\n\\frac{\\cov(y_{t+h}\\com y_t)}{\\text{sd}(y_{t+h})\\text{sd}(y_t)}\n\\\\\n&=\n\\text{cor}(y_{t+h}\\com y_t).\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/09-arma-structure.html#autocorrelation-functions",
    "href": "slides/09-arma-structure.html#autocorrelation-functions",
    "title": "ARMA models I",
    "section": "Autocorrelation functions",
    "text": "Autocorrelation functions\n\nplot_arma_acf(ar = c(0.7), lag.max = 20)"
  },
  {
    "objectID": "slides/09-arma-structure.html#autocorrelation-functions-1",
    "href": "slides/09-arma-structure.html#autocorrelation-functions-1",
    "title": "ARMA models I",
    "section": "Autocorrelation functions",
    "text": "Autocorrelation functions\n\nplot_arma_acf(ar = c(-0.7), lag.max = 20)"
  },
  {
    "objectID": "slides/09-arma-structure.html#autocorrelation-functions-2",
    "href": "slides/09-arma-structure.html#autocorrelation-functions-2",
    "title": "ARMA models I",
    "section": "Autocorrelation functions",
    "text": "Autocorrelation functions\n\nplot_arma_acf(ar = c(-0.9), lag.max = 20)"
  },
  {
    "objectID": "slides/09-arma-structure.html#autocorrelation-functions-3",
    "href": "slides/09-arma-structure.html#autocorrelation-functions-3",
    "title": "ARMA models I",
    "section": "Autocorrelation functions",
    "text": "Autocorrelation functions\n\nplot_arma_acf(ar = c(.5, .4), lag.max = 20)"
  },
  {
    "objectID": "slides/09-arma-structure.html#autocorrelation-functions-4",
    "href": "slides/09-arma-structure.html#autocorrelation-functions-4",
    "title": "ARMA models I",
    "section": "Autocorrelation functions",
    "text": "Autocorrelation functions\n\nplot_arma_acf(ar = c(1, -.8), lag.max = 20)"
  },
  {
    "objectID": "slides/09-arma-structure.html#autocorrelation-functions-5",
    "href": "slides/09-arma-structure.html#autocorrelation-functions-5",
    "title": "ARMA models I",
    "section": "Autocorrelation functions",
    "text": "Autocorrelation functions\n\nplot_arma_acf(ar = c(-1, -.8), lag.max = 20)"
  },
  {
    "objectID": "slides/09-arma-structure.html#arp-inference",
    "href": "slides/09-arma-structure.html#arp-inference",
    "title": "ARMA models I",
    "section": "AR(p) inference",
    "text": "AR(p) inference\n\nAssuming stationarity: Yule-Walker and method of moments;\nWithout stationarity: MLE and OLS;\nAlso: conjugate Bayesian inference."
  },
  {
    "objectID": "slides/09-arma-structure.html#arp-forecasting",
    "href": "slides/09-arma-structure.html#arp-forecasting",
    "title": "ARMA models I",
    "section": "AR(p) forecasting",
    "text": "AR(p) forecasting\n\nIf you already know the parameters, the forecast distributions \\(p(y_{t+h}\\mid y_{0:t})\\) are Gaussian with means and variances given above;\nTreating point estimates as fixed and known underestimates uncertainty;\nTo get proper uncertainty quantification, you need the bootstrap or a Bayesian approach."
  },
  {
    "objectID": "slides/09-arma-structure.html#the-pure-moving-average",
    "href": "slides/09-arma-structure.html#the-pure-moving-average",
    "title": "ARMA models I",
    "section": "The pure moving average",
    "text": "The pure moving average\n\\[\ny_t\n=\n\\sum_{i=1}^q\\theta_i\\varepsilon_{t-i}\n+\n\\varepsilon_t,\n\\quad\n\\varepsilon_t\n\\iid\n\\N(0\\com \\sigma^2)\n.\n\\]"
  },
  {
    "objectID": "slides/09-arma-structure.html#maq-structure",
    "href": "slides/09-arma-structure.html#maq-structure",
    "title": "ARMA models I",
    "section": "MA(q) structure",
    "text": "MA(q) structure\nThe MA(q) is a strictly stationary Gaussian process, and for all \\(t\\):\n\\[\n\\begin{aligned}\ny_t&\\sim\\N\\left(0\\com \\sigma^2 \\left( 1 + \\sum_{i=1}^q \\theta_i^2 \\right)\\right)\n\\\\\n\\gamma(h) &=\n\\begin{cases}\n\\sigma^2 \\sum_{i=0}^{q-|h|} \\theta_i \\theta_{i+|h|}, & 0 \\le |h| \\le q,\\\\[1mm]\n0, & |h| &gt; q.\n\\end{cases}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/09-arma-structure.html#autocorrelation-functions-6",
    "href": "slides/09-arma-structure.html#autocorrelation-functions-6",
    "title": "ARMA models I",
    "section": "Autocorrelation functions",
    "text": "Autocorrelation functions\n\nplot_arma_acf(ma = c(0.5), lag.max = 20)"
  },
  {
    "objectID": "slides/09-arma-structure.html#autocorrelation-functions-7",
    "href": "slides/09-arma-structure.html#autocorrelation-functions-7",
    "title": "ARMA models I",
    "section": "Autocorrelation functions",
    "text": "Autocorrelation functions\n\nplot_arma_acf(ma = c(-0.5), lag.max = 20)"
  },
  {
    "objectID": "slides/09-arma-structure.html#autocorrelation-functions-8",
    "href": "slides/09-arma-structure.html#autocorrelation-functions-8",
    "title": "ARMA models I",
    "section": "Autocorrelation functions",
    "text": "Autocorrelation functions\n\nplot_arma_acf(ma = c(-0.9), lag.max = 20)"
  },
  {
    "objectID": "slides/09-arma-structure.html#autocorrelation-functions-9",
    "href": "slides/09-arma-structure.html#autocorrelation-functions-9",
    "title": "ARMA models I",
    "section": "Autocorrelation functions",
    "text": "Autocorrelation functions\n\nplot_arma_acf(ma = c(0.4, -0.6), lag.max = 20)"
  },
  {
    "objectID": "slides/09-arma-structure.html#maq-inference",
    "href": "slides/09-arma-structure.html#maq-inference",
    "title": "ARMA models I",
    "section": "MA(q) inference?",
    "text": "MA(q) inference?\nWe worked a toy method-of-moments example for the MA(1), but beyond that, the algebra is heinous. Take a likelihood-based approach using state-space methods."
  },
  {
    "objectID": "slides/09-arma-structure.html#the-autoregressive-moving-average-model",
    "href": "slides/09-arma-structure.html#the-autoregressive-moving-average-model",
    "title": "ARMA models I",
    "section": "The autoregressive moving average model",
    "text": "The autoregressive moving average model\n\\[\ny_t\n=\n\\beta_0\n+\n\\underbrace{\\sum\\limits_{l=1}^p\\beta_ly_{t-l}}_{\\text{autoregressive}}\n+\n\\underbrace{\\sum_{i=1}^q\\theta_i\\varepsilon_{t-i}}_{\\text{moving average}}\n+\n\\varepsilon_t,\n\\quad\n\\varepsilon_t\n\\iid\n\\N(0\\com \\sigma^2)\n.\n\\]\n\nCan’t do anything analytically:\n\nMLE requires numerical optimization;\nBayes requires ugly MCMC;\nBootstrapping is a pain;\nModel combination is probably not worth it."
  },
  {
    "objectID": "slides/09-arma-structure.html#autocorrelation-function",
    "href": "slides/09-arma-structure.html#autocorrelation-function",
    "title": "ARMA models I",
    "section": "Autocorrelation function",
    "text": "Autocorrelation function\n\nplot_arma_acf(ar = c(0.6), lag.max = 20)"
  },
  {
    "objectID": "slides/09-arma-structure.html#autocorrelation-function-1",
    "href": "slides/09-arma-structure.html#autocorrelation-function-1",
    "title": "ARMA models I",
    "section": "Autocorrelation function",
    "text": "Autocorrelation function\n\nplot_arma_acf(ar = c(0.6), ma = c(-0.5), lag.max = 20)"
  },
  {
    "objectID": "slides/09-arma-structure.html#autocorrelation-function-2",
    "href": "slides/09-arma-structure.html#autocorrelation-function-2",
    "title": "ARMA models I",
    "section": "Autocorrelation function",
    "text": "Autocorrelation function\n\nplot_arma_acf(ar = c(0.9), ma = c(0.9), lag.max = 20)"
  },
  {
    "objectID": "slides/09-arma-structure.html#autocorrelation-function-3",
    "href": "slides/09-arma-structure.html#autocorrelation-function-3",
    "title": "ARMA models I",
    "section": "Autocorrelation function",
    "text": "Autocorrelation function\n\nplot_arma_acf(ar = c(0.7), ma = c(0.5), lag.max = 20)"
  },
  {
    "objectID": "slides/09-arma-structure.html#autocorrelation-function-4",
    "href": "slides/09-arma-structure.html#autocorrelation-function-4",
    "title": "ARMA models I",
    "section": "Autocorrelation function",
    "text": "Autocorrelation function\n\nplot_arma_acf(ar = c(0.7), ma = c(0), lag.max = 20)"
  },
  {
    "objectID": "slides/09-arma-structure.html#autocorrelation-function-5",
    "href": "slides/09-arma-structure.html#autocorrelation-function-5",
    "title": "ARMA models I",
    "section": "Autocorrelation function",
    "text": "Autocorrelation function\n\nplot_arma_acf(ar = c(0.6, -0.3), ma = c(0.4), lag.max = 20)"
  },
  {
    "objectID": "slides/09-arma-structure.html#autocorrelation-function-6",
    "href": "slides/09-arma-structure.html#autocorrelation-function-6",
    "title": "ARMA models I",
    "section": "Autocorrelation function",
    "text": "Autocorrelation function\n\nplot_arma_acf(ar = c(0.8), ma = c(-0.5, 0.3), lag.max = 20)"
  },
  {
    "objectID": "slides/08-moving-average.html#the-autoregressive-model",
    "href": "slides/08-moving-average.html#the-autoregressive-model",
    "title": "The moving average model",
    "section": "The autoregressive model",
    "text": "The autoregressive model\n\\[\ny_t\n=\n\\beta_0\n+\n\\sum\\limits_{l=1}^p\\beta_ly_{t-l}\n+\n\\varepsilon_t,\n\\quad\n\\varepsilon_t\n\\iid\n\\N(0\\com \\sigma^2)\n.\n\\]\n\nThis implies a Gaussian distribution with funky mean and covariance;\nNests iid, random walk, random walk with drift, stationary, and explosive;\n(Conditional) likelihood based inference is the same as iid regression: OLS, conjugate Bayes, etc;\n\nWe can make sense of both theoretically;\n\nProbabilisitic prediction is not the same as iid regression;\n\nBootstrap for classical, posterior predictive for Bayes;\n\n\\(p\\) is a tuning parameter that must be selected or averaged over."
  },
  {
    "objectID": "slides/08-moving-average.html#main-ideas",
    "href": "slides/08-moving-average.html#main-ideas",
    "title": "The moving average model",
    "section": "Main ideas",
    "text": "Main ideas\n\nA time series model is “just” a joint distribution for a dependent sequence, and you understand the model if you understand the structure of that distribution (family, moments, marginals, conditionals, etc);\nEstimation should be recursive to confront the reality of streaming data;\nPredictions should be probabilistic and incorporate many sources of uncertainty;\nModel selection criteria should be out-of-sample and tailored to your specific prediction/decision task (eg LFO-CV);\nSelection is second best to combination, but combination can be tough to implement, and the size of the gains will depend on the application;\nA Bayesian approach makes recursive estimation, probabilistic prediction, and coherent model combination straightforward (note: I didn’t say easy)."
  },
  {
    "objectID": "slides/08-moving-average.html#why-must-the-errors-be-independent",
    "href": "slides/08-moving-average.html#why-must-the-errors-be-independent",
    "title": "The moving average model",
    "section": "Why must the errors be independent?",
    "text": "Why must the errors be independent?\nWe have seen this:\n\\[\ny_t\n=\n\\beta_0\n+\n\\sum\\limits_{l=1}^p\\beta_ly_{t-l}\n+\n\\underbrace{\nu_t\n}_{\\text{iid}}\n.\n\\]\nWhat if there is time series dependence in \\(u_t\\) as well?\n\nWhat kind?"
  },
  {
    "objectID": "slides/08-moving-average.html#autoregressive-moving-average-arma",
    "href": "slides/08-moving-average.html#autoregressive-moving-average-arma",
    "title": "The moving average model",
    "section": "Autoregressive moving average (ARMA)",
    "text": "Autoregressive moving average (ARMA)\nARMA(\\(p\\), \\(q\\)):\n\\[\ny_t\n=\n\\beta_0\n+\n\\underbrace{\\sum\\limits_{l=1}^p\\beta_ly_{t-l}}_{\\text{autoregressive}}\n+\n\\underbrace{\\sum_{i=1}^q\\theta_i\\varepsilon_{t-i}}_{\\text{moving average}}\n+\n\\varepsilon_t,\n\\quad\n\\varepsilon_t\n\\iid\n\\N(0\\com \\sigma^2)\n.\n\\]\n\nFlexible class of models that can capture pretty generic linear dependencies.\n\n\\(p=q=0\\) gives the iid model;\n\\(q=0\\) gives the pure AR(\\(p\\)) you’re already sick of;\n\\(p=0\\) gives the pure MA(\\(q\\)), which we study today."
  },
  {
    "objectID": "slides/08-moving-average.html#dynamic-linear-model-dlm",
    "href": "slides/08-moving-average.html#dynamic-linear-model-dlm",
    "title": "The moving average model",
    "section": "Dynamic linear model (DLM)",
    "text": "Dynamic linear model (DLM)\n\\(\\By_t\\in\\RR^n\\) is observed and \\(\\Bs_t\\in\\RR^p\\) is latent:\n\\[\n\\begin{aligned}\n\\By_t\n&=\n\\BF\\Bs_t\n+\n\\Bepsilon_t,\n&&\n\\Bepsilon_t\\iid\\N_n(\\Bzero\\com \\BV)\n\\\\\n\\Bs_t\n&=\n\\BG\\Bs_{t-1}\n+\n\\Beta_t,\n&&\n\\Beta_t\\iid\\N_p(\\Bzero\\com \\BW)\n\\\\\n& &&\\\\\n\\Bs_0&\\sim\\N_p(\\bar{\\Bs}_{0|0}\\com \\BP_0).\n\\end{aligned}\n\\]\nARMA and several other classes of models are just special cases of this. Once you understand DLMs and the Kalman filter, it all trickles down."
  },
  {
    "objectID": "slides/08-moving-average.html#the-maq",
    "href": "slides/08-moving-average.html#the-maq",
    "title": "The moving average model",
    "section": "The MA(q)",
    "text": "The MA(q)\nMoving average of order \\(q\\):\n\\[\ny_t\n=\n\\sum_{i=1}^q\\theta_i\\varepsilon_{t-i}\n+\n\\varepsilon_t,\n\\quad\n\\varepsilon_t\n\\iid\n\\N(0\\com \\sigma^2)\n.\n\\]\nDoesn’t necessarily describe a tremendous amount of “real” time series. Combine it with the AR piece, and things get interesting."
  },
  {
    "objectID": "slides/08-moving-average.html#what-is-the-joint-distribution",
    "href": "slides/08-moving-average.html#what-is-the-joint-distribution",
    "title": "The moving average model",
    "section": "What is the joint distribution?",
    "text": "What is the joint distribution?\nNote that\n\\[\n\\begin{aligned}\n\\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_T\\end{bmatrix}\n&=\n\\begin{bmatrix}\n\\theta_q & \\theta_{q-1} & \\cdots & \\theta_1 & 1 & 0 & 0 & \\cdots & 0 \\\\\n0 & \\theta_q & \\theta_{q-1} & \\cdots & \\theta_1 & 1 & 0 & \\cdots & 0 \\\\\n0 & 0 & \\theta_q & \\theta_{q-1} & \\cdots & \\theta_1 & 1 & \\cdots & 0 \\\\\n\\vdots & & & \\ddots & & & & \\ddots & \\vdots \\\\\n0 & \\cdots & 0 & 0 & \\theta_q & \\theta_{q-1} & \\cdots & \\theta_1 & 1\n\\end{bmatrix}\n\\begin{bmatrix}\n\\varepsilon_{1-q} \\\\[1mm]\n\\varepsilon_{2-q} \\\\[1mm]\n\\vdots \\\\[1mm]\n\\varepsilon_0 \\\\[1mm]\n\\varepsilon_1 \\\\[1mm]\n\\vdots \\\\[1mm]\n\\varepsilon_T\n\\end{bmatrix}\n\\\\\n\\By&=\\BA\\boldsymbol{\\varepsilon}\n.\n\\end{aligned}\n\\]\nWe know \\(\\boldsymbol{\\varepsilon}\\sim\\N_{T+q}(\\Bzero\\com\\sigma^2\\BI_n)\\), so \\(\\By\\) is Gaussian by affine transformation."
  },
  {
    "objectID": "slides/08-moving-average.html#what-is-the-mean",
    "href": "slides/08-moving-average.html#what-is-the-mean",
    "title": "The moving average model",
    "section": "What is the mean?",
    "text": "What is the mean?\nEasy:\n\\[\n\\begin{aligned}\nE(y_t)\n&=\nE\\left(\n\\sum_{i=1}^q\\theta_i\\varepsilon_{t-i}\n+\n\\varepsilon_t\n\\right)\n\\\\\n&=\n\\sum_{i=1}^q\\theta_iE\\left(\\varepsilon_{t-i}\\right)\n+\nE\\left(\n\\varepsilon_t\n\\right)\n\\\\\n&=\n\\sum_{i=1}^q\\theta_i\\cdot0\n+\n0\n\\\\\n&=0.\n\\end{aligned}\n\\]\n\nAlways zero! No matter \\(q\\) or the parameters or anything. Didn’t use normality, either."
  },
  {
    "objectID": "slides/08-moving-average.html#what-is-the-variance",
    "href": "slides/08-moving-average.html#what-is-the-variance",
    "title": "The moving average model",
    "section": "What is the variance?",
    "text": "What is the variance?\nSince the \\(\\varepsilon_t\\) are uncorrelated:\n\\[\n\\begin{aligned}\n\\operatorname{var}(y_t)\n    &= \\operatorname{var}\\big(\\varepsilon_t + \\theta_1 \\varepsilon_{t-1} + \\cdots + \\theta_q \\varepsilon_{t-q}\\big) \\\\[0.5em]\n    &= \\operatorname{var}(\\varepsilon_t) + \\theta_1^2 \\operatorname{var}(\\varepsilon_{t-1}) + \\cdots + \\theta_q^2 \\operatorname{var}(\\varepsilon_{t-q}) \\\\[0.5em]\n      &= \\sigma^2 + \\theta_1^2 \\sigma^2 + \\cdots + \\theta_q^2 \\sigma^2 \\\\[0.5em]\n    &= \\sigma^2 \\left( 1 + \\theta_1^2 + \\theta_2^2 + \\cdots + \\theta_q^2 \\right) \\\\[0.5em]\n    &= \\sigma^2 \\left( 1 + \\sum_{i=1}^q \\theta_i^2 \\right).\n\\end{aligned}\n\\]\nDoesn’t depend on \\(t\\). Interesting…"
  },
  {
    "objectID": "slides/08-moving-average.html#great-tool",
    "href": "slides/08-moving-average.html#great-tool",
    "title": "The moving average model",
    "section": "Great tool!",
    "text": "Great tool!\n\\[\n\\operatorname{cov}\\!\\left(\\sum_{i=1}^n a_i X_i,\\; \\sum_{j=1}^m b_j Y_j\\right)\n= \\sum_{i=1}^n \\sum_{j=1}^m a_i b_j \\,\\operatorname{cov}(X_i, Y_j).\n\\]"
  },
  {
    "objectID": "slides/08-moving-average.html#covariance-of-the-ma1",
    "href": "slides/08-moving-average.html#covariance-of-the-ma1",
    "title": "The moving average model",
    "section": "Covariance of the MA(1)",
    "text": "Covariance of the MA(1)\n\\[\n\\begin{aligned}\n\\operatorname{cov}(y_t, y_{t-1})\n    &= \\operatorname{cov}(\\varepsilon_t + \\theta \\varepsilon_{t-1}, \\varepsilon_{t-1} + \\theta \\varepsilon_{t-2}) \\\\[0.5em]\n    &= \\operatorname{cov}(\\varepsilon_t, \\varepsilon_{t-1}) + \\theta \\operatorname{cov}(\\varepsilon_t, \\varepsilon_{t-2})\n       + \\theta \\operatorname{cov}(\\varepsilon_{t-1}, \\varepsilon_{t-1}) + \\theta^2 \\operatorname{cov}(\\varepsilon_{t-1}, \\varepsilon_{t-2}) \\\\[0.5em]\n    &= 0+0+\\theta \\sigma^2+\\theta^2\\cdot 0.\n\\end{aligned}\n\\] Doesn’t depend on \\(t\\)."
  },
  {
    "objectID": "slides/08-moving-average.html#covariance-of-the-ma1-1",
    "href": "slides/08-moving-average.html#covariance-of-the-ma1-1",
    "title": "The moving average model",
    "section": "Covariance of the MA(1)",
    "text": "Covariance of the MA(1)\n\\[\n\\begin{aligned}\n\\operatorname{cov}(y_t, y_{t-2})\n    &= \\operatorname{cov}(\\varepsilon_t + \\theta \\varepsilon_{t-1}, \\varepsilon_{t-2} + \\theta \\varepsilon_{t-3}) \\\\[0.5em]\n    &= \\operatorname{cov}(\\varepsilon_t, \\varepsilon_{t-3}) + \\theta \\operatorname{cov}(\\varepsilon_t, \\varepsilon_{t-3})\n       + \\theta \\operatorname{cov}(\\varepsilon_{t-1}, \\varepsilon_{t-2}) + \\theta^2 \\operatorname{cov}(\\varepsilon_{t-1}, \\varepsilon_{t-3}) \\\\[0.5em]\n    &= 0+0+\\theta \\cdot0+\\theta^2\\cdot 0.\n\\end{aligned}\n\\] Doesn’t depend on \\(t\\)."
  },
  {
    "objectID": "slides/08-moving-average.html#autocovariance-of-the-ma1",
    "href": "slides/08-moving-average.html#autocovariance-of-the-ma1",
    "title": "The moving average model",
    "section": "Autocovariance of the MA(1)",
    "text": "Autocovariance of the MA(1)\nFor any \\(t\\), we have:\n\\[\n\\gamma(h) = \\operatorname{cov}(y_t, y_{t-h}) =\n\\begin{cases}\n\\sigma^2 (1 + \\theta^2), & h = 0,\\\\[2mm]\n\\theta \\, \\sigma^2, & |h| = 1,\\\\[1mm]\n0, & |h| &gt; 1.\n\\end{cases}\n\\]\nMA(1) is Gaussian with time-invariant mean and variance, and shift-invariant covariance. Stationary!"
  },
  {
    "objectID": "slides/08-moving-average.html#autocovariance-of-the-ma2",
    "href": "slides/08-moving-average.html#autocovariance-of-the-ma2",
    "title": "The moving average model",
    "section": "Autocovariance of the MA(2)",
    "text": "Autocovariance of the MA(2)\nFor any \\(t\\), we have:\n\\[\n\\gamma(h) =\n\\begin{cases}\n\\sigma^2 \\big(1 + \\theta_1^2 + \\theta_2^2 \\big), & h = 0,\\\\[1mm]\n\\sigma^2 \\big(\\theta_1 + \\theta_1 \\theta_2 \\big), & |h| = 1,\\\\[1mm]\n\\sigma^2 \\theta_2, & |h| = 2,\\\\[1mm]\n0, & |h| &gt; 2.\n\\end{cases}\n\\]\n\nProblem Set 1!"
  },
  {
    "objectID": "slides/08-moving-average.html#autocovariance-of-the-maq",
    "href": "slides/08-moving-average.html#autocovariance-of-the-maq",
    "title": "The moving average model",
    "section": "Autocovariance of the MA(q)",
    "text": "Autocovariance of the MA(q)\nSet \\(\\theta_0=1\\). Then for any \\(t\\), we have:\n\\[\n\\gamma(h) =\n\\begin{cases}\n\\sigma^2 \\sum_{i=0}^{q} \\theta_i^2, & h = 0,\\\\[1mm]\n\\sigma^2 \\sum_{i=0}^{q-|h|} \\theta_i \\theta_{i+|h|}, & 1 \\le |h| \\le q,\\\\[1mm]\n0, & |h| &gt; q.\n\\end{cases}\n\\] Across the board no matter \\(q\\) or the parameters."
  },
  {
    "objectID": "slides/08-moving-average.html#summary",
    "href": "slides/08-moving-average.html#summary",
    "title": "The moving average model",
    "section": "Summary",
    "text": "Summary\n\\[\ny_t\n=\n\\sum_{i=1}^q\\theta_i\\varepsilon_{t-i}\n+\n\\varepsilon_t,\n\\quad\n\\varepsilon_t\n\\iid\n\\N(0\\com \\sigma^2)\n.\n\\]\nThe MA(q) is a strictly stationary Gaussian process, and for all \\(t\\):\n\\[\n\\begin{aligned}\ny_t&\\sim\\N\\left(0\\com \\sigma^2 \\left( 1 + \\sum_{i=1}^q \\theta_i^2 \\right)\\right)\n\\\\\n\\gamma(h) &=\n\\begin{cases}\n\\sigma^2 \\sum_{i=0}^{q-|h|} \\theta_i \\theta_{i+|h|}, & 0 \\le |h| \\le q,\\\\[1mm]\n0, & |h| &gt; q.\n\\end{cases}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/08-moving-average.html#what-does-this-look-like",
    "href": "slides/08-moving-average.html#what-does-this-look-like",
    "title": "The moving average model",
    "section": "What does this look like?",
    "text": "What does this look like?\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\nlibrary(shiny)\n\nlibrary(shiny)\n\n# --- Simulation of MA(3) ---\nsimulate_ma_3 &lt;- function(T, theta1, theta2, theta3, sigma){\n  y &lt;- numeric(T)\n  eps &lt;- rnorm(T + 3, 0, sigma)  # extra for lags\n  for(t in 1:T){\n    y[t] &lt;- eps[t+3] + theta1*eps[t+2] + theta2*eps[t+1] + theta3*eps[t]\n  }\n  return(y)\n}\n\n# --- Theoretical autocovariance function of MA(3) ---\nma3_acov &lt;- function(h, theta1, theta2, theta3, sigma2){\n  if(h == 0){\n    return(sigma2 * (1 + theta1^2 + theta2^2 + theta3^2))\n  } else if(h == 1){\n    return(sigma2 * (theta1 + theta1*theta2 + theta2*theta3))\n  } else if(h == 2){\n    return(sigma2 * (theta2 + theta1*theta3))\n  } else if(h == 3){\n    return(sigma2 * theta3)\n  } else {\n    return(0)\n  }\n}\n\n# --- UI ---\nui &lt;- fluidPage(\n  \n  titlePanel(\"MA(3): Sample paths and autocovariance function\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"theta1\", \"θ₁\",\n                  min = -2, max = 2, value = 0, step = 0.1),\n      sliderInput(\"theta2\", \"θ₂\",\n                  min = -2, max = 2, value = 0, step = 0.1),\n      sliderInput(\"theta3\", \"θ₃\",\n                  min = -2, max = 2, value = 0, step = 0.1),\n      sliderInput(\"sigma\", \"σ\",\n                  min = 0, max = 2, value = 1, step = 0.1),\n      sliderInput(\"T\", \"T\",\n                  min = 20, max = 200, step = 20, value = 100),\n      actionButton(\"redo\", \"New sample path\")\n    ),\n    \n    mainPanel(\n      plotOutput(\"pathPlot\", height = \"300px\"),\n      plotOutput(\"acovPlot\", height = \"300px\")\n    )\n  )\n)\n\n# --- Server ---\nserver &lt;- function(input, output) {\n  \n  output$pathPlot &lt;- renderPlot({\n    input$redo\n\n      T &lt;- input$T\n      y &lt;- simulate_ma_3(T, input$theta1, input$theta2, input$theta3, input$sigma)\n      \n      # Marginal mean and sd (stationary, constant in t)\n      mean_y &lt;- 0\n      sd_y &lt;- sqrt(ma3_acov(0, input$theta1, input$theta2, input$theta3, input$sigma^2))\n      \n      range &lt;- 1:T\n      alpha &lt;- c(0.01, seq(0.1, 0.9, by = 0.1))\n      \n      plot(range, rep(0, T), type = \"l\",\n           xaxt = \"n\", yaxt = \"n\",\n           xlab = \"t\", ylab = expression(y[t]),\n           ylim = c(-20, 20), bty = \"n\",\n           col = \"white\",\n           main = \"Simulated MA(3) Sample Path with Marginal Distribution\")\n      \n      for(a in alpha){\n        U &lt;- qnorm(1 - a/2, mean = mean_y, sd = sd_y)\n        L &lt;- qnorm(a/2, mean = mean_y, sd = sd_y)\n        polygon(\n          c(range, rev(range)),\n          c(rep(U, T), rev(rep(L, T))),\n          col = rgb(1, 0, 0, 0.15),\n          border = NA\n        )\n      }\n      \n      axis(1, pos = 0)\n      axis(2, pos = 0)\n      \n      lines(range, y, col = \"black\", lwd = 2)\n  })\n  \n  output$acovPlot &lt;- renderPlot({\n    h_max &lt;- 10\n    h_vals &lt;- -h_max:h_max\n    gamma_vals &lt;- sapply(h_vals, function(h) \n      ma3_acov(abs(h), input$theta1, input$theta2, input$theta3, input$sigma^2))\n    \n    plot(h_vals, gamma_vals, type = \"h\", lwd = 2,\n         main = \"Theoretical Autocovariance Function γ(h)\",\n         xlab = \"Lag h\", ylab = expression(gamma(h)))\n    points(h_vals, gamma_vals, pch = 19)\n  })\n}\n\n# --- Run App ---\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "slides/08-moving-average.html#consider-an-ma1",
    "href": "slides/08-moving-average.html#consider-an-ma1",
    "title": "The moving average model",
    "section": "Consider an MA(1)",
    "text": "Consider an MA(1)\nIf \\(\\theta_1=0.5\\) and \\(\\sigma^2=1\\), then\n\n\\[\n\\begin{aligned}\n\\gamma(0)\n&=\n\\sigma^2(1+\\theta_1^2)\n=\n1+0.5^2=1.25\n\\\\\n\\gamma(1)\n&=\n\\sigma^2\\theta_1\n=\n0.5\n.\n\\end{aligned}\n\\]\n\n\nBut if \\(\\theta_1=2\\) and \\(\\sigma^2=.25\\), then\n\n\n\\[\n\\begin{aligned}\n\\gamma(0)\n&=\n0.25(1+4)\n=\n1.25\n\\\\\n\\gamma(1)\n&=\n0.25\\times 2\n=\n0.5\n.\n\\end{aligned}\n\\]\n\n\nUh oh!"
  },
  {
    "objectID": "slides/08-moving-average.html#identification",
    "href": "slides/08-moving-average.html#identification",
    "title": "The moving average model",
    "section": "Identification",
    "text": "Identification\n\nIn general, the parameters \\((\\theta_1\\com\\sigma^2)\\) are not identified;\nThere are several values that give you the same joint distribution for \\(y_t\\);\nGiven a data set from (.5, 1), even if \\(T\\) were literally \\(\\infty\\), you won’t be able to tell if these data were generated by (.5, 1) or (2, .25). They are observationally-equivalent.\nFor the MA(1) we impose a restriction that \\(|\\theta_1|&lt;1\\);\nWe’ll revisit MA(q) later."
  },
  {
    "objectID": "slides/08-moving-average.html#comments",
    "href": "slides/08-moving-average.html#comments",
    "title": "The moving average model",
    "section": "Comments",
    "text": "Comments\n\nEstimating a pure MA(q) in isolation is probably not super useful;\nIn general, state space methods and the Kalman filter are absolutely the way to go;\nBut for purposes of illustration, let’s work through a simple method-of-moments calculation for the MA(1)."
  },
  {
    "objectID": "slides/08-moving-average.html#estimating-equations",
    "href": "slides/08-moving-average.html#estimating-equations",
    "title": "The moving average model",
    "section": "Estimating equations",
    "text": "Estimating equations\nWe know:\n\\[\n\\begin{aligned}\n\\gamma(1) &\\;=\\; \\theta_1\\sigma^2\\\\\n\\gamma(0) &\\;=\\; (1+\\theta_1^2)\\sigma^2=\\sigma^2+\\theta_1^2\\sigma^2. \\\\\n\\end{aligned}\n\\]\n\nTo solve, notice that\n\\[\n\\begin{aligned}\n\\gamma(1)\\theta_1^2-\\gamma(0)\\theta_1+\\gamma(1)=0\\\\\n\\end{aligned}\n\\]\n\n\nSo\n\\[\n\\begin{aligned}\n\\theta_1&=\\frac{\\gamma(0){\\color{red}\\pm}\\sqrt{\\gamma(0)^2-4\\gamma(1)^2}}{2\\gamma(1)}\n\\\\\n\\sigma^2&=\n\\frac{\\gamma(1)}{\\theta_1}\n=\n\\frac{\\gamma(0)}{1+\\theta_1^2}.\n\\end{aligned}\n\\]\nBy convention, we pick whichever solution that is between -1 and 1."
  },
  {
    "objectID": "slides/08-moving-average.html#sample-averages",
    "href": "slides/08-moving-average.html#sample-averages",
    "title": "The moving average model",
    "section": "Sample averages",
    "text": "Sample averages\n\\[\n\\begin{aligned}\n\\hat{\\gamma}(0) &= \\frac{1}{T}\\sum_{t=1}^{T} (y_t - \\bar{y})^2,\\\\\n\\hat{\\gamma}(1) &= \\frac{1}{T}\\sum_{t=2}^{T} (y_t - \\bar{y})(y_{t-1} - \\bar{y})\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/08-moving-average.html#method-of-moments",
    "href": "slides/08-moving-average.html#method-of-moments",
    "title": "The moving average model",
    "section": "Method of moments",
    "text": "Method of moments\n\\[\n\\begin{aligned}\n\\hat{\\theta}_1&=\\frac{\\hat{\\gamma}(0){\\color{red}\\pm}\\sqrt{\\hat{\\gamma}(0)^2-4\\hat{\\gamma}(1)^2}}{2\\hat{\\gamma}(1)}\n\\\\\n\\hat{\\sigma^2}&=\n\\frac{\\hat{\\gamma}(1)}{\\hat{\\theta}_1}\n=\n\\frac{\\hat{\\gamma}(0)}{1+\\hat{\\theta}_1^2}.\n\\end{aligned}\n\\] Have to pick a solution."
  },
  {
    "objectID": "slides/08-moving-average.html#asymptotics",
    "href": "slides/08-moving-average.html#asymptotics",
    "title": "The moving average model",
    "section": "Asymptotics",
    "text": "Asymptotics\n\n\n\n\n\n\nShumway and Stoffer (2025) Example 3.28\n\n\nAssuming \\(|\\theta_1|&lt;1\\), then \\[\n\\sqrt{T}(\\hat{\\theta}_1-\\theta_1)\\cd\\N\\left(0\\com\\frac{1+\\theta_1^2+4\\theta_1^4+\\theta_1^6+\\theta_1^8}{(1-\\theta_1^2)^2}\\right).\n\\]"
  }
]