[
  {
    "objectID": "syllabus/syllabus.html",
    "href": "syllabus/syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Class meeting: MoWe 11:45 AM - 1:00 PM in Old Chem 201;\nWhat the course catalog says: This is an introductory course in time series analysis with a focus on applications. Two basic approaches, including time domain and frequency domain methods, will be covered. A modern time-frequency analysis approach to study nonstationary time series analysis will also be introduced. The main goal is to guide the students to appreciate the main issues involved in time series analysis and solve practical challenges, particularly those coming from the high-frequency and ultra-long biomedical time series. The primary audience for this course is graduate students in statistics;\nWhat JZ says: Modern data science environments often feature high volumes of dependent, streaming data. Central banks, hedge funds, e-commerce sites, energy grids, meteorological stations, disease count trackers, radar systems, and autonomous vehicles are all sequentially analyzing a stream of incoming time series data and using it to forecast the future in real-time. But these forecasts are often more than just point predictions; they take the form of interval and density predictions that guide decision-making by incorporating many sources of uncertainty. This is what our course is about.\nA time series is a set of measurements collected over time. We model these data as a sequence of dependent random variables, and a time series model is their joint probability distribution. In this course, we will study many of the main methods for specifying this distribution: autoregressive moving average (ARMA) models, vector autoregressions (VARs), dynamic linear models (DLMs), hidden Markov models (HMMs), etc. Throughout, we will focus on sequential inference (recursive estimation when the data are streaming) and probabilistic prediction (point, interval, and density forecasting). Classical and Bayesian approaches to inference will be discussed, but the latter will be emphasized.\nPrerequisite: STA 521; Corequisite: STA 532.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#course-description",
    "href": "syllabus/syllabus.html#course-description",
    "title": "Syllabus",
    "section": "",
    "text": "Class meeting: MoWe 11:45 AM - 1:00 PM in Old Chem 201;\nWhat the course catalog says: This is an introductory course in time series analysis with a focus on applications. Two basic approaches, including time domain and frequency domain methods, will be covered. A modern time-frequency analysis approach to study nonstationary time series analysis will also be introduced. The main goal is to guide the students to appreciate the main issues involved in time series analysis and solve practical challenges, particularly those coming from the high-frequency and ultra-long biomedical time series. The primary audience for this course is graduate students in statistics;\nWhat JZ says: Modern data science environments often feature high volumes of dependent, streaming data. Central banks, hedge funds, e-commerce sites, energy grids, meteorological stations, disease count trackers, radar systems, and autonomous vehicles are all sequentially analyzing a stream of incoming time series data and using it to forecast the future in real-time. But these forecasts are often more than just point predictions; they take the form of interval and density predictions that guide decision-making by incorporating many sources of uncertainty. This is what our course is about.\nA time series is a set of measurements collected over time. We model these data as a sequence of dependent random variables, and a time series model is their joint probability distribution. In this course, we will study many of the main methods for specifying this distribution: autoregressive moving average (ARMA) models, vector autoregressions (VARs), dynamic linear models (DLMs), hidden Markov models (HMMs), etc. Throughout, we will focus on sequential inference (recursive estimation when the data are streaming) and probabilistic prediction (point, interval, and density forecasting). Classical and Bayesian approaches to inference will be discussed, but the latter will be emphasized.\nPrerequisite: STA 521; Corequisite: STA 532.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#teaching-team",
    "href": "syllabus/syllabus.html#teaching-team",
    "title": "Syllabus",
    "section": "Teaching team",
    "text": "Teaching team\n\n\n\n\n\n\n\n\n\nMug\nName\nRole\nOffice Hours\n\n\n\n\n\nLi, Aihua\nTA\nWed 10:30 AM - 12:30 PM\nremote (Zoom link on Canvas)\n\n\n\nZito, John\nInstructor\nTue 1:00 PM - 3:00 PM\nor by appointment\nOld Chem 207",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#textbooks",
    "href": "syllabus/syllabus.html#textbooks",
    "title": "Syllabus",
    "section": "Textbooks",
    "text": "Textbooks\nThe course does not have a designated textbook, and you are not required to purchase anything, but here are several you should know about:\n\n[SS] Time Series Analysis and Its Applications, 5e by Robert Shumway and David Stoffer (free pdf);\n[Skä] Bayesian Filtering and Smoothing, 1e by Simo Särkkä (free pdf);\n[WH] Bayesian Forecasting and Dynamic Models, 2e by Mike West and Jeff Harrison (free pdf);\n[PPC] Dynamic Linear Models with R by Giovanni Petris, Sonia Petrone, and Patrizia Campagnoli (free pdf);\n[HA] Forecasting: Principles and Practice, 3e by Rob Hyndman and George Athanasopoulos (online edition);\n[PFW] Time Series: Modeling, Computation, and Inference, 2e by Raquel Prado, Marco Ferreira, and Mike West;\n[Ham] Time Series Analysis by James Hamilton;\n[DK] Time Series Analysis by State Space Methods, 2e by James Durbin and Siem Jan Koopman;\n[BD] Time Series: Theory and Methods by Peter Brockwell and Richard Davis.\n\nI will list readings in the PREPARE column of the course schedule.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#assignments-and-grading",
    "href": "syllabus/syllabus.html#assignments-and-grading",
    "title": "Syllabus",
    "section": "Assignments and grading",
    "text": "Assignments and grading\nThere are four course components, weighted as follows:\n\n\n\n\nCategory\nPercentage\n\n\n\n\nProblem Sets\n25%\n\n\nExam 1\n25%\n\n\nExam 2\n25%\n\n\nProject\n25%\n\n\n\n\nYour final letter grade will be determined based on the usual thresholds, which will never move upward but may move downward in your favor:\n\n\n\n\nLetter Grade\nFinal Course Grade\n\n\n\n\nA+\n&gt;= 97\n\n\nA\n93 - 96.99\n\n\nA-\n90 - 92.99\n\n\nB+\n87 - 89.99\n\n\nB\n83 - 86.99\n\n\nB-\n80 - 82.99\n\n\nC+\n77 - 79.99\n\n\nC\n73 - 76.99\n\n\nC-\n70 - 72.99\n\n\nD+\n67 - 69.99\n\n\nD\n63 - 66.99\n\n\nD-\n60 - 62.99\n\n\nF\n&lt; 60\n\n\n\n\nHere are some details about the course assignments:\n\nProblem sets: there will be six in total, with one due roughly every two weeks;\nExams: there will be two written, in-class exams. The only resource you are allowed is both sides of one 8.5” x 11” sheet of notes prepared by you. The exam dates are…\n\nWednesday October 8 11:45 AM - 12:00 PM in Old Chem 201;\nMonday November 24 11:45 AM - 12:00 PM in Old Chem 201;\n\nFinal project: to end the semester, each student will write a final report that applies or extends ideas from the course. This is due at 9AM on Wednesday December 10. Full details about the project will be announced after the first exam.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#communication",
    "href": "syllabus/syllabus.html#communication",
    "title": "Syllabus",
    "section": "Communication",
    "text": "Communication\nIf you wish to ask content-related questions in writing, please do not do so via e-mail. Instead, please use the course discussion forum Ed Discussion. That way all members of the teaching team can see your question, and all students can benefit from the ensuing discussion. You are also encouraged to answer one another’s questions.\nIf you have questions about personal matters that may not be appropriate for the public course forum (e.g. illness, accommodations, etc), then please e-mail the instructor directly (john.zito@duke.edu).\n\n\n\n\n\n\nNote\n\n\n\nYou can ask questions anonymously on Ed. The teaching team will still know your identity, but your peers will not.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#late-work-and-extensions",
    "href": "syllabus/syllabus.html#late-work-and-extensions",
    "title": "Syllabus",
    "section": "Late work and extensions",
    "text": "Late work and extensions\nNo late work will be accepted unless you request an extension in advance by e-mailing the instructor directly (john.zito@duke.edu). All reasonable requests will be entertained, but extensions will not be long.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#attendance",
    "href": "syllabus/syllabus.html#attendance",
    "title": "Syllabus",
    "section": "Attendance",
    "text": "Attendance\nLive your life. Attendance is not strictly required for any of the class meetings. The responsibility lies with us to make class meetings sufficiently engaging and informative that you choose to attend.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#collaboration",
    "href": "syllabus/syllabus.html#collaboration",
    "title": "Syllabus",
    "section": "Collaboration",
    "text": "Collaboration\nYou are enthusiastically encouraged to work together and help one another on labs and problem sets. I would much rather you consult your peers than some daft language model. What I ask is that you acknowledge your collaborators. So, at the end of each problem in your write-up, leave a comment like “Ursula, Ignatius and I worked together on this problem,” or “Ethel explained to me how to do part b.” You are not being judged based on your acknowledgements, and there is no penalty for getting “too much” help from others. If you omit acknowledgements, I will assume you did your work solo. If your classmates tell a different story, I will have questions. But otherwise, so long as you are thorough and honest, there will be no problems.\nHaving said all of that, you should not be crassly handing your solutions to others for them to brainlessly copy. This is plagiarism, and all involved will earn a zero on the assignment and be referred to the conduct office, both sharers and recipients alike. The write-up you submit must be your own work.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#use-of-outside-resources-including-ai",
    "href": "syllabus/syllabus.html#use-of-outside-resources-including-ai",
    "title": "Syllabus",
    "section": "Use of outside resources, including AI",
    "text": "Use of outside resources, including AI\nThere are at least two reasons you might seek outside resources:\n\n✅ Extra practice or alternative instruction: Go crazy. Knock yourself out. Have a ball. The internet includes many good (and horrible) resources for learning this material, so if you find something that really resonates with you, have at it;\n❌ Doing the problems for you: If you find a solution online, or ask a language model to generate one, and you copy it down and submit it as your own work, that is plagiarism. If we detect it, you will earn a zero for that part of your write-up.\n\n“Using ChatGPT to complete assignments is like bringing a forklift into the weight room; you will never improve your cognitive fitness that way.” Furthermore, 50% of your final course grade is determined by your performance on old school, no-tech exams. As such, if you outsource all of your thinking to an AI, you will probably fail both exams. To avoid this, I suggest you refrain from using language models to do the problems for you.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#regrade-requests",
    "href": "syllabus/syllabus.html#regrade-requests",
    "title": "Syllabus",
    "section": "Regrade requests",
    "text": "Regrade requests\nIf you receive a graded assignment back, and you believe that some part of it was graded incorrectly, you may dispute the grade by submitting a regrade request in Gradescope. JZ is the sole reviewer of these. Note the following:\n\nYou have one week after you receive a grade to submit a regrade request;\nYou should submit separate regrade requests for each question you wish to dispute, not a single catch-all request;\nRequests will be considered if there was an error in the grade calculation or if a correct answer was mistakenly marked as incorrect;\nRequests to dispute the number of points deducted for an incorrect response will not be considered.\n\n\n\n\n\n\n\nWarning\n\n\n\nA regrade request can result in your grade going up, staying the same, or going down if we determine that, in fact, the original grader was too lenient.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus/syllabus.html#duke-community-standard",
    "href": "syllabus/syllabus.html#duke-community-standard",
    "title": "Syllabus",
    "section": "Duke Community Standard",
    "text": "Duke Community Standard\nDuke University is a community dedicated to scholarship, leadership, and service and to the principles of honesty, fairness, respect, and accountability. Members of this community commit to reflect upon and uphold these principles in all academic and non-academic endeavors, and to protect and promote a culture of integrity.\nDuke University has high expectations for students’ scholarship and conduct. In accepting admission, students indicate their willingness to subscribe to and be governed by the rules and regulations of the university, which flow from the Duke Community Standard (DCS).\nRegardless of course delivery format, it is the responsibility of all students to understand and follow all Duke policies, including but not limited to the academic integrity policy (e.g., completing one’s own work, following proper citation of sources, adhering to guidance around group work projects, and more). Ignoring these requirements is a violation of the DCS.\nStudents can direct any questions or concerns regarding academic integrity to the Office of Student Conduct and Community Standards at conduct@duke.edu and can access the DCS guide at https://dukecommunitystandard.students.duke.edu/.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "slides/00-welcome.html#teaching-team",
    "href": "slides/00-welcome.html#teaching-team",
    "title": "Welcome to STA 542!",
    "section": "Teaching team",
    "text": "Teaching team\n\n\n\n\n\n\n\n\nMug\nName\nRole\nOffice Hours\n\n\n\n\nLi, Aihua\nTA\nWed 10:30AM - 12:30PM\n\n\n\nZito, John\nInstructor\nTue 1PM - 3PM"
  },
  {
    "objectID": "slides/00-welcome.html#motivation",
    "href": "slides/00-welcome.html#motivation",
    "title": "Welcome to STA 542!",
    "section": "Motivation",
    "text": "Motivation\n\n\n\n\n\n\n\nControversial statement\n\n\nStatistics is about quantifying uncertainty to help make decisions.\n\n\n\n\n\nIn modern data science environments, stakeholders are sequentially analyzing high volumes of dependent data and using it to forecast the future in real-time.\n\n\nLet’s see that in action."
  },
  {
    "objectID": "slides/00-welcome.html#central-banking",
    "href": "slides/00-welcome.html#central-banking",
    "title": "Welcome to STA 542!",
    "section": "Central banking",
    "text": "Central banking"
  },
  {
    "objectID": "slides/00-welcome.html#e-commerce-and-web-traffic",
    "href": "slides/00-welcome.html#e-commerce-and-web-traffic",
    "title": "Welcome to STA 542!",
    "section": "E-commerce and web traffic",
    "text": "E-commerce and web traffic\namazon.com is never “offline.”"
  },
  {
    "objectID": "slides/00-welcome.html#extreme-weather-forecasting",
    "href": "slides/00-welcome.html#extreme-weather-forecasting",
    "title": "Welcome to STA 542!",
    "section": "Extreme weather forecasting",
    "text": "Extreme weather forecasting"
  },
  {
    "objectID": "slides/00-welcome.html#monitoring-disease-case-counts",
    "href": "slides/00-welcome.html#monitoring-disease-case-counts",
    "title": "Welcome to STA 542!",
    "section": "Monitoring disease case counts",
    "text": "Monitoring disease case counts"
  },
  {
    "objectID": "slides/00-welcome.html#monitoring-the-energy-grid",
    "href": "slides/00-welcome.html#monitoring-the-energy-grid",
    "title": "Welcome to STA 542!",
    "section": "Monitoring the energy grid",
    "text": "Monitoring the energy grid"
  },
  {
    "objectID": "slides/00-welcome.html#object-tracking",
    "href": "slides/00-welcome.html#object-tracking",
    "title": "Welcome to STA 542!",
    "section": "Object tracking",
    "text": "Object tracking\n\n\n\n\n\nAny ethical complaints about this one?"
  },
  {
    "objectID": "slides/00-welcome.html#autonomous-vehicle-navigation",
    "href": "slides/00-welcome.html#autonomous-vehicle-navigation",
    "title": "Welcome to STA 542!",
    "section": "Autonomous vehicle navigation",
    "text": "Autonomous vehicle navigation"
  },
  {
    "objectID": "slides/00-welcome.html#lets-take-a-second-look-at-this",
    "href": "slides/00-welcome.html#lets-take-a-second-look-at-this",
    "title": "Welcome to STA 542!",
    "section": "Let’s take a second look at this",
    "text": "Let’s take a second look at this\n\n\n\n\n\nSequential inference and probabilistic prediction!"
  },
  {
    "objectID": "slides/00-welcome.html#point-forecast",
    "href": "slides/00-welcome.html#point-forecast",
    "title": "Welcome to STA 542!",
    "section": "Point forecast",
    "text": "Point forecast\nYour single-number best guess at tomorrow’s observation:"
  },
  {
    "objectID": "slides/00-welcome.html#forecast-interval",
    "href": "slides/00-welcome.html#forecast-interval",
    "title": "Welcome to STA 542!",
    "section": "Forecast interval",
    "text": "Forecast interval\nA range of likely values for tomorrow’s observation:"
  },
  {
    "objectID": "slides/00-welcome.html#forecast-density",
    "href": "slides/00-welcome.html#forecast-density",
    "title": "Welcome to STA 542!",
    "section": "Forecast density",
    "text": "Forecast density\nFull distribution capturing uncertainty about tomorrow:"
  },
  {
    "objectID": "slides/00-welcome.html#and-then-tomorrow-finally-comes",
    "href": "slides/00-welcome.html#and-then-tomorrow-finally-comes",
    "title": "Welcome to STA 542!",
    "section": "And then tomorrow finally comes",
    "text": "And then tomorrow finally comes\nSo…how’d we do?"
  },
  {
    "objectID": "slides/00-welcome.html#the-main-themes-of-the-course",
    "href": "slides/00-welcome.html#the-main-themes-of-the-course",
    "title": "Welcome to STA 542!",
    "section": "The main themes of the course",
    "text": "The main themes of the course\nWe will focus on the following:\n\n\nInference should be sequential. We want recursive estimation techniques to handle data that are streaming in real-time;\nPredictions should be probabilistic. We want point, interval, and density forecasts that incorporate many sources of uncertainty;\nIf you can manipulate joint distributions, you can do time series analysis. Too many TS textbooks quickly lose sight of this;\n\n\n\nAnd there is a secret fourth theme:\n\n\nA Bayesian approach is an excellent way of achieving the goals of sequential inference and probabilistic prediction."
  },
  {
    "objectID": "slides/00-welcome.html#topics-may-include",
    "href": "slides/00-welcome.html#topics-may-include",
    "title": "Welcome to STA 542!",
    "section": "Topics may include",
    "text": "Topics may include\n\nARMA models;\nVector autoregressions (VARs);\nDynamic linear models (DLMs);\nHidden Markov Models (HMMs);\nProbabilistic forecast evaluation;\nNonlinear non-Gaussian state-space models;\nSequential Monte Carlo (AKA particle filtering);\nForecast combination."
  },
  {
    "objectID": "slides/00-welcome.html#bookmark-the-course-page",
    "href": "slides/00-welcome.html#bookmark-the-course-page",
    "title": "Welcome to STA 542!",
    "section": "Bookmark the course page!",
    "text": "Bookmark the course page!\n\n\n\n\n\nhttps://sta542-f25.github.io/"
  },
  {
    "objectID": "slides/00-welcome.html#final-grade-breakdown",
    "href": "slides/00-welcome.html#final-grade-breakdown",
    "title": "Welcome to STA 542!",
    "section": "Final grade breakdown",
    "text": "Final grade breakdown\nYour final course grade will be calculated as follows:\n\n\nCategory\nPercentage\n\n\n\nProblem Sets\n25%\n\n\nExam 1\n25%\n\n\nExam 2\n25%\n\n\nFinal Project\n25%\n\n\n\n\n\n\n\n\n\nNote\n\n\nThe final letter grade will be based on the usual thresholds, and there might be a curve."
  },
  {
    "objectID": "slides/00-welcome.html#course-components",
    "href": "slides/00-welcome.html#course-components",
    "title": "Welcome to STA 542!",
    "section": "Course components",
    "text": "Course components\n\n\nProblem sets: 6 in total; one due about every 2 weeks;\n\nExams: in-class, with only an 8.5” x 11” note sheet:\n\nWednesday October 8 11:45 AM - 12:00 PM;\nMonday November 24 11:45 AM - 12:00 PM;\n\n\nFinal project: no clue, honestly. Talk to me in October."
  },
  {
    "objectID": "slides/00-welcome.html#late-policy",
    "href": "slides/00-welcome.html#late-policy",
    "title": "Welcome to STA 542!",
    "section": "Late policy",
    "text": "Late policy\nNo late work will be accepted unless you request an extension in advance by e-mailing JZ. All reasonable requests will be entertained, but extensions will not be long."
  },
  {
    "objectID": "slides/00-welcome.html#attendance",
    "href": "slides/00-welcome.html#attendance",
    "title": "Welcome to STA 542!",
    "section": "Attendance",
    "text": "Attendance\nNot required. Live your life."
  },
  {
    "objectID": "slides/00-welcome.html#communication",
    "href": "slides/00-welcome.html#communication",
    "title": "Welcome to STA 542!",
    "section": "Communication",
    "text": "Communication\nIf you wish to ask questions in writing…\n\nPost on Ed: about general course policies and content;\nEmail JZ directly: personal matters.\n\nYou should not really be emailing Aihua directly for any reason."
  },
  {
    "objectID": "slides/00-welcome.html#collaboration",
    "href": "slides/00-welcome.html#collaboration",
    "title": "Welcome to STA 542!",
    "section": "Collaboration",
    "text": "Collaboration\nYou are enthusiastically encouraged to work together on the problem sets. You will learn a lot from each other! Two policies:\n\n✅ Acknowledge your collaborators: “Aloysius, Cybill, and I worked together on this problem;”\n❌ Do not outright share or copy solutions. All submitted work must be your own.\n\nViolation of the second policy is plagiarism. Sharers and recipients alike are referred to the conduct office and receive zeros."
  },
  {
    "objectID": "slides/00-welcome.html#use-of-outside-resources-including-ai",
    "href": "slides/00-welcome.html#use-of-outside-resources-including-ai",
    "title": "Welcome to STA 542!",
    "section": "Use of outside resources, including AI",
    "text": "Use of outside resources, including AI\n\nUsing ChatGPT to complete assignments is like bringing a forklift into the weight room; you will never improve your cognitive fitness that way.\n\n\nIf you find a problem solution online (or prompt an LLM to generate one) and submit it as your own work, that will obviously be considered plagiarism;\nOtherwise, all outside resources are fair game for you to study and get extra practice;\nIf you outsource all of your thinking to a language model, you will probably fail both exams. Good luck!"
  },
  {
    "objectID": "slides/00-welcome.html#what-background-do-you-need",
    "href": "slides/00-welcome.html#what-background-do-you-need",
    "title": "Welcome to STA 542!",
    "section": "What background do you need?",
    "text": "What background do you need?\nI assume you have a working knowledge of…\n\n\nmatrix algebra;\nOLS regression;\nprobability and math stat at the level of Casella & Berger;\nBayesian statistics at the level of STA 602;\nThe R programming language.\n\n\n\nProblem Set 0 gives you a workout in all of the above."
  },
  {
    "objectID": "slides/00-welcome.html#time-series",
    "href": "slides/00-welcome.html#time-series",
    "title": "Welcome to STA 542!",
    "section": "Time series",
    "text": "Time series\n\nA time series is a set of measurements collected over time;\nWe model these data as a sequence of dependent random variables:\n\n\\[\n\\mathbf{y}_{0:T} = \\{\\mathbf{y}_0,\\,\\mathbf{y}_1,\\,\\mathbf{y}_2,\\,...,\\,\\mathbf{y}_T\\}.\n\\]\n\nA time series model is “just” their joint probability distribution:\n\n\\[\np(\\mathbf{y}_{0:T}) = p(\\mathbf{y}_0)\\prod_{t=1}^Tp(\\mathbf{y}_t\\,|\\,\\mathbf{y}_{0:t-1}).\n\\]\n\n\n\n\n\n\n\nStay grounded.\n\n\nLike much wisdom, that last bullet is simultaneously vacuous and profound. It tells you everything and it tells you nothing all at once. But don’t let this basic fact get lost in the sea of details."
  },
  {
    "objectID": "slides/00-welcome.html#notation-to-get-used-to",
    "href": "slides/00-welcome.html#notation-to-get-used-to",
    "title": "Welcome to STA 542!",
    "section": "Notation to get used to",
    "text": "Notation to get used to\n\nI will not use uppercase \\(Y_t\\) versus lowercase \\(y_t\\) to distinguish random variables and fixed realizations. It’s all just \\(y_t\\);\nA vector \\(\\mathbf{y}\\in\\RR^n\\) is always an \\(n\\times 1\\) column. The corresponding row vector is \\(\\By^\\tr\\);\nFor integers \\(i&lt;j\\), you will see this shorthand all the time:\n\n\\[\ny_{i:j}\n=\n\\{y_i\\com y_{i+1}\\com y_{i+2}\\com...\\com y_{j-2}\\com y_{j-1}\\com y_{j}\\}\n.\n\\]\n\nThe symbol “\\(p\\)” will be aggressively abused and overloaded to represent any probability distribution, sometimes several in the same line:\n\n\\[\np(\\mathbf{y}_{0:T}) = p(\\mathbf{y}_0)\\prod_{t=1}^Tp(\\mathbf{y}_t\\,|\\,\\mathbf{y}_{0:t-1}).\n\\]"
  },
  {
    "objectID": "slides/00-welcome.html#definition",
    "href": "slides/00-welcome.html#definition",
    "title": "Welcome to STA 542!",
    "section": "Definition",
    "text": "Definition\nA random vector \\(\\mathbf{x}=\\begin{bmatrix}x_1&x_2&\\cdots&x_n\\end{bmatrix}^{\\scriptscriptstyle\\mathsf{T}}\\in\\mathbb{R}^n\\) has the multivariate normal distribution with mean vector \\(\\boldsymbol{\\mu}\\in\\mathbb{R}^n\\) and covariance matrix \\(\\boldsymbol{\\Sigma}\\in\\mathbb{R}^{n\\times n}\\) if its density is\n\n\\[\np(\\mathbf{x})\n=\n\\frac\n{\n\\exp\n\\left(\n-\\frac{1}{2}\n(\\mathbf{x} - \\boldsymbol{\\mu})^{\\scriptscriptstyle\\mathsf{T}}\n\\boldsymbol{\\Sigma}^{-1}\n(\\mathbf{x}-\\boldsymbol{\\mu})\n\\right)\n}\n{\n(2\\pi)^{\\frac{n}{2}}\n|\\boldsymbol{\\Sigma}|^{1/2}\n}\n,\n\\quad\n\\mathbf{x}\n\\in\n\\mathbb{R}^n.\n\\]\n\n\nWe denote this \\(\\mathbf{x}\\sim\\text{N}_n(\\boldsymbol{\\mu},\\,\\boldsymbol{\\Sigma})\\).\n\n\n\n\n\n\n\n\nPlenty of linear algebra coming your way!\n\n\n\nTranspose \\((\\mathbf{x} - \\boldsymbol{\\mu})^{\\scriptscriptstyle\\mathsf{T}}\\);\nInverse \\(\\boldsymbol{\\Sigma}^{-1}\\);\nMatrix multiplication \\((\\mathbf{x} - \\boldsymbol{\\mu})^{\\scriptscriptstyle\\mathsf{T}}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\);\nDeterminant \\(|\\boldsymbol{\\Sigma}|\\);\n\n\\(\\boldsymbol{\\Sigma}\\) is a symmetric and positive definite matrix."
  },
  {
    "objectID": "slides/00-welcome.html#univariate",
    "href": "slides/00-welcome.html#univariate",
    "title": "Welcome to STA 542!",
    "section": "Univariate",
    "text": "Univariate\nIf \\(n=1\\), then we meet an old friend:\n\n\n\n\\[\np(x)=\\frac{\\exp\\left(-\\frac{1}{2}\\frac{(x-\\mu)^2}{\\sigma^2}\\right)}{\\sqrt{2\\pi\\sigma^2}}.\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\nSo \\(\\mathbf{x} = [x]\\sim\\text{N}_1([\\mu],\\,[\\sigma^2])\\) is just \\(x\\sim\\text{N}(\\mu,\\,\\sigma^2)\\)."
  },
  {
    "objectID": "slides/00-welcome.html#bivariate",
    "href": "slides/00-welcome.html#bivariate",
    "title": "Welcome to STA 542!",
    "section": "Bivariate",
    "text": "Bivariate\nElliptical contours!"
  },
  {
    "objectID": "slides/00-welcome.html#moments",
    "href": "slides/00-welcome.html#moments",
    "title": "Welcome to STA 542!",
    "section": "Moments",
    "text": "Moments\nFirst:\n\\[\n\\boldsymbol{\\mu}\n=\nE(\\mathbf{x})\n=\n\\begin{bmatrix}\n\\mu_1\n&\n\\mu_2\n&\n\\cdots\n&\n\\mu_n\n\\end{bmatrix}^{\\scriptscriptstyle\\mathsf{T}}\n=\n\\begin{bmatrix}\nE(x_1)\n&\nE(x_2)\n&\n\\cdots\n&\nE(x_n)\n\\end{bmatrix}^{\\scriptscriptstyle\\mathsf{T}}\n.\n\\]\n\nSecond:\n\\[\n\\begin{aligned}\n\\boldsymbol{\\Sigma}\n=\n\\text{cov}(\\mathbf{x})\n&=\n\\begin{bmatrix}\n\\sigma^2_1 & \\sigma_{1,2} & \\cdots & \\sigma_{1,n}\\\\\n\\sigma_{1,2} & \\sigma_{2}^2 & \\cdots & \\sigma_{2,n}\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n\\sigma_{1,n} & \\sigma_{2,n} & \\cdots & \\sigma_{n}^2\\\\\n\\end{bmatrix}\n\\\\\n&=\n\\begin{bmatrix}\n\\text{var}(x_1) & \\text{cov}(x_1,\\,x_2) & \\cdots & \\text{cov}(x_1,\\,x_n)\\\\\n\\text{cov}(x_2,\\,x_1) & \\text{var}(x_2) & \\cdots & \\text{cov}(x_2,\\,x_n)\\\\\n\\vdots & \\vdots & \\ddots & \\vdots\\\\\n\\text{cov}(x_n,\\,x_1) & \\text{cov}(x_n,\\,x_2) & \\cdots & \\text{var}(x_n)\\\\\n\\end{bmatrix}\n.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/00-welcome.html#oh-my-aching-eigen",
    "href": "slides/00-welcome.html#oh-my-aching-eigen",
    "title": "Welcome to STA 542!",
    "section": "Oh my aching eigen",
    "text": "Oh my aching eigen\n\n\n\n\n\n\n\nEigenvectors and eigenvalues\n\n\nThere are \\(n\\) orthogonal vectors \\(\\mathbf{v}_i\\in\\mathbb{R}^n\\) and values \\(\\lambda_i&gt;0\\) satisfying:\n\\[\n\\boldsymbol{\\Sigma}\\mathbf{v}_i=\\lambda_i\\mathbf{v}_i.\n\\] Positive definite means all the eigenvalues are real and strictly postive.\n\n\n\n\n\n\n\n\n\n\n\nEigendecomposition (AKA spectral decomposition)\n\n\nA useful way to rewrite \\(\\boldsymbol{\\Sigma}\\): \\[\n\\begin{aligned}\n\\boldsymbol{\\Sigma}\n&=\n\\begin{bmatrix}\n\\mathbf{v}_1 & \\mathbf{v}_2 & \\cdots & \\mathbf{v}_n\n\\end{bmatrix}\n\\begin{bmatrix}\n\\lambda_1 & & &\\mathbf{0}\\\\\n& \\lambda_2 & & \\\\\n  & & \\ddots& \\\\\n  \\mathbf{0}  & & & \\lambda_n\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\mathbf{v}_1^{\\scriptscriptstyle\\mathsf{T}} \\\\ \\mathbf{v}_2^{\\scriptscriptstyle\\mathsf{T}} \\\\ \\vdots \\\\ \\mathbf{v}_n^{\\scriptscriptstyle\\mathsf{T}}\n\\end{bmatrix}\n=\n\\mathbf{Q}\\boldsymbol{\\Lambda}\\mathbf{Q}^{\\scriptscriptstyle\\mathsf{T}}.\n\\end{aligned}\n\\]\nWe often set \\(||\\mathbf{v}_i||_2=1\\), and so the \\(n\\times n\\) matrix \\(\\mathbf{Q}\\) is orthogonal: \\(\\mathbf{Q}\\mathbf{Q}^{\\scriptscriptstyle\\mathsf{T}}=\\mathbf{Q}^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{Q}=\\mathbf{I}_n\\)."
  },
  {
    "objectID": "slides/00-welcome.html#who-cares",
    "href": "slides/00-welcome.html#who-cares",
    "title": "Welcome to STA 542!",
    "section": "Who cares?",
    "text": "Who cares?\nThe eigenvectors of \\(\\boldsymbol{\\Sigma}\\) point along the axes of the elliptical density contours. These are the directions of the principal components!"
  },
  {
    "objectID": "slides/00-welcome.html#review-joint-distributions",
    "href": "slides/00-welcome.html#review-joint-distributions",
    "title": "Welcome to STA 542!",
    "section": "Review: joint distributions",
    "text": "Review: joint distributions\n\nThe marginal distribution:\n\\[\np(\\mathbf{y}) = \\int p(\\mathbf{x},\\,\\mathbf{y})\\,\\text{d}\\mathbf{x}.\n\\]\n\n\nThe conditional distribution:\n\\[\np(\\mathbf{y}\\,|\\,\\mathbf{x})\n=\n\\frac{p(\\mathbf{x},\\,\\mathbf{y})}{p(\\mathbf{x})}\n=\n\\frac{p(\\mathbf{x}\\,|\\,\\mathbf{y})p(\\mathbf{y})}{p(\\mathbf{x})}\n.\n\\]"
  },
  {
    "objectID": "slides/00-welcome.html#marginals-and-conditionals",
    "href": "slides/00-welcome.html#marginals-and-conditionals",
    "title": "Welcome to STA 542!",
    "section": "Marginals and conditionals",
    "text": "Marginals and conditionals\nIf you apply those formulas to this\n\\[\n\\begin{bmatrix}\n\\mathbf{x} \\\\\n\\mathbf{y}\n\\end{bmatrix}\n\\sim\n\\text{N}_{n+m}\n\\left(\n\\begin{bmatrix}\n\\boldsymbol{\\mu}_x \\\\\n\\boldsymbol{\\mu}_y\n\\end{bmatrix}\n,\\,\n\\begin{bmatrix}\n\\boldsymbol{\\Sigma}_x & \\boldsymbol{\\Sigma}_{xy}\\\\\n\\boldsymbol{\\Sigma}_{xy}^{\\scriptscriptstyle\\mathsf{T}} & \\boldsymbol{\\Sigma}_y\n\\end{bmatrix}\n\\right),\n\\]\n\nthen you get this:\n\\[\n\\begin{aligned}\n\\mathbf{x} &\\sim  \\text{N}_n(\\boldsymbol{\\mu}_x,\\,\\boldsymbol{\\Sigma}_x)\\\\\n\\mathbf{y} &\\sim  \\text{N}_m(\\boldsymbol{\\mu}_y,\\,\\boldsymbol{\\Sigma}_y)\\\\\n\\mathbf{x} \\,|\\,\\mathbf{y}\n&\\sim\n\\text{N}_n\n\\left(\n\\boldsymbol{\\mu}_x + \\boldsymbol{\\Sigma}_{xy}\\boldsymbol{\\Sigma}_y^{-1}(\\mathbf{y}-\\boldsymbol{\\mu}_y)\n,\\,\n\\boldsymbol{\\Sigma}_x - \\boldsymbol{\\Sigma}_{xy}\n\\boldsymbol{\\Sigma}_y^{-1}\n\\boldsymbol{\\Sigma}_{xy}^{\\scriptscriptstyle\\mathsf{T}}\n\\right)\n\\\\\n\\mathbf{y} \\,|\\,\\mathbf{x}\n&\\sim\n\\text{N}_m\n\\left(\n\\boldsymbol{\\mu}_y + \\boldsymbol{\\Sigma}_{xy}^{\\scriptscriptstyle\\mathsf{T}}\\boldsymbol{\\Sigma}_x^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_x)\n,\\,\n\\boldsymbol{\\Sigma}_y - \\boldsymbol{\\Sigma}_{xy}^{\\scriptscriptstyle\\mathsf{T}}\n\\boldsymbol{\\Sigma}_x^{-1}\n\\boldsymbol{\\Sigma}_{xy}\n\\right).\n\\end{aligned}\n\\]\n\n\nSo the individual components are all normal: \\(x_i\\sim\\text{N}(\\mu_i,\\,\\sigma^2_i)\\)."
  },
  {
    "objectID": "slides/00-welcome.html#those-ugly-conditional-formulas-are-not-new",
    "href": "slides/00-welcome.html#those-ugly-conditional-formulas-are-not-new",
    "title": "Welcome to STA 542!",
    "section": "Those ugly conditional formulas are not new",
    "text": "Those ugly conditional formulas are not new\nConsider the bivariate case:\n\\[\n\\begin{bmatrix}\nx \\\\\ny\n\\end{bmatrix}\n\\sim\\text{N}_2\n\\left(\n\\begin{bmatrix}\n\\mu_x \\\\\n\\mu_y\n\\end{bmatrix}\n,\\,\n\\begin{bmatrix}\n\\sigma_x^2\n&\n\\rho\\sigma_x\\sigma_y\n\\\\\n\\rho\\sigma_x\\sigma_y\n&\n\\sigma^2_y\n\\end{bmatrix}\n\\right).\n\\]\n\nThen the formula collapses to:\n\\[\ny\\,|\\,\nx\n\\sim\n\\text{N}\n\\left(\n\\underbrace{\\left(\\mu_y-\\rho\\frac{\\sigma_y}{\\sigma_x}\\mu_x\\right)}_{\\beta_0}\n+\n\\underbrace{\\rho\\frac{\\sigma_y}{\\sigma_x}}_{\\beta_1}\nx\n,\\,\n\\underbrace{(1-\\rho)^2\n\\sigma^2_y}_{\\sigma^2}\n\\right).\n\\]\n\n\nIn other words:\n\\[\ny = \\beta_0+\\beta_1x + \\varepsilon,\\quad \\varepsilon\\sim\\text{N}(0,\\,\\sigma^2).\n\\]\n\n\nWelcome back to regression 101!"
  },
  {
    "objectID": "slides/00-welcome.html#affine-transformations",
    "href": "slides/00-welcome.html#affine-transformations",
    "title": "Welcome to STA 542!",
    "section": "Affine transformations",
    "text": "Affine transformations\nFix some objects:\n\n\nRandom \\(\\mathbf{x}\\sim\\text{N}_n(\\boldsymbol{\\mu},\\,\\boldsymbol{\\Sigma})\\);\nConstant \\(\\mathbf{A}\\in\\mathbb{R}^{m\\times n}\\);\nConstant \\(\\mathbf{c}\\in\\mathbb{R}^m\\).\n\n\n\nThen\n\\[\n\\mathbf{y}\n=\n\\mathbf{A}\n\\mathbf{x}\n+\n\\mathbf{c}\n\\sim\n\\text{N}_m\n\\left(\n\\mathbf{A}\n\\boldsymbol{\\mu}\n+\n\\mathbf{c}\n,\\,\n\\mathbf{A}\n\\boldsymbol{\\Sigma}\n\\mathbf{A}^{\\scriptscriptstyle\\mathsf{T}}\n\\right)\n.\n\\]\n\n\nProve it on Problem Set 0!"
  },
  {
    "objectID": "slides/00-welcome.html#a-useful-special-case-problem-set-0",
    "href": "slides/00-welcome.html#a-useful-special-case-problem-set-0",
    "title": "Welcome to STA 542!",
    "section": "A useful special case (Problem Set 0!)",
    "text": "A useful special case (Problem Set 0!)\n\n\n\n\n\n\nLinear combinations of independent normals are normal\n\n\nIf \\(x_i\\overset{\\text{indep}}{\\sim}\\text{N}(\\mu_i,\\,\\sigma^2_i)\\) and \\(a_i\\in\\mathbb{R}\\) are constant, then\n\\[\n\\sum\\limits_{i=1}^na_ix_i\\sim\\text{N}\\left(\\sum\\limits_{i=1}^na_i\\mu_i,\\, \\sum\\limits_{i=1}^na_i^2\\sigma_i^2\\right).\n\\]\n\n\n\n\n\n\nThe result on the previous slide is way more general, and the linear combination is still normal even if the \\(X_i\\) are dependent, but the formula for the variance is less nice;\nThe mean formula is right even if the \\(x_i\\) are dependent and non-Gaussian;\nThe variance formula is right even if the \\(x_i\\) are non-Gaussian."
  },
  {
    "objectID": "slides/00-welcome.html#independence",
    "href": "slides/00-welcome.html#independence",
    "title": "Welcome to STA 542!",
    "section": "Independence",
    "text": "Independence\n\n\nIf random variables are independent, then they are uncorrelated (their covariance is zero). The reverse is false in general!\nSo writing \\(x_1,\\,x_2,\\,...,\\,x_n\\overset{\\text{iid}}{\\sim}\\text{N}(\\mu,\\,\\sigma^2)\\) is the same as saying\n\n\n\n\\[\n\\begin{bmatrix}\nx_1\\\\\nx_2\\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix}\n\\sim\\text{N}_n\\left(\\mu\\mathbf{1}_n,\\,\\sigma^2\\mathbf{I}_n\\right).\n\\]\n\n\n\n\n\\(\\mathbf{1}_n\\) is the \\(n\\times 1\\) vector of all ones;\n\n\n\n\n\n\\(\\mathbf{I}_n\\) is the \\(n\\times n\\) identity matrix: ones on the diagonal, zeros off."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STA 542 Introduction to Time Series Analysis",
    "section": "",
    "text": "Below is a prospective outline for the course. Due dates are firm, but topics may change with advanced notice.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWEEK\nDATE\nPREPARE\nTOPIC\nMATERIALS\nDUE\n\n\n\n1\nMon, Aug 25\n\n🧑‍🏫 Welcome!\nslides\n\n\n\n\nWed, Aug 27\n\n🧑‍🏫 AR(1) structure\nslides\n\n\n\n2\nMon, Sep 1\n\n❌ Labor Day - No Lecture\n\n\n\n\n\n\nWed, Sep 3\n\n🧑‍🏫 AR(1) inference\nslides\n\n\n\n\nFri, Sep 5\n\n\n\n\nPSET 0 @ 5PM\n\n\n3\nMon, Sep 8\n\n🧑‍🏫 AR(1) inference\n\n\n\n\n\nWed, Sep 10\n\n🧑‍🏫 AR(1) forecasting\n\n\n\n\n4\nMon, Sep 15\n\nSS: Ch. 3\n🧑‍🏫 AR(p)\n\n\n\n\n\nWed, Sep 17\n\nSS: Ch. 3\n🧑‍🏫 AR(p)\n\n\n\n\n\nFri, Sep 19\n\n\n\nPSET 1 @ 5PM\n\n\n5\nMon, Sep 22\n\nSS: Ch. 3\n🧑‍🏫 MA\n\n\n\n\n\nWed, Sep 24\n\nSS: Ch. 3\n🧑‍🏫 MA\n\n\n\n\n6\nMon, Sep 29\n\nSS: Ch. 3\n🧑‍🏫 ARMA\n\n\n\n\n\nWed, Oct 1\n\nSS: Ch. 3\n🧑‍🏫 ARMA\n\n\n\n\n\nFri, Oct 3\n\n\n\nPSET 2 @ 5PM\n\n\n7\nMon, Oct 6\n\nSS: Ch. 3\n🧑‍🏫 ARMA\n\n\n\n\n\nWed, Oct 8\n\n📝 Exam 1\n\n\n\n\n\n8\nMon, Oct 13\n\n❌ Fall Break - No Lecture\n\n\n\n\n\n\nWed, Oct 15\n\nSS: Ch. 5.5\n🧑‍🏫 VAR\n\n\n\n\n9\nMon, Oct 20\nKarlsson (2013 chapter)\n🧑‍🏫 VAR\n\n\n\n\n\nWed, Oct 22\n\n🧑‍🏫 VAR\n\n\n\n\n\nFri, Oct 24\n\n\n\nPSET 3 @ 5PM\n\n\n10\nMon, Oct 27\n\nPPC: Ch. 2\n🧑‍🏫 DLMs\n\n\n\n\n\nWed, Oct 29\n\nSkä: Ch. 4\n🧑‍🏫 DLMs\n\n\n\n\n11\nMon, Nov 3\n\nSkä: Ch. 8\n🧑‍🏫 DLMs\n\n\n\n\n\nWed, Nov 5\n\n🧑‍🏫 DLMs\n\n\n\n\n\nFri, Nov 7\n\n\n\nPSET 4 @ 5PM\n\n\n12\nMon, Nov 10\nScott (2002 JASA)\n🧑‍🏫 HMMs\n\n\n\n\n\nWed, Nov 12\n\n🧑‍🏫 HMMs\n\n\n\n\n13\nMon, Nov 17\nDoucet, Johansen (2008 chapter)\n🧑‍🏫 Nonlinear models\n\n\n\n\n\nWed, Nov 19\n\n🧑‍🏫 Nonlinear models\n\n\n\n\n\nFri, Nov 21\n\n\n\nPSET 5 @ 5PM\n\n\n14\nMon, Nov 24\n\n📝 Exam 2\n\n\n\n\n\n\nWed, Nov 26\n\n❌ Thanksgiving - No Lecture\n\n\n\n\n\n16\nWed, Dec 10\n\n\n\nProject @ 9AM"
  },
  {
    "objectID": "problems/pset-1.html",
    "href": "problems/pset-1.html",
    "title": "Problem Set 1",
    "section": "",
    "text": "Do some basic AR(2) derivations\nderive AR(1) covariance kernel (not assuming stationarity)\ndo recursive and joint simulations many times and verify that you get the same sample moments.\nBayesian AR(1) with a different prior.\nsimulation to explore sampling properties when it’s a unit root or explosive or whatever.\ngiven the joint, what is the recursion?"
  },
  {
    "objectID": "problems/bank/review/mvnormal-1.html",
    "href": "problems/bank/review/mvnormal-1.html",
    "title": "STA 542 Fall 2025",
    "section": "",
    "text": "Slicing-and-dicing the multivariate normal distribution is a very important skill in time series analysis, so let’s make sure we can do that.\n\nLet \\(\\mathbf{x}\\sim\\text{N}_n(\\boldsymbol{\\mu},\\,\\boldsymbol{\\Sigma})\\), and fix constants \\(\\mathbf{A}\\in\\mathbb{R}^{m\\times n}\\) and \\(\\mathbf{c}\\in\\mathbb{R}^m\\). Show that\n\n\\[\n\\mathbf{y}\n=\n\\mathbf{A}\n\\mathbf{x}\n+\n\\mathbf{c}\n\\sim\n\\text{N}_m\n\\left(\n\\mathbf{A}\n\\boldsymbol{\\mu}\n+\n\\mathbf{c}\n,\\,\n\\mathbf{A}\n\\boldsymbol{\\Sigma}\n\\mathbf{A}^{\\scriptscriptstyle\\mathsf{T}}\n\\right)\n.\n\\]\n\nConsider the joint distribution \\(p(\\mathbf{x},\\,\\mathbf{y})=p(\\mathbf{y}\\,|\\,\\mathbf{x})p(\\mathbf{x})\\) written hierarchically as \\(\\mathbf{x}\\sim\\text{N}_n(\\boldsymbol{\\mu},\\,\\boldsymbol{\\Sigma})\\) and \\(\\mathbf{y}\\,|\\,\\mathbf{x}\\sim\\text{N}_m(\\mathbf{A}\\mathbf{x}+\\mathbf{c},\\,\\mathbf{R})\\). Use the result in part a to show that the joint distribution implied by this hierarchy is\n\n\\[\n\\begin{bmatrix}\n\\mathbf{x}\n\\\\\n\\mathbf{y}\n\\end{bmatrix}\n\\sim\n\\text{N}_{n+m}\n\\left(\n\\begin{bmatrix}\n\\boldsymbol{\\mu}\n\\\\\n\\mathbf{A}\\boldsymbol{\\mu}+\\mathbf{c}\n\\end{bmatrix}\n,\\,\n\\begin{bmatrix}\n\\boldsymbol{\\Sigma}\n&\n\\boldsymbol{\\Sigma}\\mathbf{A}^{\\scriptscriptstyle\\mathsf{T}}\n\\\\\n\\mathbf{A}\n\\boldsymbol{\\Sigma}\n&\n\\mathbf{A}\n\\boldsymbol{\\Sigma}\n\\mathbf{A}^{\\scriptscriptstyle\\mathsf{T}} + \\mathbf{R}\n\\end{bmatrix}\n\\right)\n.\n\\]\n\nUse the result in part a to show that the linear combination of independent normals is normal. That is, if \\(x_i\\sim\\text{N}(\\mu_i,\\,\\sigma^2_i)\\) are independent and \\(a_i\\in\\mathbb{R}\\) are constant, then prove that\n\n\\[\n\\sum\\limits_{i=1}^na_ix_i\\sim\\text{N}\\left(\\sum\\limits_{i=1}^na_i\\mu_i,\\, \\sum\\limits_{i=1}^na_i^2\\sigma_i^2\\right).\n\\]\n\n\n\n\n\n\nHint\n\n\n\n\n\nJZ changed this hint on 8/30/2025. The original hint mentioned using the change-of-variables formula. That’s a fine solution if \\(\\mathbf{A}\\) is invertible, but the result holds even if \\(\\mathbf{A}\\) is not invertible, and we will often use that version (e.g. part c here). If you used change-of-variables, don’t redo it. We’ll award full credit if you did it right. But otherwise, consider using the moment-generating function of a random vector \\(\\mathbf{x}\\):\n\\[\nM(\\mathbf{t})=E\\left(e^{\\mathbf{t}^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{x}}\\right).\n\\]"
  },
  {
    "objectID": "problems/bank/review/online-least-squares.html",
    "href": "problems/bank/review/online-least-squares.html",
    "title": "STA 542 Fall 2025",
    "section": "",
    "text": "An important consideration in time series analysis is sequential or online inference. In this environment, our data are not available to us all at once in a batch. Instead, the observations are streaming; they are arriving one-after-another in real-time, and we want to come up with a scheme that allows us to recursively update our inferences as new information arrives. Let’s explore this in the context of least squares regression.\n\n\n\n\n\n\nOLS review\n\n\n\nConsider the usual setting where we observe iid pairs \\(y_i\\in\\mathbb{R}\\) and \\(\\mathbf{x}_i\\in\\mathbb{R}^p\\) from the linear model:\n\\[\ny_i= \\mathbf{x}_i^{\\scriptscriptstyle\\mathsf{T}}\\boldsymbol{\\beta}+\\varepsilon_i,\\quad \\varepsilon_i\\overset{\\text{iid}}{\\sim}\\text{N}(0,\\,\\sigma^2).\n\\] Given \\(n\\) observations, we can form the matrices\n\\[\n\\mathbf{y}_n\n=\n\\underbrace{\n\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix}\n}\n_{n\\times 1}\n\\quad\n\\mathbf{X}_n\n=\n\\underbrace{\n\\begin{bmatrix}\n\\mathbf{x}_1^{\\scriptscriptstyle\\mathsf{T}} \\\\\n\\mathbf{x}_2^{\\scriptscriptstyle\\mathsf{T}} \\\\\n\\vdots \\\\\n\\mathbf{x}_n^{\\scriptscriptstyle\\mathsf{T}}\n\\end{bmatrix}\n}_{n\\times p}\n\\quad\n\\boldsymbol{\\varepsilon}_n\n=\n\\underbrace{\n\\begin{bmatrix}\n\\varepsilon_1 \\\\\n\\varepsilon_2 \\\\\n\\vdots \\\\\n\\varepsilon_n\n\\end{bmatrix}\n}\n_{n\\times 1}\n,\n\\] and we can rewrite the model in matrix form:\n\\[\n\\mathbf{y}_n=\\mathbf{X}_n\\boldsymbol{\\beta}+\\boldsymbol{\\varepsilon}_n,\\quad \\boldsymbol{\\varepsilon}_n\\sim\\text{N}_n(\\mathbf{0},\\,\\sigma^2\\mathbf{I}_n).\n\\] The ordinary least squares (OLS) estimator of the regression coefficients is \\(\\hat{\\boldsymbol{\\beta}}_n=(\\mathbf{X}_n^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{X}_n)^{-1}\\mathbf{X}_n^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{y}_n.\\)\n\n\nHopefully that was all review, and it describes how to do batch or offline inference for the linear model. We are assuming that all of our data are available in a single batch, and we only need to compute the estimate once. But in our class, we will often care about streaming or online inference: efficiently computing the sequence of estimates \\(\\hat{\\boldsymbol{\\beta}}_1\\), \\(\\hat{\\boldsymbol{\\beta}}_2\\), \\(\\hat{\\boldsymbol{\\beta}}_3\\), … as the pairs \\((\\mathbf{x}_1,\\,y_1)\\), \\((\\mathbf{x}_2,\\,y_2)\\), \\((\\mathbf{x}_3,\\,y_3)\\), … arrive one-after-another in real-time.\n\nSo, imagine we have observed \\(n-1\\) pairs, and we have computed the estimate \\(\\hat{\\boldsymbol{\\beta}}_{n-1}=(\\mathbf{X}_{n-1}^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{X}_{n-1})^{-1}\\mathbf{X}_{n-1}^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{y}_{n-1}.\\) Then, a new observation \\((\\mathbf{x}_n,\\,y_n)\\) arrives. How can we efficiently update our estimate of \\(\\boldsymbol{\\beta}\\) to incorporate this new information? In other words, how can we quickly compute \\(\\hat{\\boldsymbol{\\beta}}_{n}\\) given only \\(\\hat{\\boldsymbol{\\beta}}_{n-1}\\) and the new \\((\\mathbf{x}_n,\\,y_n)\\)? Of course, we could always just add new rows to \\(\\mathbf{X}_{n-1}\\) and \\(\\mathbf{y}_{n-1}\\) and recompute the whole estimate from scratch, but that is super inefficient and we can do better. To that end, show that the new estimate is related to the old estimate and the new data by the following recursion: \\(\\hat{\\boldsymbol{\\beta}}_n=\\hat{\\boldsymbol{\\beta}}_{n-1}+\\mathbf{k}_n(y_n-\\mathbf{x}_n^{\\scriptscriptstyle\\mathsf{T}}\\hat{\\boldsymbol{\\beta}}_{n-1})\\). What is \\(\\mathbf{k}_n\\)?\nInterpret the recursion you derived in part a. Isn’t \\((y_n-\\mathbf{x}_n^{\\scriptscriptstyle\\mathsf{T}}\\hat{\\boldsymbol{\\beta}}_{n-1})\\) a residual? What is this \\(\\mathbf{k}_n\\) thing doing? What would it mean if we had \\(\\hat{\\boldsymbol{\\beta}}_n=\\hat{\\boldsymbol{\\beta}}_{n-1}\\)?\nWrite a for loop in R that uses your recursion to process a dataset one observation at a time, and verify that at the end of the loop, you get the same estimates that you would have gotten if you had just used lm. Do this for the mtcars dataset and the regression mpg ~ wt.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe Sherman–Morrison formula is a great tool! For invertible \\(\\mathbf{A}\\in\\mathbb{R}^{p\\times p}\\) and \\(\\mathbf{v}\\in\\mathbb{R}^p\\), we have\n\\[\n(\\mathbf{A}+\\mathbf{v}\\mathbf{v}^{\\scriptscriptstyle\\mathsf{T}})^{-1}\n=\n\\mathbf{A}^{-1}\n-\n\\frac{\n\\mathbf{A}^{-1}\n\\mathbf{v}\\mathbf{v}^{\\scriptscriptstyle\\mathsf{T}}\n\\mathbf{A}^{-1}\n}{\n1\n+\n\\mathbf{v}^{\\scriptscriptstyle\\mathsf{T}}\n\\mathbf{A}^{-1}\n\\mathbf{v}\n}\n.\n\\]"
  },
  {
    "objectID": "problems/bank/review/bayes.html",
    "href": "problems/bank/review/bayes.html",
    "title": "STA 542 Fall 2025",
    "section": "",
    "text": "Recall the parametric family from the previous part, and consider the Bayesian model\n\\[\n\\begin{aligned}\n\\theta &\\sim \\text{Gamma}(a_0,\\,b_0) \\\\\nx_i\\,|\\, \\theta &\\overset{\\text{iid}}{\\sim }F(x;\\,\\theta).\n\\end{aligned}\n\\]\n\nDerive the posterior distribution \\(p(\\theta\\,|\\,x_{1:n})\\);\nShow that the posterior mean \\(E(\\theta\\,|\\,x_{1:n})\\) can be written as a convex combination of the prior mean and the MLE;\nDerive the posterior predictive distribution \\(p(x_{n+1}\\,|\\,x_{1:n})\\)."
  },
  {
    "objectID": "lecture-notes/ar-1.html",
    "href": "lecture-notes/ar-1.html",
    "title": "Everything You Always Wanted to Know About the AR(1) But Were Afraid to Ask",
    "section": "",
    "text": "A model is a probability distribution over a sequence..\nIn the spirit of Larry Wasserman’s All of Statistics and All of Nonparametric Statistics, this note introduces “All of Time Series Analysis” for the simplest time series model: the autoregression of order 1, or AR(1) for short. Topics include:\nOnce you’re comfortable with all that, everything else in the course is in some sense just a theme and variations on these main ideas."
  },
  {
    "objectID": "lecture-notes/ar-1.html#what-is-a-time-series-model",
    "href": "lecture-notes/ar-1.html#what-is-a-time-series-model",
    "title": "Everything You Always Wanted to Know About the AR(1) But Were Afraid to Ask",
    "section": "What is a time series model?",
    "text": "What is a time series model?\nIn some sense, all we are doing is manipulating joint distributions: computing their marginals, conditionals, means, and covariances. If you can do that, you can “do” time series. Of course, TA has its own special features\nBut dont lose te forest for the trees\nIf you can manipulate joint distributions, then you can do time series analysis is a generic sense. Of course, TS will pose their own special problems that require new techniques, but don’t lost the forest for the trees.\n\n\n\n\n\n\nA quick word on notation\n\n\n\nTwo things you have to get used to:\n\nWe will not use uppercase \\(Y_t\\) versus lowercase \\(y_t\\) to distinguish random variables versus fixed realizations. Everything will just be \\(y_t\\), and context will make clear if something functions as a random variable or a constant;\nThe symbol \\(p\\) will be aggressively abused and overloaded to represent any probability distribution, sometimes several in the same sentence. So \\(p(x)\\) is the density of the random variable \\(x\\), and \\(p(y,\\,z)\\) is the joint density of the random pair \\((y,\\,z)\\), and \\(p(\\boldsymbol{\\beta}\\,|\\,\\sigma^2)\\) is the conditional density of…you get the idea. CHANGE THIS TO REFER TO THE EQUATION\nAlso the \\(y_{i:j}\\) notation"
  },
  {
    "objectID": "lecture-notes/ar-1.html#the-simplest-time-series-model",
    "href": "lecture-notes/ar-1.html#the-simplest-time-series-model",
    "title": "Everything You Always Wanted to Know About the AR(1) But Were Afraid to Ask",
    "section": "The simplest time series model",
    "text": "The simplest time series model\nIn truth, the “simplest” time series model is the one where there is no dependence at all: \\(y_t\\overset{\\text{iid}}{\\sim}F\\). But in that case, why bother with STA 542? Things get interesting when there is dependence, and in particular the kind of dependence where you think the past can help to predict the future. The simplest model that captures this basic idea is the autoregression of order 1, or AR(1) for short:\n\\[\n\\begin{aligned}\ny_t&=\\beta_0+\\beta_1y_{t-1}+\\varepsilon_t,\\quad \\varepsilon_t\\overset{\\text{iid}}{\\sim}\\text{N}(0,\\,\\sigma^2)\\\\\ny_0&\\sim\\text{N}(\\mu_0,\\,\\gamma_0).\n\\end{aligned}\n\\]\nThis takes the form of a simple linear regression where \\(y_t\\) is the response and its first lagged value \\(y_{t-1}\\) is the predictor, hence the name\n\\(y_0\\) independent of the errors.\nI don’t like the notation for the initial condition variance."
  },
  {
    "objectID": "lecture-notes/ar-1.html#what-is-the-joint-distribution",
    "href": "lecture-notes/ar-1.html#what-is-the-joint-distribution",
    "title": "Everything You Always Wanted to Know About the AR(1) But Were Afraid to Ask",
    "section": "What is the joint distribution?",
    "text": "What is the joint distribution?\n\\(y_t\\,|\\,y_{t-1}\\sim\\text{N}(\\beta_0+\\beta_1y_{t-1},\\,\\sigma^2)\\)\n\\[\np(y_{0:T}) = p(y_0)\\prod_{t=1}^Tp(y_t\\,|\\,y_{t-1})\n\\]\nto understand the joint distribution\n\\[\n\\begin{aligned}\ny_0\n&=\ny_0\n\\\\\ny_1\n&=\n\\beta_0+\\beta_1y_0+\\varepsilon_1\n\\\\\ny_2\n&=\n\\beta_0+\\beta_1\n\\\\\ny_3\n&=\n\\\\\n&\\vdots\n\\\\\ny_t\n&=\n\\end{aligned}\n\\]\nsummarized\n\\[\n\\begin{bmatrix}\ny_0\\\\\ny_1\\\\\ny_2\\\\\ny_3\\\\\n\\vdots \\\\\ny_T\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nbloop\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n1 & 0 & 0 & 0 & \\cdots & 0\n\\end{bmatrix}\n\\begin{bmatrix}\ny_0\\\\\n\\varepsilon_1\\\\\n\\varepsilon_2\\\\\n\\varepsilon_3\\\\\n\\vdots \\\\\n\\varepsilon_T\n\\end{bmatrix}\n\\]\nSo \\(\\sim\\text{N}_{T+1}()\\) and \\(\\mathbf{y}\\) is a linear transformation of BLAH, so it is also normal, with mean and covariance. What are these?"
  },
  {
    "objectID": "lecture-notes/ar-1.html#stationarity",
    "href": "lecture-notes/ar-1.html#stationarity",
    "title": "Everything You Always Wanted to Know About the AR(1) But Were Afraid to Ask",
    "section": "Stationarity",
    "text": "Stationarity\nMost interesting time series are not stationary. This comes down to us as an historical artifact because certain kinds of (classical) statistical theory require it as an assumption in order to prove theorems, but it’s never true in practice.\nweird concept. dependent but not too dependent. necessary for stable prediction and inference, but do we really care? Probably false in practice, but so what. I just want a model that generates good and useful predictions.\nImportant historically and theoretically, and it simplifies the model tremendously."
  },
  {
    "objectID": "lecture-notes/ar-1.html#classical-inference",
    "href": "lecture-notes/ar-1.html#classical-inference",
    "title": "Everything You Always Wanted to Know About the AR(1) But Were Afraid to Ask",
    "section": "Classical inference",
    "text": "Classical inference\n\nMethod of moments\n\n\nMaximum likelihood estimaton"
  },
  {
    "objectID": "lecture-notes/ar-1.html#bayesian-inference",
    "href": "lecture-notes/ar-1.html#bayesian-inference",
    "title": "Everything You Always Wanted to Know About the AR(1) But Were Afraid to Ask",
    "section": "Bayesian inference",
    "text": "Bayesian inference\n\\[\n\\begin{aligned}\n\\sigma^2\n&\\sim\n\\text{IG}(a_0,\\,b_0)\n\\\\\n\\boldsymbol{\\beta}\n\\,|\\,\n\\sigma^2\n&\\sim\n\\text{N}_2(\\bar{\\boldsymbol{\\beta}}_0,\\,)\n\\\\\ny_t\\,|\\,y_{t-1},\\,\\boldsymbol{\\beta},\\,\\sigma^2\n&\\sim\n\\end{aligned}\n\\]\nSo we want this:\n\\[\np(\\boldsymbol{\\beta},\\,\\sigma^2\\,|\\,y_{0:T})\n=\n\\frac\n{p(y_{1:T}\\,|\\,\\boldsymbol{\\beta},\\,\\sigma^2,\\,y_0)p(\\boldsymbol{\\beta},\\,\\sigma^2)}\n{p(y_{1:T}\\,|\\,y_0)}\n\\]\nFor simplicity, we’re just going to condition on \\(y_0\\).\nAlso, we do not have to assume stationarity to do this.\nOur prior is conjugate, so we can compute the exact posterior.\n\n\n\n\n\n\nTip\n\n\n\nconjugate updates\n\n\nOther priors: normal - IG that isn’t conjugate, whatever else is in West’s book, the prior that enforces stationarity, either with accept-reject or the Heaps stuff, maybe that West Huerta thing idk."
  },
  {
    "objectID": "lecture-notes/ar-1.html#forecasting",
    "href": "lecture-notes/ar-1.html#forecasting",
    "title": "Everything You Always Wanted to Know About the AR(1) But Were Afraid to Ask",
    "section": "Forecasting",
    "text": "Forecasting\n\nClassical\ntypically ignores estimation uncertainty from coefficients.\nshow a picture comparing\n\n\n\n\n\n\nTip\n\n\n\nsieve bootstrap Buhlmann bernoulli\n\n\nthese draws are a discrete approximation.\npoint forecast: sample mean or median interval forecast: quantiles, HPD, whatever\n\n\nBayes\nProbabilistic prediction is automatic, and it’s easier (not easy, but easier) to augment your inferences with more sources of uncertainty: missing data, model uncertainty, etc."
  },
  {
    "objectID": "lecture-notes/ar-1.html#forecast-evaluation",
    "href": "lecture-notes/ar-1.html#forecast-evaluation",
    "title": "Everything You Always Wanted to Know About the AR(1) But Were Afraid to Ask",
    "section": "Forecast evaluation",
    "text": "Forecast evaluation\nmarginal distributions\njoint distributions\nconditional (forecast) distributions\nstationarity\ndependence structure\nmethod of moments (Yule-Walker)\nmaximum likelihood (unconditional and conditional)\nemphasize sequential recursions\nbayesian inference\nprobabilistic prediction\nparametric bootstrap\npoint prediction\ninterval prediction\ndensity prediction"
  },
  {
    "objectID": "problems/bank/review/mle.html",
    "href": "problems/bank/review/mle.html",
    "title": "STA 542 Fall 2025",
    "section": "",
    "text": "Here is a cdf indexed by a parameter \\(\\theta&gt;0\\):\n\\[\nF(x;\\,\\theta)\n=\n1 - \\frac{1}{(1+x)^\\theta},\\quad x\\geq 0.\n\\]\n\nDescribe an algorithm that will simulate (pseudo)random numbers from this distribution;\nLet \\(x_1,\\,x_2,\\,...,\\,x_n\\overset{\\text{iid}}{\\sim}F(x;\\,\\theta)\\) and derive the maximum likelihood estimator \\(\\hat{\\theta}_n^{(\\text{MLE})}\\) for the parameter \\(\\theta\\);\nDerive the exact sampling distribution of \\(\\hat{\\theta}_n^{(\\text{MLE})}\\);\nImplement the bootstrap to simulate the sampling distribution of \\(\\hat{\\theta}_n^{(\\text{MLE})}\\), and compare the bootstrap distribution to the exact one you derived in part c. In order to do this, you will probably need your algorithm from part a.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nHere is a schematic of how the bootstrap works. We start with some ground truth parameter value \\(\\theta_0&gt;0\\), and then implement this:\n\\[\n\\begin{matrix}\n\\text{0. True distribution} &&& F_{\\theta_0} && \\\\\n&&& \\downarrow && \\\\\n\\text{1. Original data} &&& x_{1:n} && \\\\\n&&& \\downarrow && \\\\\n\\text{2. Empirical distribution} &&& \\hat{F}_{n} && \\\\\n&\\swarrow &\\swarrow& \\cdots &\\searrow&\\searrow \\\\\n\\text{3. Synthetic data}&\\tilde{x}_{1:n}^{(1)} &\\tilde{x}_{1:n}^{(2)}& \\cdots &\\tilde{x}_{1:n}^{(k-1)}&\\tilde{x}_{1:n}^{(k)} \\\\\n&\\downarrow &\\downarrow& \\cdots &\\downarrow&\\downarrow \\\\\n\\text{4. Bootstrap estimates}&\\hat{\\theta}_n^{(1)} &\\hat{\\theta}_n^{(2)}& \\cdots &\\hat{\\theta}_n^{(k-1)}&\\hat{\\theta}_n^{(k)} \\\\\n\\end{matrix}\n\\]\nIf you implemented this correctly, and if the original sample size \\(n\\) is “large enough,” then a histogram of the \\(k\\) estimates \\(\\hat{\\theta}_n^{(1)}\\), \\(\\hat{\\theta}_n^{(2)}\\), …, \\(\\hat{\\theta}_n^{(k)}\\) ought to resemble the exact density you derived in part c."
  },
  {
    "objectID": "problems/bank/review/prediction-interval.html",
    "href": "problems/bank/review/prediction-interval.html",
    "title": "STA 542 Fall 2025",
    "section": "",
    "text": "Consider these data\n\\[\nX_1,\\,X_2,\\,...,\\,X_n\\overset{\\text{iid}}{\\sim}\\text{N}(\\theta,\\,1)\n\\]\nand the usual estimator \\(\\hat{\\theta}_n=\\sum\\limits_{i=1}^nX_i/n\\).\n\nDerive an exact \\(100\\times (1-\\alpha)\\%\\) classical confidence interval for the unknown mean \\(\\theta\\). That is, find a random interval \\((L_n,\\,U_n)\\) satisfying:\n\n\\[\nP\\left(L_n&lt;\\theta&lt;U_n\\right)=1-\\alpha.\n\\]\n\nDerive an exact \\(100\\times (1-\\alpha)\\%\\) classical prediction interval for a new observation \\(X_{n+1}\\). That is, find a random interval \\((L_n',\\,U_n')\\) satisfying\n\n\\[\nP\\left(L_n'&lt;X_{n+1}&lt;U_n'\\right)=1-\\alpha.\n\\]\n\nComment on the difference between the two intervals."
  },
  {
    "objectID": "problems/bank/review/mvnormal-2.html",
    "href": "problems/bank/review/mvnormal-2.html",
    "title": "STA 542 Fall 2025",
    "section": "",
    "text": "Recall that if \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are jointly distributed according to\n\\[\n\\begin{bmatrix}\n\\mathbf{x} \\\\\n\\mathbf{y}\n\\end{bmatrix}\n\\sim\n\\text{N}_{n+m}\n\\left(\n\\begin{bmatrix}\n\\boldsymbol{\\mu}_x \\\\\n\\boldsymbol{\\mu}_y\n\\end{bmatrix}\n,\\,\n\\begin{bmatrix}\n\\boldsymbol{\\Sigma}_x & \\boldsymbol{\\Sigma}_{xy}\\\\\n\\boldsymbol{\\Sigma}_{xy}^{\\scriptscriptstyle\\mathsf{T}} & \\boldsymbol{\\Sigma}_y\n\\end{bmatrix}\n\\right),\n\\]\nthen the conditional distribution is\n\\[\n\\mathbf{y} \\,|\\,\\mathbf{x}\n\\sim\n\\text{N}_m\n\\left(\n\\boldsymbol{\\mu}_y + \\boldsymbol{\\Sigma}_{xy}^{\\scriptscriptstyle\\mathsf{T}}\\boldsymbol{\\Sigma}_x^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_x)\n,\\,\n\\boldsymbol{\\Sigma}_y - \\boldsymbol{\\Sigma}_{xy}^{\\scriptscriptstyle\\mathsf{T}}\n\\boldsymbol{\\Sigma}_x^{-1}\n\\boldsymbol{\\Sigma}_{xy}\n\\right).\n\\]\nWe will use this fact often, but proving it is really nasty. I’m not going to make you do it. This problem set is unreasonable enough already. Instead, I want you to read the proof very carefully. If you’re like me, there is some step in the derivation that will make you say “what just happened?” Drill down on that. Pick a section of the proof where you get confused, or where a technique is being used that you are unfamiliar with. Study that part carefully until you understand it, and then in your problem set write-up, explain it to me like I’m an idiot (shouldn’t be too difficult). Use pictures, numerical examples, specialize to the bivariate case, whatever you need to do.\n\n\n\n\n\n\nDon’t be a silly goose.\n\n\n\nWe are grading this part of the problem set for completion, but give it a good faith effort so that you actually, like, learn something."
  },
  {
    "objectID": "problems/pset-0.html",
    "href": "problems/pset-0.html",
    "title": "Problem Set 0",
    "section": "",
    "text": "Slicing-and-dicing the multivariate normal distribution is a very important skill in time series analysis, so let’s make sure we can do that.\n\nLet \\(\\mathbf{x}\\sim\\text{N}_n(\\boldsymbol{\\mu},\\,\\boldsymbol{\\Sigma})\\), and fix constants \\(\\mathbf{A}\\in\\mathbb{R}^{m\\times n}\\) and \\(\\mathbf{c}\\in\\mathbb{R}^m\\). Show that\n\n\\[\n\\mathbf{y}\n=\n\\mathbf{A}\n\\mathbf{x}\n+\n\\mathbf{c}\n\\sim\n\\text{N}_m\n\\left(\n\\mathbf{A}\n\\boldsymbol{\\mu}\n+\n\\mathbf{c}\n,\\,\n\\mathbf{A}\n\\boldsymbol{\\Sigma}\n\\mathbf{A}^{\\scriptscriptstyle\\mathsf{T}}\n\\right)\n.\n\\]\n\nConsider the joint distribution \\(p(\\mathbf{x},\\,\\mathbf{y})=p(\\mathbf{y}\\,|\\,\\mathbf{x})p(\\mathbf{x})\\) written hierarchically as \\(\\mathbf{x}\\sim\\text{N}_n(\\boldsymbol{\\mu},\\,\\boldsymbol{\\Sigma})\\) and \\(\\mathbf{y}\\,|\\,\\mathbf{x}\\sim\\text{N}_m(\\mathbf{A}\\mathbf{x}+\\mathbf{c},\\,\\mathbf{R})\\). Use the result in part a to show that the joint distribution implied by this hierarchy is\n\n\\[\n\\begin{bmatrix}\n\\mathbf{x}\n\\\\\n\\mathbf{y}\n\\end{bmatrix}\n\\sim\n\\text{N}_{n+m}\n\\left(\n\\begin{bmatrix}\n\\boldsymbol{\\mu}\n\\\\\n\\mathbf{A}\\boldsymbol{\\mu}+\\mathbf{c}\n\\end{bmatrix}\n,\\,\n\\begin{bmatrix}\n\\boldsymbol{\\Sigma}\n&\n\\boldsymbol{\\Sigma}\\mathbf{A}^{\\scriptscriptstyle\\mathsf{T}}\n\\\\\n\\mathbf{A}\n\\boldsymbol{\\Sigma}\n&\n\\mathbf{A}\n\\boldsymbol{\\Sigma}\n\\mathbf{A}^{\\scriptscriptstyle\\mathsf{T}} + \\mathbf{R}\n\\end{bmatrix}\n\\right)\n.\n\\]\n\nUse the result in part a to show that the linear combination of independent normals is normal. That is, if \\(x_i\\sim\\text{N}(\\mu_i,\\,\\sigma^2_i)\\) are independent and \\(a_i\\in\\mathbb{R}\\) are constant, then prove that\n\n\\[\n\\sum\\limits_{i=1}^na_ix_i\\sim\\text{N}\\left(\\sum\\limits_{i=1}^na_i\\mu_i,\\, \\sum\\limits_{i=1}^na_i^2\\sigma_i^2\\right).\n\\]\n\n\n\n\n\n\nHint\n\n\n\n\n\nJZ changed this hint on 8/30/2025. The original hint mentioned using the change-of-variables formula. That’s a fine solution if \\(\\mathbf{A}\\) is invertible, but the result holds even if \\(\\mathbf{A}\\) is not invertible, and we will often use that version (e.g. part c here). If you used change-of-variables, don’t redo it. We’ll award full credit if you did it right. But otherwise, consider using the moment-generating function of a random vector \\(\\mathbf{x}\\):\n\\[\nM(\\mathbf{t})=E\\left(e^{\\mathbf{t}^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{x}}\\right).\n\\]",
    "crumbs": [
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "problems/pset-0.html#problem-1",
    "href": "problems/pset-0.html#problem-1",
    "title": "Problem Set 0",
    "section": "",
    "text": "Slicing-and-dicing the multivariate normal distribution is a very important skill in time series analysis, so let’s make sure we can do that.\n\nLet \\(\\mathbf{x}\\sim\\text{N}_n(\\boldsymbol{\\mu},\\,\\boldsymbol{\\Sigma})\\), and fix constants \\(\\mathbf{A}\\in\\mathbb{R}^{m\\times n}\\) and \\(\\mathbf{c}\\in\\mathbb{R}^m\\). Show that\n\n\\[\n\\mathbf{y}\n=\n\\mathbf{A}\n\\mathbf{x}\n+\n\\mathbf{c}\n\\sim\n\\text{N}_m\n\\left(\n\\mathbf{A}\n\\boldsymbol{\\mu}\n+\n\\mathbf{c}\n,\\,\n\\mathbf{A}\n\\boldsymbol{\\Sigma}\n\\mathbf{A}^{\\scriptscriptstyle\\mathsf{T}}\n\\right)\n.\n\\]\n\nConsider the joint distribution \\(p(\\mathbf{x},\\,\\mathbf{y})=p(\\mathbf{y}\\,|\\,\\mathbf{x})p(\\mathbf{x})\\) written hierarchically as \\(\\mathbf{x}\\sim\\text{N}_n(\\boldsymbol{\\mu},\\,\\boldsymbol{\\Sigma})\\) and \\(\\mathbf{y}\\,|\\,\\mathbf{x}\\sim\\text{N}_m(\\mathbf{A}\\mathbf{x}+\\mathbf{c},\\,\\mathbf{R})\\). Use the result in part a to show that the joint distribution implied by this hierarchy is\n\n\\[\n\\begin{bmatrix}\n\\mathbf{x}\n\\\\\n\\mathbf{y}\n\\end{bmatrix}\n\\sim\n\\text{N}_{n+m}\n\\left(\n\\begin{bmatrix}\n\\boldsymbol{\\mu}\n\\\\\n\\mathbf{A}\\boldsymbol{\\mu}+\\mathbf{c}\n\\end{bmatrix}\n,\\,\n\\begin{bmatrix}\n\\boldsymbol{\\Sigma}\n&\n\\boldsymbol{\\Sigma}\\mathbf{A}^{\\scriptscriptstyle\\mathsf{T}}\n\\\\\n\\mathbf{A}\n\\boldsymbol{\\Sigma}\n&\n\\mathbf{A}\n\\boldsymbol{\\Sigma}\n\\mathbf{A}^{\\scriptscriptstyle\\mathsf{T}} + \\mathbf{R}\n\\end{bmatrix}\n\\right)\n.\n\\]\n\nUse the result in part a to show that the linear combination of independent normals is normal. That is, if \\(x_i\\sim\\text{N}(\\mu_i,\\,\\sigma^2_i)\\) are independent and \\(a_i\\in\\mathbb{R}\\) are constant, then prove that\n\n\\[\n\\sum\\limits_{i=1}^na_ix_i\\sim\\text{N}\\left(\\sum\\limits_{i=1}^na_i\\mu_i,\\, \\sum\\limits_{i=1}^na_i^2\\sigma_i^2\\right).\n\\]\n\n\n\n\n\n\nHint\n\n\n\n\n\nJZ changed this hint on 8/30/2025. The original hint mentioned using the change-of-variables formula. That’s a fine solution if \\(\\mathbf{A}\\) is invertible, but the result holds even if \\(\\mathbf{A}\\) is not invertible, and we will often use that version (e.g. part c here). If you used change-of-variables, don’t redo it. We’ll award full credit if you did it right. But otherwise, consider using the moment-generating function of a random vector \\(\\mathbf{x}\\):\n\\[\nM(\\mathbf{t})=E\\left(e^{\\mathbf{t}^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{x}}\\right).\n\\]",
    "crumbs": [
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "problems/pset-0.html#problem-2",
    "href": "problems/pset-0.html#problem-2",
    "title": "Problem Set 0",
    "section": "Problem 2",
    "text": "Problem 2\nRecall that if \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are jointly distributed according to\n\\[\n\\begin{bmatrix}\n\\mathbf{x} \\\\\n\\mathbf{y}\n\\end{bmatrix}\n\\sim\n\\text{N}_{n+m}\n\\left(\n\\begin{bmatrix}\n\\boldsymbol{\\mu}_x \\\\\n\\boldsymbol{\\mu}_y\n\\end{bmatrix}\n,\\,\n\\begin{bmatrix}\n\\boldsymbol{\\Sigma}_x & \\boldsymbol{\\Sigma}_{xy}\\\\\n\\boldsymbol{\\Sigma}_{xy}^{\\scriptscriptstyle\\mathsf{T}} & \\boldsymbol{\\Sigma}_y\n\\end{bmatrix}\n\\right),\n\\]\nthen the conditional distribution is\n\\[\n\\mathbf{y} \\,|\\,\\mathbf{x}\n\\sim\n\\text{N}_m\n\\left(\n\\boldsymbol{\\mu}_y + \\boldsymbol{\\Sigma}_{xy}^{\\scriptscriptstyle\\mathsf{T}}\\boldsymbol{\\Sigma}_x^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_x)\n,\\,\n\\boldsymbol{\\Sigma}_y - \\boldsymbol{\\Sigma}_{xy}^{\\scriptscriptstyle\\mathsf{T}}\n\\boldsymbol{\\Sigma}_x^{-1}\n\\boldsymbol{\\Sigma}_{xy}\n\\right).\n\\]\nWe will use this fact often, but proving it is really nasty. I’m not going to make you do it. This problem set is unreasonable enough already. Instead, I want you to read the proof very carefully. If you’re like me, there is some step in the derivation that will make you say “what just happened?” Drill down on that. Pick a section of the proof where you get confused, or where a technique is being used that you are unfamiliar with. Study that part carefully until you understand it, and then in your problem set write-up, explain it to me like I’m an idiot (shouldn’t be too difficult). Use pictures, numerical examples, specialize to the bivariate case, whatever you need to do.\n\n\n\n\n\n\nDon’t be a silly goose.\n\n\n\nWe are grading this part of the problem set for completion, but give it a good faith effort so that you actually, like, learn something.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "problems/pset-0.html#problem-3",
    "href": "problems/pset-0.html#problem-3",
    "title": "Problem Set 0",
    "section": "Problem 3",
    "text": "Problem 3\nConsider these data\n\\[\nX_1,\\,X_2,\\,...,\\,X_n\\overset{\\text{iid}}{\\sim}\\text{N}(\\theta,\\,1)\n\\]\nand the usual estimator \\(\\hat{\\theta}_n=\\sum\\limits_{i=1}^nX_i/n\\).\n\nDerive an exact \\(100\\times (1-\\alpha)\\%\\) classical confidence interval for the unknown mean \\(\\theta\\). That is, find a random interval \\((L_n,\\,U_n)\\) satisfying:\n\n\\[\nP\\left(L_n&lt;\\theta&lt;U_n\\right)=1-\\alpha.\n\\]\n\nDerive an exact \\(100\\times (1-\\alpha)\\%\\) classical prediction interval for a new observation \\(X_{n+1}\\). That is, find a random interval \\((L_n',\\,U_n')\\) satisfying\n\n\\[\nP\\left(L_n'&lt;X_{n+1}&lt;U_n'\\right)=1-\\alpha.\n\\]\n\nComment on the difference between the two intervals.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "problems/pset-0.html#problem-4",
    "href": "problems/pset-0.html#problem-4",
    "title": "Problem Set 0",
    "section": "Problem 4",
    "text": "Problem 4\nHere is a cdf indexed by a parameter \\(\\theta&gt;0\\):\n\\[\nF(x;\\,\\theta)\n=\n1 - \\frac{1}{(1+x)^\\theta},\\quad x\\geq 0.\n\\]\n\nDescribe an algorithm that will simulate (pseudo)random numbers from this distribution;\nLet \\(x_1,\\,x_2,\\,...,\\,x_n\\overset{\\text{iid}}{\\sim}F(x;\\,\\theta)\\) and derive the maximum likelihood estimator \\(\\hat{\\theta}_n^{(\\text{MLE})}\\) for the parameter \\(\\theta\\);\nDerive the exact sampling distribution of \\(\\hat{\\theta}_n^{(\\text{MLE})}\\);\nImplement the bootstrap to simulate the sampling distribution of \\(\\hat{\\theta}_n^{(\\text{MLE})}\\), and compare the bootstrap distribution to the exact one you derived in part c. In order to do this, you will probably need your algorithm from part a.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nHere is a schematic of how the bootstrap works. We start with some ground truth parameter value \\(\\theta_0&gt;0\\), and then implement this:\n\\[\n\\begin{matrix}\n\\text{0. True distribution} &&& F_{\\theta_0} && \\\\\n&&& \\downarrow && \\\\\n\\text{1. Original data} &&& x_{1:n} && \\\\\n&&& \\downarrow && \\\\\n\\text{2. Empirical distribution} &&& \\hat{F}_{n} && \\\\\n&\\swarrow &\\swarrow& \\cdots &\\searrow&\\searrow \\\\\n\\text{3. Synthetic data}&\\tilde{x}_{1:n}^{(1)} &\\tilde{x}_{1:n}^{(2)}& \\cdots &\\tilde{x}_{1:n}^{(k-1)}&\\tilde{x}_{1:n}^{(k)} \\\\\n&\\downarrow &\\downarrow& \\cdots &\\downarrow&\\downarrow \\\\\n\\text{4. Bootstrap estimates}&\\hat{\\theta}_n^{(1)} &\\hat{\\theta}_n^{(2)}& \\cdots &\\hat{\\theta}_n^{(k-1)}&\\hat{\\theta}_n^{(k)} \\\\\n\\end{matrix}\n\\]\nIf you implemented this correctly, and if the original sample size \\(n\\) is “large enough,” then a histogram of the \\(k\\) estimates \\(\\hat{\\theta}_n^{(1)}\\), \\(\\hat{\\theta}_n^{(2)}\\), …, \\(\\hat{\\theta}_n^{(k)}\\) ought to resemble the exact density you derived in part c.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "problems/pset-0.html#problem-5",
    "href": "problems/pset-0.html#problem-5",
    "title": "Problem Set 0",
    "section": "Problem 5",
    "text": "Problem 5\nRecall the parametric family from the previous part, and consider the Bayesian model\n\\[\n\\begin{aligned}\n\\theta &\\sim \\text{Gamma}(a_0,\\,b_0) \\\\\nx_i\\,|\\, \\theta &\\overset{\\text{iid}}{\\sim }F(x;\\,\\theta).\n\\end{aligned}\n\\]\n\nDerive the posterior distribution \\(p(\\theta\\,|\\,x_{1:n})\\);\nShow that the posterior mean \\(E(\\theta\\,|\\,x_{1:n})\\) can be written as a convex combination of the prior mean and the MLE;\nDerive the posterior predictive distribution \\(p(x_{n+1}\\,|\\,x_{1:n})\\).",
    "crumbs": [
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "problems/pset-0.html#problem-6",
    "href": "problems/pset-0.html#problem-6",
    "title": "Problem Set 0",
    "section": "Problem 6",
    "text": "Problem 6\nAn important consideration in time series analysis is sequential or online inference. In this environment, our data are not available to us all at once in a batch. Instead, the observations are streaming; they are arriving one-after-another in real-time, and we want to come up with a scheme that allows us to recursively update our inferences as new information arrives. Let’s explore this in the context of least squares regression.\n\n\n\n\n\n\nOLS review\n\n\n\nConsider the usual setting where we observe iid pairs \\(y_i\\in\\mathbb{R}\\) and \\(\\mathbf{x}_i\\in\\mathbb{R}^p\\) from the linear model:\n\\[\ny_i= \\mathbf{x}_i^{\\scriptscriptstyle\\mathsf{T}}\\boldsymbol{\\beta}+\\varepsilon_i,\\quad \\varepsilon_i\\overset{\\text{iid}}{\\sim}\\text{N}(0,\\,\\sigma^2).\n\\] Given \\(n\\) observations, we can form the matrices\n\\[\n\\mathbf{y}_n\n=\n\\underbrace{\n\\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix}\n}\n_{n\\times 1}\n\\quad\n\\mathbf{X}_n\n=\n\\underbrace{\n\\begin{bmatrix}\n\\mathbf{x}_1^{\\scriptscriptstyle\\mathsf{T}} \\\\\n\\mathbf{x}_2^{\\scriptscriptstyle\\mathsf{T}} \\\\\n\\vdots \\\\\n\\mathbf{x}_n^{\\scriptscriptstyle\\mathsf{T}}\n\\end{bmatrix}\n}_{n\\times p}\n\\quad\n\\boldsymbol{\\varepsilon}_n\n=\n\\underbrace{\n\\begin{bmatrix}\n\\varepsilon_1 \\\\\n\\varepsilon_2 \\\\\n\\vdots \\\\\n\\varepsilon_n\n\\end{bmatrix}\n}\n_{n\\times 1}\n,\n\\] and we can rewrite the model in matrix form:\n\\[\n\\mathbf{y}_n=\\mathbf{X}_n\\boldsymbol{\\beta}+\\boldsymbol{\\varepsilon}_n,\\quad \\boldsymbol{\\varepsilon}_n\\sim\\text{N}_n(\\mathbf{0},\\,\\sigma^2\\mathbf{I}_n).\n\\] The ordinary least squares (OLS) estimator of the regression coefficients is \\(\\hat{\\boldsymbol{\\beta}}_n=(\\mathbf{X}_n^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{X}_n)^{-1}\\mathbf{X}_n^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{y}_n.\\)\n\n\nHopefully that was all review, and it describes how to do batch or offline inference for the linear model. We are assuming that all of our data are available in a single batch, and we only need to compute the estimate once. But in our class, we will often care about streaming or online inference: efficiently computing the sequence of estimates \\(\\hat{\\boldsymbol{\\beta}}_1\\), \\(\\hat{\\boldsymbol{\\beta}}_2\\), \\(\\hat{\\boldsymbol{\\beta}}_3\\), … as the pairs \\((\\mathbf{x}_1,\\,y_1)\\), \\((\\mathbf{x}_2,\\,y_2)\\), \\((\\mathbf{x}_3,\\,y_3)\\), … arrive one-after-another in real-time.\n\nSo, imagine we have observed \\(n-1\\) pairs, and we have computed the estimate \\(\\hat{\\boldsymbol{\\beta}}_{n-1}=(\\mathbf{X}_{n-1}^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{X}_{n-1})^{-1}\\mathbf{X}_{n-1}^{\\scriptscriptstyle\\mathsf{T}}\\mathbf{y}_{n-1}.\\) Then, a new observation \\((\\mathbf{x}_n,\\,y_n)\\) arrives. How can we efficiently update our estimate of \\(\\boldsymbol{\\beta}\\) to incorporate this new information? In other words, how can we quickly compute \\(\\hat{\\boldsymbol{\\beta}}_{n}\\) given only \\(\\hat{\\boldsymbol{\\beta}}_{n-1}\\) and the new \\((\\mathbf{x}_n,\\,y_n)\\)? Of course, we could always just add new rows to \\(\\mathbf{X}_{n-1}\\) and \\(\\mathbf{y}_{n-1}\\) and recompute the whole estimate from scratch, but that is super inefficient and we can do better. To that end, show that the new estimate is related to the old estimate and the new data by the following recursion: \\(\\hat{\\boldsymbol{\\beta}}_n=\\hat{\\boldsymbol{\\beta}}_{n-1}+\\mathbf{k}_n(y_n-\\mathbf{x}_n^{\\scriptscriptstyle\\mathsf{T}}\\hat{\\boldsymbol{\\beta}}_{n-1})\\). What is \\(\\mathbf{k}_n\\)?\nInterpret the recursion you derived in part a. Isn’t \\((y_n-\\mathbf{x}_n^{\\scriptscriptstyle\\mathsf{T}}\\hat{\\boldsymbol{\\beta}}_{n-1})\\) a residual? What is this \\(\\mathbf{k}_n\\) thing doing? What would it mean if we had \\(\\hat{\\boldsymbol{\\beta}}_n=\\hat{\\boldsymbol{\\beta}}_{n-1}\\)?\nWrite a for loop in R that uses your recursion to process a dataset one observation at a time, and verify that at the end of the loop, you get the same estimates that you would have gotten if you had just used lm. Do this for the mtcars dataset and the regression mpg ~ wt.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\nThe Sherman–Morrison formula is a great tool! For invertible \\(\\mathbf{A}\\in\\mathbb{R}^{p\\times p}\\) and \\(\\mathbf{v}\\in\\mathbb{R}^p\\), we have\n\\[\n(\\mathbf{A}+\\mathbf{v}\\mathbf{v}^{\\scriptscriptstyle\\mathsf{T}})^{-1}\n=\n\\mathbf{A}^{-1}\n-\n\\frac{\n\\mathbf{A}^{-1}\n\\mathbf{v}\\mathbf{v}^{\\scriptscriptstyle\\mathsf{T}}\n\\mathbf{A}^{-1}\n}{\n1\n+\n\\mathbf{v}^{\\scriptscriptstyle\\mathsf{T}}\n\\mathbf{A}^{-1}\n\\mathbf{v}\n}\n.\n\\]",
    "crumbs": [
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "problems/pset-0.html#submission",
    "href": "problems/pset-0.html#submission",
    "title": "Problem Set 0",
    "section": "Submission",
    "text": "Submission\nYou are free to compose your solutions for this problem set however you wish (scan or photograph written work, handwriting capture on a tablet device, LaTeX, Quarto, whatever) as long as the final product is a single PDF file. You must upload this to Gradescope and mark the pages associated with each problem.\nDo not forget to include the following:\n\nFor each problem, please acknowledge your collaborators;\nIf a problem required you to code something, please include both the code and the output.",
    "crumbs": [
      "Problem Sets",
      "Problem Set 0"
    ]
  },
  {
    "objectID": "slides/01-ar-1-structure.html#time-series",
    "href": "slides/01-ar-1-structure.html#time-series",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Time series",
    "text": "Time series\n\nA time series is a set of measurements collected over time;\nWe model these data as a sequence of dependent random variables:\n\n\\[\n\\By_{0:T} = \\{\\By_0,\\,\\By_1,\\,\\By_2,\\,...,\\,\\By_T\\}.\n\\]\n\nA time series model is “just” their joint probability distribution:\n\n\\[\np(\\By_{0:T}) = p(\\By_0)\\prod_{t=1}^Tp(\\By_t\\,|\\,\\By_{0:t-1}).\n\\]"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#course-themes",
    "href": "slides/01-ar-1-structure.html#course-themes",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Course themes",
    "text": "Course themes\nWe will focus on a small set of themes, but go deep on them:\n\n\nInference should be sequential. We want recursive estimation techniques to handle data that are streaming in real-time;\nPredictions should be probabilistic. We want point, interval, and density forecasts that incorporate many sources of uncertainty;\nIf you can manipulate joint distributions, you can do time series analysis. This is obscured in too many TS texts;\n\n\n\nAnd there is a secret fourth theme:\n\n\nA Bayesian approach is an excellent way of achieving the goals of sequential inference and probabilistic prediction."
  },
  {
    "objectID": "slides/01-ar-1-structure.html#the-simplest-non-trivial-time-series-model",
    "href": "slides/01-ar-1-structure.html#the-simplest-non-trivial-time-series-model",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "The simplest non-trivial time series model",
    "text": "The simplest non-trivial time series model\nThe autoregression of order 1, or AR(1):\n\n\\[\n\\begin{aligned}\ny_t\n&=\n\\beta_0\n+\n\\beta_1\ny_{t-1}\n+\n\\varepsilon_t,\n\\quad\n\\varepsilon_t\\iid\\text{N}(0\\com\\sigma^2)\n\\\\\ny_0\n&\\sim\n\\text{N}(\\mu_0\\com \\initvar),\n\\end{aligned}\n\\]\n\n\nThat’s the recursive form. It implies a joint distribution written marginal-conditional style:\n\n\n\\[\n\\begin{aligned}\np(y_{0:T})\n&=\np(y_0)\n\\prod_{t=1}^T\np(y_t\\given y_{0:t-1})\n\\\\\n&=\np(y_0)\n\\prod_{t=1}^T\np(y_t\\given y_{t-1}).\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#putting-the-regression-in-autoregression",
    "href": "slides/01-ar-1-structure.html#putting-the-regression-in-autoregression",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Putting the “regression” in autoregression",
    "text": "Putting the “regression” in autoregression\nIn some sense the AR(1) is “just” a simple linear regression\n\\[\ny_t\n=\n\\beta_0\n+\n\\beta_1\nx_t\n+\n\\varepsilon_t,\n\\quad\n\\varepsilon_t\\iid\\text{N}(0\\com\\sigma^2),\n\\]\nwhere we took the predictor to be \\(x_t=y_{t-1}\\).\nThis perspective obscures the dependence structure, but will be useful for likelihood-based inference next week."
  },
  {
    "objectID": "slides/01-ar-1-structure.html#the-game-plan-for-the-next-few-lectures",
    "href": "slides/01-ar-1-structure.html#the-game-plan-for-the-next-few-lectures",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "The game plan for the next few lectures",
    "text": "The game plan for the next few lectures\nDo “All of Time Series Analysis” for this simple model:\n\n\nwhat is the joint distribution and what is its structure (marginals, conditionals, moments);\nstationarity;\nclassical inference;\nBayesian inference;\n(emphasizing recursive estimation in both cases);\nprobabilistic forecasting from both inferential perspectives;\nevaluating probabilistic forecasts.\n\n\n\nThe rest of the course is in some sense theme and variations."
  },
  {
    "objectID": "slides/01-ar-1-structure.html#last-time-substitution-fest",
    "href": "slides/01-ar-1-structure.html#last-time-substitution-fest",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Last time: substitution fest",
    "text": "Last time: substitution fest\n\n\\(t=0\\):\n\\[\ny_0=y_0\n\\]\n\n\n\\(t=1\\):\n\\[\ny_1=\\beta_0+\\beta_1y_0+\\varepsilon_1\n\\]\n\n\n\\(t=2\\):\n\\[\n\\begin{aligned}\ny_2\n&=\n\\beta_0\n+\n\\beta_1\ny_1\n+\n\\varepsilon_2\n\\\\\n&=\n\\beta_0\n+\n\\beta_1\n(\\beta_0+\\beta_1y_0+\\varepsilon_1)\n+\n\\varepsilon_2\n\\\\\n&=\n\\beta_0+\\beta_0\\beta_1+\\beta_1^2y_0+\\beta_1\\varepsilon_1+\\varepsilon_2.\n\\end{aligned}\n\\]\n\n\n\\(t=3\\):\n\\[\n\\begin{aligned}\ny_3\n&=\n\\beta_0\n+\n\\beta_1\ny_2\n+\n\\varepsilon_3\n\\\\\n&=\n\\beta_0\n+\n\\beta_1\n(\\beta_0+\\beta_0\\beta_1+\\beta_1^2y_0+\\beta_1\\varepsilon_1+\\varepsilon_2)\n+\n\\varepsilon_3\n\\\\\n&=\n\\beta_0+\\beta_0\\beta_1+\\beta_0\\beta_1^2+\\beta_1^3y_0+\\beta_1^2\\varepsilon_1+\\beta_1\\varepsilon_2+\\varepsilon_3.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#get-organized-and-notice-the-pattern",
    "href": "slides/01-ar-1-structure.html#get-organized-and-notice-the-pattern",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Get organized and notice the pattern",
    "text": "Get organized and notice the pattern\n\\[\n\\begin{aligned}\ny_0\n&=\n&\ny_0\n\\\\\ny_1\n&=\n\\beta_0\n&+\\,\n\\beta_1y_0\n&+\\,\n{\\color{white}\\beta_{\\color{white}1}}\\varepsilon_1\n\\\\\ny_2\n&=\n\\beta_0+\\beta_0\\beta_1\n&+\\,\n\\beta_1^2y_0\n&+\\,\n\\beta_1\\varepsilon_1\n+\n{\\color{white}\\beta_{\\color{white}1}}\\varepsilon_2\n\\\\\ny_3\n&=\n\\beta_0+\\beta_0\\beta_1+\\beta_0\\beta_1^2\n&+\\,\n\\beta_1^3y_0\n&+\\,\n\\beta_1^2\\varepsilon_1\n+\n\\beta_1\\varepsilon_2\n+\n\\varepsilon_3\n\\\\\n&\\vdots\n\\\\\ny_t\n&=\n\\beta_0\\sum\\limits_{i=0}^{t-1}\\beta_1^i\n&+\\,\n\\beta_1^ty_0\n&+\\,\n\\sum\\limits_{i=0}^{t-1}\\beta_1^i\\varepsilon_{t-i}\n\\\\\n&\\vdots\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#the-matrix-equation",
    "href": "slides/01-ar-1-structure.html#the-matrix-equation",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "The matrix equation",
    "text": "The matrix equation\nWriting the linear system as a matrix equation, you get:\n\n\\[\n\\begin{aligned}\n\\underbrace{\n\\begin{bmatrix}\ny_0\n\\\\\ny_1\n\\\\\ny_2\n\\\\\ny_3\n\\\\\n\\vdots\n\\\\\ny_T\n\\end{bmatrix}\n}_{\\By}\n&=\n\\underbrace{\n\\beta_0\n\\begin{bmatrix}\n0\n\\\\\n1\n\\\\\n1+\\beta_1\n\\\\\n1+\\beta_1+\\beta_1^2\n\\\\\n\\vdots\n\\\\\n\\sum\\limits_{i=0}^{T-1}\\beta_1^i\n\\end{bmatrix}\n}_{\\Bc}\n+\n\\underbrace{\n\\begin{bmatrix}\n1 & 0 & 0 & 0 & \\cdots & 0 \\\\\n\\beta_1 & 1 & 0 & 0 & \\cdots & 0 \\\\\n\\beta_1^2 & \\beta_1 & 1 & 0 & \\cdots & 0 \\\\\n\\beta_1^3 & \\beta_1^2 & \\beta_1 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\beta_1^T & \\beta_1^{T-1} & \\beta_1^{T-2} & \\beta_1^{T-3} & \\cdots & 1 \\\\\n\\end{bmatrix}\n}_{\\BA}\n\\underbrace{\n\\begin{bmatrix}\ny_0\n\\\\\n\\varepsilon_1\n\\\\\n\\varepsilon_2\n\\\\\n\\varepsilon_3\n\\\\\n\\vdots\n\\\\\n\\varepsilon_T\n\\end{bmatrix}\n}_{\\Be}\n.\n\\end{aligned}\n\\]\n\n\nBy assumption,\n\\[\n\\Be\n\\sim\n\\text{N}_{T+1}\n\\left(\n\\Bm=\n\\begin{bmatrix}\n\\mu_0\n\\\\\n\\Bzero\n\\end{bmatrix}\n\\com\n\\BS\n=\n\\begin{bmatrix}\n\\initvar\n&\n\\Bzero^\\tr\\\\\n\\Bzero & \\sigma^2\\BI_T\n\\end{bmatrix}\n\\right)\n.\n\\]\n\n\nSo by linearity,\n\\[\n\\By\n\\sim\n\\text{N}_{T+1}\n\\left(\n\\Bmu=\\Bc+\\BA\\Bm\n\\com\n\\BSigma\n=\n\\BA\\BS\\BA^\\tr\n\\right)\n.\n\\]"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#whats-the-mean",
    "href": "slides/01-ar-1-structure.html#whats-the-mean",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "What’s the mean?",
    "text": "What’s the mean?\n\nRecall that\n\\[\nE\\left(\n\\sum\\limits_{i=1}^na_iX_i\n\\right)\n=\n\\sum\\limits_{i=1}^na_iE(X_i).\n\\]\n\n\nSo:\n\\[\n\\begin{aligned}\nE(y_t)\n&=\nE\n\\left(\n\\beta_0\\sum\\limits_{i=0}^{t-1}\\beta_1^i\n+\n\\beta_1^ty_0\n+\n\\sum\\limits_{i=0}^{t-1}\\beta_1^i\\varepsilon_{t-i}\n\\right)\n\\\\\n&=\n\\beta_0\\sum\\limits_{i=0}^{t-1}\\beta_1^i\n+\n\\beta_1^t\nE\n\\left(\ny_0\n\\right)\n+\n\\sum\\limits_{i=0}^{t-1}\\beta_1^i\nE\n\\left(\n\\varepsilon_{t-i}\n\\right)\n\\\\\n&=\n\\beta_0\\sum\\limits_{i=0}^{t-1}\\beta_1^i\n+\n\\beta_1^t\n\\mu_0.\n\\end{aligned}\n\\]\n\n\nWe only used \\(E(\\varepsilon_t)=0\\). Didn’t need independence or normality."
  },
  {
    "objectID": "slides/01-ar-1-structure.html#whats-the-variance",
    "href": "slides/01-ar-1-structure.html#whats-the-variance",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "What’s the variance?",
    "text": "What’s the variance?\n\nRecall that for independent random variables,\n\\[\n\\var\\left(\n\\sum\\limits_{i=1}^na_iX_i\n\\right)\n=\n\\sum\\limits_{i=1}^na_i^2\\var(X_i).\n\\]\n\n\nSo:\n\\[\n\\begin{aligned}\n\\var(y_t)\n&=\n\\var\n\\left(\n\\beta_0\\sum\\limits_{i=0}^{t-1}\\beta_1^i\n+\n\\beta_1^ty_0\n+\n\\sum\\limits_{i=0}^{t-1}\\beta_1^i\\varepsilon_{t-i}\n\\right)\n\\\\\n&=\n\\beta_1^{2t}\n\\var\n\\left(\ny_0\n\\right)\n+\n\\sum\\limits_{i=0}^{t-1}\\beta_1^{2i}\n\\var\n\\left(\n\\varepsilon_{t-i}\n\\right)\n\\\\\n&=\n\\beta_1^{2t}\\initvar\n+\n\\sigma^2\n\\sum\\limits_{i=0}^{t-1}\\beta_1^{2i}.\n\\end{aligned}\n\\]\n\n\nWe used time-invariance and independence of \\(\\varepsilon_t\\) but not normality."
  },
  {
    "objectID": "slides/01-ar-1-structure.html#whats-the-covariance",
    "href": "slides/01-ar-1-structure.html#whats-the-covariance",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "What’s the covariance?",
    "text": "What’s the covariance?\nHere you go:\n\\[\n\\cov(y_t\\com y_s)\n=\n\\begin{cases}\n\\beta_1^{s-t}\\var(y_t) & t\\leq s\\\\\n\\beta_1^{t-s}\\var(y_s) & s &lt; t.\n\\end{cases}\n\\]\nDerivation deferred to Problem Set 1."
  },
  {
    "objectID": "slides/01-ar-1-structure.html#summary",
    "href": "slides/01-ar-1-structure.html#summary",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Summary",
    "text": "Summary\n\nRecursive form:\n\\[\n\\begin{aligned}\ny_t\n&=\n\\beta_0\n+\n\\beta_1\ny_{t-1}\n+\n\\varepsilon_t,\n\\quad\n\\varepsilon_t\\iid\\text{N}(0\\com\\sigma^2)\n\\\\\ny_0\n&\\sim\n\\text{N}(\\mu_0\\com \\initvar).\n\\end{aligned}\n\\]\n\n\nJoint distribution:\n\\[\n\\begin{bmatrix}\ny_0 & y_1 & \\cdots & y_T\n\\end{bmatrix}^\\tr\n\\sim\\text{N}_{T+1}\\left(\\Bmu\\com \\BSigma\\right).\n\\]\n\n\nMoments:\n\\[\n\\begin{aligned}\nE(y_t)\n&=\n\\beta_0\\sum\\limits_{i=0}^{t-1}\\beta_1^i\n+\n\\beta_1^t\\mu_0\n\\\\\n\\var(y_t)\n&=\n\\sigma^2\n\\sum\\limits_{i=0}^{t-1}\\beta_1^{2i}+\n\\beta_1^{2t}\\initvar\n\\\\\n\\cov(y_t\\com y_s)\n&=\n\\begin{cases}\n\\beta_1^{s-t}\\var(y_t) & t\\leq s\\\\\n\\beta_1^{t-s}\\var(y_s) & s &lt; t.\n\\end{cases}\n.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#some-special-cases",
    "href": "slides/01-ar-1-structure.html#some-special-cases",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Some special cases",
    "text": "Some special cases\n\n\niid: set \\(\\beta_1=0\\) (and \\(\\mu_0=\\beta_0\\); \\(\\initvar=\\sigma^2\\)), and\n\n\n\\[\ny_t\\iid\\text{N}(\\beta_0\\com\\sigma^2).\n\\]\n\n\n\n\nrandom walk with drift: set \\(\\beta_1=1\\), and\n\n\n\n\\[\n\\begin{aligned}\nE(y_t)\n&=\n\\beta_0t+\\mu_0\n\\\\\n\\var(y_t)\n&=\n\\sigma^2t+\\initvar.\n\\end{aligned}\n\\]\n\n\n\n\nfunky: set \\(\\beta_1=-1\\), and\n\n\n\n\\[\n\\begin{aligned}\nE(y_t)\n&=\n\\beta_0\\frac{1 - (-1)^t}{2}+(-1)^t\\mu_0\n\\\\\n\\var(y_t)\n&=\n\\sigma^2t+\\initvar.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#what-does-this-look-like",
    "href": "slides/01-ar-1-structure.html#what-does-this-look-like",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "What does this look like?",
    "text": "What does this look like?\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\nlibrary(shiny)\n\nsimulate_ar_1 &lt;- function(T, b0, b1, s, m0, s0){\n  y &lt;- numeric(T)\n  y[1] &lt;- rnorm(1, m0, s0)\n  for(t in 2:T){\n    y[t] &lt;- b0 + b1 * y[t - 1] + rnorm(1, 0, s)\n  }\n  return(y)\n}\n\nar_1_mean &lt;- function(t, b0, b1, m0){\n  if(t == 0){\n    return(m0)\n  }else{\n    return(b0 * sum(b1 ^ (0:(t-1))) + m0 * (b1^t)) \n  }\n}\n\nar_1_var &lt;- function(t, b1, s, s0){\n  if(t == 0){\n    return(s0^2)\n  }else{\n    return((s0^2) * (b1^(2*t)) + (s^2) * sum(b1 ^ (2*(0:(t-1)))))\n  }\n}\n\nar_1_sd &lt;- function(t, b1, s, s0){\n  sqrt(ar_1_var(t, b1, s, s0))\n}\n\n# Define UI for application that draws a histogram\nui &lt;- fluidPage(\n  \n  # Application title\n  titlePanel(\"Marginal distributions and sample paths of a Gaussian AR(1)\"),\n  \n  # Sidebar with a slider input for number of bins \n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"b0\",\n                  \"β₀\",\n                  min = -5,\n                  max = 5,\n                  value = 0,\n                  step = 0.1),\n      sliderInput(\"b1\",\n                  \"β₁\",\n                  min = -2,\n                  max = 2,\n                  value = 0,\n                  step = 0.1),\n      sliderInput(\"s\",\n                  \"σ\",\n                  min = 0,\n                  max = 2,\n                  value = 1, \n                  step = 0.1),\n      sliderInput(\"m0\",\n                  \"μ₀\",\n                  min = -5,\n                  max = 5,\n                  value = 0,\n                  step = 0.1),\n      sliderInput(\"T\",\n                  \"T\",\n                  min = 20,\n                  max = 200,\n                  step = 20,\n                  value = 100),\n      actionButton(\"redo\", \"New sample path\"),\n    ),\n    \n    # Show a plot of the generated distribution\n    mainPanel(\n      plotOutput(\"distPlot\", height = \"600px\")\n    )\n  )\n)\n\n# Define server logic required to draw a histogram\nserver &lt;- function(input, output) {\n  \n  output$distPlot &lt;- renderPlot({\n    input$redo\n    b0 &lt;- input$b0\n    b1 &lt;- input$b1\n    redo &lt;- input$redo\n    T &lt;- input$T\n    s &lt;- input$s\n    m0 &lt;- input$m0\n    s0 = 1\n    \n    range = 0:T\n    alpha = c(0.01, seq(0.1, 0.9, by = 0.1))\n    \n    middle &lt;- sapply(range, ar_1_mean, b0, b1, m0)\n    sds &lt;- sapply(range, ar_1_sd, b1, s, s0)\n    \n    \n    plot(range, middle, type = \"l\",\n         xaxt = \"n\", \n         yaxt = \"n\",\n         xlab = \"t\",\n         ylab = expression(y[t]),\n         ylim = c(-20, 20), bty = \"n\",\n         col = \"white\")\n    \n    for(a in alpha){\n      \n      U = qnorm(1 - a / 2, mean = middle, sd = sds)\n      L = qnorm(a / 2, mean = middle, sd = sds)\n      \n      polygon(\n        c(range, rev(range)),\n        c(U, rev(L)),\n        col = rgb(1, 0, 0, 0.15),\n        border = NA\n      )\n    }\n    \n    inc = 20\n    axis(1, pos = 0, at = seq(0, max(range), by = inc), \n         labels = c(NA, seq(inc, max(range), by = inc)))\n    axis(2, pos = 0)\n    \n    lines(range, simulate_ar_1(max(range) + 1, b0, b1, s, m0, s0), col = \"black\", lwd = 2)\n  })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#assume-beta_1neq-1",
    "href": "slides/01-ar-1-structure.html#assume-beta_1neq-1",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Assume \\(\\beta_1\\neq 1\\)\n",
    "text": "Assume \\(\\beta_1\\neq 1\\)\n\nFinite geometric sum formula gives:\n\\[\n\\begin{aligned}\nE(y_t)\n&=\n\\beta_0\\frac{1-\\beta_1^t}{1-\\beta_1}\n+\n\\beta_1^t\\mu_0\n\\\\\n\\var(y_t)\n&=\n\\sigma^2\n\\frac{1-\\beta_1^{2t}}{1-\\beta_1^2}+\n\\beta_1^{2t}\\initvar.\n\\end{aligned}\n\\]\nWhat happens as \\(t\\to\\infty\\)?"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#stationarity",
    "href": "slides/01-ar-1-structure.html#stationarity",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Stationarity",
    "text": "Stationarity\nA joint distribution is (strictly) stationary if it is “shift invariant”:\n\\[\n\\{y_{t_1}\\com y_{t_2}\\com ...\\com y_{t_n}\\}\\overset{d}{=}\\{y_{t_1+h}\\com y_{t_2+h}\\com ...\\com y_{t_n+h}\\}.\n\\]\nThe Gaussian AR(1) with \\(|\\beta_1|&lt;1\\) has this property."
  },
  {
    "objectID": "slides/01-ar-1-structure.html#stationary-ar1",
    "href": "slides/01-ar-1-structure.html#stationary-ar1",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Stationary AR(1)",
    "text": "Stationary AR(1)\nIf \\(-1&lt;\\beta_1&lt;1\\), \\(\\mu_0=\\beta_0/(1-\\beta_1)\\), and \\(\\initvar=\\sigma^2/(1-\\beta_1^2)\\), then the AR(1) is strictly stationary with the following:\n\\[\n\\begin{aligned}\nE(y_t)\n&=\n\\frac{\\beta_0}{1-\\beta_1}\n\\\\\n\\var(y_t)\n&=\n\\frac{\\sigma^2}{1-\\beta_1^2}\n\\\\\n\\cov(y_t\\com y_s)\n&=\n\\beta_1^{|t-s|}\\var(y_t)\n=\n\\beta_1^{|t-s|}\\frac{\\sigma^2}{1-\\beta_1^2}.\n\\end{aligned}\n\\]\nThe common marginal shared by all \\(y_t\\) is called the stationary distribution:\n\\[\ny_t\\sim\\text{N}\\left(\\frac{\\beta_0}{1-\\beta_1}\\com \\frac{\\sigma^2}{1-\\beta_1^2}\\right).\n\\]\nSo “did: dependent but identically distributed.”"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#chat-about-stationarity",
    "href": "slides/01-ar-1-structure.html#chat-about-stationarity",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Chat about stationarity",
    "text": "Chat about stationarity\n\n\nHopefully your first thought upon encountering this concept is “real data won’t be stationary.” True!\n\nSo why care about this?\n\nCute from a pure math point of view;\nIf you simulate distributions with Markov chain Monte Carlo (MCMC), you bow down at the altar of stationarity;\nStationarity means “dependent but not too dependent.” You can redo classical statistical theory replacing “iid” with “stationary,” and not much necessarily changes (convergence rates get worse);\n(A Bayesian won’t necessarily care about that last point.)"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#autocovariance-of-a-stationary-process",
    "href": "slides/01-ar-1-structure.html#autocovariance-of-a-stationary-process",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Autocovariance of a stationary process",
    "text": "Autocovariance of a stationary process\nFor a stationary process, the covariance kernel satisfies\n\\[\n\\cov(y_t\\com y_{s})=\\cov(y_{t+h}\\com y_{s+h})\\quad \\forall (t\\com s\\com h).\n\\]\nSo you can define something called the autocovariance function:\n\\[\n\\gamma(h)=\\cov(y_{t+h}\\com y_{t}).\n\\]\nFor the AR(1), this is\n\\[\n\\begin{aligned}\n\\gamma(0)&=\\sigma^2/(1-\\beta_1^2)\n\\\\\n\\gamma(h)&=\\beta_1^h\\gamma(0).\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#what-does-the-autocov-function-look-like",
    "href": "slides/01-ar-1-structure.html#what-does-the-autocov-function-look-like",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "What does the autocov function look like?",
    "text": "What does the autocov function look like?"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#what-does-the-autocov-function-look-like-1",
    "href": "slides/01-ar-1-structure.html#what-does-the-autocov-function-look-like-1",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "What does the autocov function look like?",
    "text": "What does the autocov function look like?"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#what-does-the-autocov-function-look-like-2",
    "href": "slides/01-ar-1-structure.html#what-does-the-autocov-function-look-like-2",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "What does the autocov function look like?",
    "text": "What does the autocov function look like?"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#what-does-the-autocov-function-look-like-3",
    "href": "slides/01-ar-1-structure.html#what-does-the-autocov-function-look-like-3",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "What does the autocov function look like?",
    "text": "What does the autocov function look like?"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#what-does-the-autocov-function-look-like-4",
    "href": "slides/01-ar-1-structure.html#what-does-the-autocov-function-look-like-4",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "What does the autocov function look like?",
    "text": "What does the autocov function look like?"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#what-does-the-autocov-function-look-like-5",
    "href": "slides/01-ar-1-structure.html#what-does-the-autocov-function-look-like-5",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "What does the autocov function look like?",
    "text": "What does the autocov function look like?"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#plug-in-estimators-for-the-moments",
    "href": "slides/01-ar-1-structure.html#plug-in-estimators-for-the-moments",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Plug-in estimators for the moments",
    "text": "Plug-in estimators for the moments\nIf your AR(1) is stationary, then given data, expected values like\n\n\\[\n\\begin{aligned}\n\\mu\n&=\nE(y_t)\\\\\n\\gamma(h)\n&=\nE[(y_{t+h}-\\mu)(y_t-\\mu)],\n\\end{aligned}\n\\]\n\n\ncan be estimated with simple sample averages\n\n\n\\[\n\\begin{aligned}\n\\hat{\\mu}_T\n&=\n\\frac{1}{T}\\sum\\limits_{t=1}^Ty_t\n\\\\\n\\hat{\\gamma}_T(h)\n&=\n\\frac{1}{T}\\sum\\limits_{t=1}^{T-h}(y_{t+h}-\\hat{\\mu}_T)(y_t-\\hat{\\mu}_T).\n\\end{aligned}\n\\]\n\n\nThe last is the sample autocovariance function."
  },
  {
    "objectID": "slides/01-ar-1-structure.html#recall-some-facts-about-the-stationary-ar1",
    "href": "slides/01-ar-1-structure.html#recall-some-facts-about-the-stationary-ar1",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Recall some facts about the stationary AR(1)",
    "text": "Recall some facts about the stationary AR(1)\nHere’s what we’ve got:\n\n\\[\n\\begin{aligned}\n\\gamma(1)&=\\beta_1\\gamma(0) &&\\implies\\beta_1=\\frac{\\gamma(1)}{\\gamma(0)}\\\\\n\\mu&=\\frac{\\beta_0}{1-\\beta_1}&&\\implies\\beta_0=(1-\\beta_1)\\mu\\\\\n\\gamma(0)&=\\frac{\\sigma^2}{1-\\beta_1^2}&&\\implies \\sigma^2=(1-\\beta_1^2)\\gamma(0).\n\\end{aligned}\n\\]\n\n\nSo…any ideas how to estimate these?"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#the-baby-yule-walker-equations",
    "href": "slides/01-ar-1-structure.html#the-baby-yule-walker-equations",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "The (baby) Yule-Walker equation(s)",
    "text": "The (baby) Yule-Walker equation(s)\nEverywhere you see a “population” expected value, plug in the sample version:\n\n\\[\n\\begin{aligned}\n\\hat{\\beta}_1&=\\frac{\\hat{\\gamma}_T(1)}{\\hat{\\gamma}_T(0)}\n\\\\\n\\hat{\\beta}_0&=(1-\\hat{\\beta}_1)\\hat{\\mu}_T\\\\\n\\hat{\\sigma^2_T}&=(1-\\hat{\\beta}_1^2)\\hat{\\gamma}_T(0).\n\\end{aligned}\n\\]\nThese are method of moments estimators for the AR(1) parameters."
  },
  {
    "objectID": "slides/01-ar-1-structure.html#asymptotics",
    "href": "slides/01-ar-1-structure.html#asymptotics",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "Asymptotics",
    "text": "Asymptotics\n\n\n\n\n\n\nShumway and Stoffer (2025) Property 3.7\n\n\nFor a stationary AR(1), the Yuke-Walker estimator has:\n\\[\n\\sqrt{T}(\\hat{\\beta}_1-\\beta_1)\\cd\\N\\left(0\\com\\frac{\\sigma^2}{\\gamma(0)}\\right).\n\\] And note in this case that \\(\\sigma^2 / \\gamma(0)=1-\\beta_1^2\\). The result remains true if you plug in the estimator of the asymptotic variance.\n\n\n\n\nAsymptotically valid \\(100\\times(1-\\alpha)\\%\\) confidence interval:\n\\[\n\\hat{\\beta}_1\\pm z_{1-\\alpha/2}\\sqrt{\\frac{1-\\hat{\\beta}_1^2}{T}}.\n\\]"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#what-happens-for-beta_1-close-to-pm1",
    "href": "slides/01-ar-1-structure.html#what-happens-for-beta_1-close-to-pm1",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "What happens for \\(\\beta_1\\) close to \\(\\pm1\\)?",
    "text": "What happens for \\(\\beta_1\\) close to \\(\\pm1\\)?\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n#| viewerHeight: 700\n\n# app.R\nlibrary(shiny)\n\n# ---------- Configuration ----------\nSAMPLE_SIZES &lt;- seq(10, 200, by = 20)   # smaller default for speed\nNSIM &lt;- 500                             # number of simulated AR(1) paths per sample size\nY_LIMITS &lt;- c(-1, 1)\n# ------------------------------------\n\n# Yule-Walker AR(1) estimator\nyule_walker_ar1 &lt;- function(x) {\n  n &lt;- length(x)\n  x &lt;- x - mean(x)  # center\n  gamma0 &lt;- mean(x^2)\n  gamma1 &lt;- mean(x[-1] * x[-n])\n  gamma1 / gamma0\n}\n\n# Simulate one stationary AR(1) with variance 1\nsimulate_ar_1 &lt;- function(T, b0, b1, s, m0, s0){\n  y &lt;- numeric(T)\n  y[1] &lt;- rnorm(1, m0, s0)\n  for(t in 2:T){\n    y[t] &lt;- b0 + b1 * y[t - 1] + rnorm(1, 0, s)\n  }\n  return(y)\n}\n\nui &lt;- fluidPage(\n  titlePanel(\"Sampling distribution of Yule–Walker AR(1) estimator\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"b1\", \"True value of β₁:\",\n                  min = -0.99, max = 0.99, value = 0.5, step = 0.01)\n    ),\n    mainPanel(\n      plotOutput(\"boxPlot\", height = \"600px\")\n    )\n  )\n)\n\nserver &lt;- function(input, output, session) {\n  \n  output$boxPlot &lt;- renderPlot({\n    b0 &lt;- 0\n    b1 &lt;- input$b1\n    s &lt;- 1\n    m0 &lt;- b0 / (1 - b1)\n    s0 &lt;- s / sqrt(1 - b1^2)\n    sizes &lt;- SAMPLE_SIZES\n    nsim &lt;- NSIM\n    \n    # Collect estimates in a list, one element per sample size\n    est_list &lt;- vector(\"list\", length(sizes))\n    \n    for (i in seq_along(sizes)) {\n      n &lt;- sizes[i]\n      phi_hats &lt;- numeric(nsim)\n      for (s in 1:nsim) {\n        x &lt;- simulate_ar_1(n, b0, b1, s, m0, s0)#simulate_ar1(n, phi)\n        phi_hats[s] &lt;- yule_walker_ar1(x)\n      }\n      est_list[[i]] &lt;- phi_hats\n    }\n    \n    # Draw boxplots side by side\n    par()\n    boxplot(est_list,\n            names = sizes,\n            ylim = Y_LIMITS,\n            xlab = \"Sample size T\",\n            ylab = \"Estimate\",\n            main = paste(\"True value: \", b1),\n            col = \"lightgray\", pch = 19)\n    \n    abline(h = b1, col = \"red\", lty = 2, lwd = 2)\n  })\n}\n\nshinyApp(ui, server)"
  },
  {
    "objectID": "slides/01-ar-1-structure.html#a-word-about-assumptions",
    "href": "slides/01-ar-1-structure.html#a-word-about-assumptions",
    "title": "What is the joint distribution of an AR(1)?",
    "section": "A word about assumptions",
    "text": "A word about assumptions\n\n\n\nNormality was not essential, but stationarity absolutely was:\n\nthere is no such thing as \\(\\gamma(h)\\) without it;\nthe estimating equations for method-of-moments make no sense if the process isn’t stationary;\n\n\nBut interesting “real-world” data probably are not stationary, and so it would be nice to have estimation techniques that don’t completely break down without it.\n\n\n\n…what’s that?\n\n\n\n\n\nDo you hear something?\n\n\n\n\n\nPeekaboo"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#the-simplest-non-trivial-time-series-model",
    "href": "slides/02-ar-1-inference.html#the-simplest-non-trivial-time-series-model",
    "title": "What do you do likelihood-based inference in the AR(1)?",
    "section": "The simplest non-trivial time series model",
    "text": "The simplest non-trivial time series model\nThe autoregression of order 1, or AR(1):\n\n\\[\n\\begin{aligned}\ny_t\n&=\n\\beta_0\n+\n\\beta_1\ny_{t-1}\n+\n\\varepsilon_t,\n\\quad\n\\varepsilon_t\\iid\\text{N}(0\\com\\sigma^2)\n\\\\\ny_0\n&\\sim\n\\text{N}(\\mu_0\\com \\initvar),\n\\end{aligned}\n\\]\n\n\nThat’s the recursive form. It implies a joint distribution written marginal-conditional style:\n\n\n\\[\n\\begin{aligned}\np(y_{0:T})\n&=\np(y_0)\n\\prod_{t=1}^T\np(y_t\\given y_{0:t-1})\n\\\\\n&=\np(y_0)\n\\prod_{t=1}^T\np(y_t\\given y_{t-1}).\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#the-ar1-joint-distribution",
    "href": "slides/02-ar-1-inference.html#the-ar1-joint-distribution",
    "title": "What do you do likelihood-based inference in the AR(1)?",
    "section": "The AR(1) joint distribution",
    "text": "The AR(1) joint distribution"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#the-special-case-of-beta_11",
    "href": "slides/02-ar-1-inference.html#the-special-case-of-beta_11",
    "title": "What do you do likelihood-based inference in the AR(1)?",
    "section": "The special case of \\(|\\beta_1|<1\\)",
    "text": "The special case of \\(|\\beta_1|&lt;1\\)"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#thats-an-example-of-stationarity",
    "href": "slides/02-ar-1-inference.html#thats-an-example-of-stationarity",
    "title": "What do you do likelihood-based inference in the AR(1)?",
    "section": "That’s an example of stationarity",
    "text": "That’s an example of stationarity"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#yule-walker",
    "href": "slides/02-ar-1-inference.html#yule-walker",
    "title": "What do you do likelihood-based inference in the AR(1)?",
    "section": "Yule Walker",
    "text": "Yule Walker\nnews flash: it’s a correlation!"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#add-parameter-to-notation",
    "href": "slides/02-ar-1-inference.html#add-parameter-to-notation",
    "title": "What do you do likelihood-based inference in the AR(1)?",
    "section": "Add parameter to notation",
    "text": "Add parameter to notation"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#initial-condition-is-a-pain-in-the-ass",
    "href": "slides/02-ar-1-inference.html#initial-condition-is-a-pain-in-the-ass",
    "title": "What do you do likelihood-based inference in the AR(1)?",
    "section": "Initial condition is a pain in the ass",
    "text": "Initial condition is a pain in the ass"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#in-practice-we-just-condition-on-it.",
    "href": "slides/02-ar-1-inference.html#in-practice-we-just-condition-on-it.",
    "title": "What do you do likelihood-based inference in the AR(1)?",
    "section": "In practice we just condition on it.",
    "text": "In practice we just condition on it."
  },
  {
    "objectID": "slides/02-ar-1-inference.html#do-mle-recover-ols",
    "href": "slides/02-ar-1-inference.html#do-mle-recover-ols",
    "title": "What do you do likelihood-based inference in the AR(1)?",
    "section": "do MLE, recover OLS",
    "text": "do MLE, recover OLS"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#if-the-data-are-streaming-do-pset-0",
    "href": "slides/02-ar-1-inference.html#if-the-data-are-streaming-do-pset-0",
    "title": "What do you do likelihood-based inference in the AR(1)?",
    "section": "if the data are streaming, do PSET 0",
    "text": "if the data are streaming, do PSET 0"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#asymptotics-if-its-stationary-theyre-familiar",
    "href": "slides/02-ar-1-inference.html#asymptotics-if-its-stationary-theyre-familiar",
    "title": "What do you do likelihood-based inference in the AR(1)?",
    "section": "Asymptotics, if it’s stationary, they’re familiar",
    "text": "Asymptotics, if it’s stationary, they’re familiar"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#if-not-stationary-still-works-but-hella-weird-dickey-fuller-and-all-them",
    "href": "slides/02-ar-1-inference.html#if-not-stationary-still-works-but-hella-weird-dickey-fuller-and-all-them",
    "title": "What do you do likelihood-based inference in the AR(1)?",
    "section": "If not stationary, still works, but hella weird (Dickey Fuller and all them)",
    "text": "If not stationary, still works, but hella weird (Dickey Fuller and all them)"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#just-regression-so-conjugate-stuff-applies-the-same-as-iid-y-x",
    "href": "slides/02-ar-1-inference.html#just-regression-so-conjugate-stuff-applies-the-same-as-iid-y-x",
    "title": "What do you do likelihood-based inference in the AR(1)?",
    "section": "Just regression, so conjugate stuff applies the same as iid (y, x)",
    "text": "Just regression, so conjugate stuff applies the same as iid (y, x)"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#non-conjugate-prior",
    "href": "slides/02-ar-1-inference.html#non-conjugate-prior",
    "title": "What do you do likelihood-based inference in the AR(1)?",
    "section": "non-conjugate prior",
    "text": "non-conjugate prior"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#another",
    "href": "slides/02-ar-1-inference.html#another",
    "title": "What do you do likelihood-based inference in the AR(1)?",
    "section": "another",
    "text": "another"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#truncation-prior-to-enforce-stationarity",
    "href": "slides/02-ar-1-inference.html#truncation-prior-to-enforce-stationarity",
    "title": "What do you do likelihood-based inference in the AR(1)?",
    "section": "truncation prior to enforce stationarity",
    "text": "truncation prior to enforce stationarity\nput stationarity in its proper place: as a modeling assumption you can adopt or not by incorporating it into your prior"
  },
  {
    "objectID": "slides/02-ar-1-inference.html#bayes-is-inherently-recursive",
    "href": "slides/02-ar-1-inference.html#bayes-is-inherently-recursive",
    "title": "What do you do likelihood-based inference in the AR(1)?",
    "section": "Bayes is inherently recursive",
    "text": "Bayes is inherently recursive\nnon-conjugate priors and/or MCMC breaks this, but we can reclaim it with sequential Monte Carlo methods.\nOne of the great virtues of Bayes is that it is inherently recursive. One of the great vices of MCMC is that it breaks that.\nWe will reclaim the sequential nature at the end of the semester when we study sequential Monte Carlo (AKA particle filtering)."
  }
]