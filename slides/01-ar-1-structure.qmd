---
title: "What is the joint distribution of an AR(1)?"
subtitle: "Lecture 1"
format: revealjs
auto-stretch: false
---

## Time series {.medium}

- A [**time series**]{style="color:blue;"} is a set of measurements collected over time;

- We model these data as a sequence of [**dependent**]{style="color:blue;"} random variables:

$$
\By_{0:T} = \{\By_0,\,\By_1,\,\By_2,\,...,\,\By_T\}.
$$

- A [**time series model**]{style="color:blue;"} is "just" their joint probability distribution:

$$
p(\By_{1:T}) = p(\By_0)\prod_{t=1}^Tp(\By_t\,|\,\By_{0:t-1}).
$$

## Course themes

We will focus on a small set of themes, but go deep on them: 

::: incremental
1. Inference should be [sequential]{style="color:red;"}. We want recursive estimation techniques to handle data that are *streaming* in real-time;
1. Predictions should be [probabilistic]{style="color:red;"}. We want point, interval, and density forecasts that incorporate many sources of uncertainty;
1. If you can manipulate joint distributions, you can *do* time series analysis. This is obscured in too many TS texts;
:::

. . .

And there is a secret fourth theme:

::: incremental
4. A Bayesian approach is an *excellent* way of achieving the goals of sequential inference and probabilistic prediction.
:::

## The simplest non-trivial time series model {.medium}

The autoregression of order 1, or AR(1):

. . .

$$
\begin{aligned}
y_t
&=
\beta_0
+
\beta_1
y_{t-1}
+
\varepsilon_t,
\quad
\varepsilon_t\iid\text{N}(0\com\sigma^2)
\\
y_0
&\sim
\text{N}(\mu_0\com ?),
\end{aligned}
$$

. . .

That's the recursive form. It implies a joint distribution written marginal-conditional style:

. . .

$$
\begin{aligned}
p(y_{0:T})
&=
p(y_0)
\prod_{t=1}^T
p(y_t\given y_{0:t-1})
\\
&=
p(y_0)
\prod_{t=1}^T
p(y_t\given y_{t-1}).
\end{aligned}
$$

## Putting the "auto" in autoregression

"Just" regression. 

## The game plan for the next few lectures {.medium}

Do "All of Time Series Analysis" for this simple model:

::: incremental
- what is the joint distribution and what is its structure (marginals, conditionals, moments);
- stationarity;
- classical inference;
- Bayesian inference;
- (emphasizing recursive estimation in both cases);
- probabilistic forecasting from both inferential perspectives;
- evaluating probabilistic forecasts.
:::

. . .

The rest of the course is in some sense theme and variations.

## Last time: substitution fest {.scrollable}

. . .

$t=0$:

$$
y_0=y_0
$$

. . .

$t=1$:

$$
y_1=\beta_0+\beta_1y_0+\varepsilon_1
$$

. . .

$t=2$:

$$
\begin{aligned}
y_2
&=
\beta_0
+
\beta_1
y_1
+
\varepsilon_2
\\
&=
\beta_0
+
\beta_1
(\beta_0+\beta_1y_0+\varepsilon_1)
+
\varepsilon_2
\\
&=
\beta_0+\beta_0\beta_1+\beta_1^2y_0+\beta_1\varepsilon_1+\varepsilon_2.
\end{aligned}
$$

. . .

$t=3$:

$$
\begin{aligned}
y_3
&=
\beta_0
+
\beta_1
y_2
+
\varepsilon_3
\\
&=
\beta_0
+
\beta_1
(\beta_0+\beta_0\beta_1+\beta_1^2y_0+\beta_1\varepsilon_1+\varepsilon_2)
+
\varepsilon_3
\\
&=
\beta_0+\beta_0\beta_1+\beta_0\beta_1^2+\beta_1^3y_0+\beta_1^2\varepsilon_1+\beta_1\varepsilon_2+\varepsilon_3.
\end{aligned}
$$

## Get organized and notice the pattern

$$
\begin{aligned}
y_0
&=
&
y_0
\\
y_1
&=
\beta_0
&+\,
\beta_1y_0
&+\,
{\color{white}\beta_{\color{white}1}}\varepsilon_1
\\
y_2
&=
\beta_0+\beta_0\beta_1
&+\,
\beta_1^2y_0
&+\,
\beta_1\varepsilon_1
+
{\color{white}\beta_{\color{white}1}}\varepsilon_2
\\
y_3
&=
\beta_0+\beta_0\beta_1+\beta_0\beta_1^2
&+\,
\beta_1^3y_0
&+\,
\beta_1^2\varepsilon_1
+
\beta_1\varepsilon_2
+
\varepsilon_3
\\
&\vdots
\\
y_t
&=
\beta_0\sum\limits_{i=0}^{t-1}\beta_1^i
&+\,
\beta_1^ty_0
&+\,
\sum\limits_{i=0}^{t-1}\beta_1^i\varepsilon_{t-i}
\\
&\vdots
\end{aligned}
$$

## The matrix equation {.small .scrollable}

Writing the linear system as a matrix equation, you get:

. . .

$$
\begin{aligned}
\underbrace{
\begin{bmatrix}
y_0
\\
y_1
\\
y_2
\\
y_3
\\
\vdots
\\
y_T
\end{bmatrix}
}_{\By}
&=
\underbrace{
\beta_0
\begin{bmatrix}
0
\\
1
\\
1+\beta_1
\\
1+\beta_1+\beta_1^2
\\
\vdots
\\
\sum\limits_{i=0}^{T-1}\beta_1^i
\end{bmatrix}
}_{\Bc}
+
\underbrace{
\begin{bmatrix}
1 & 0 & 0 & 0 & \cdots & 0 \\
\beta_1 & 1 & 0 & 0 & \cdots & 0 \\
\beta_1^2 & \beta_1 & 1 & 0 & \cdots & 0 \\
\beta_1^3 & \beta_1^2 & \beta_1 & 1 & \cdots & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
\beta_1^T & \beta_1^{T-1} & \beta_1^{T-2} & \beta_1^{T-3} & \cdots & 1 \\
\end{bmatrix}
}_{\BA}
\underbrace{
\begin{bmatrix}
y_0
\\
\varepsilon_1
\\
\varepsilon_2
\\
\varepsilon_3
\\
\vdots
\\
\varepsilon_T
\end{bmatrix}
}_{\Be}
.
\end{aligned}
$$

. . .

By assumption, 

$$
\Be
\sim 
\text{N}_{T+1}
\left(
\Bm=
\begin{bmatrix}
\mu_0
\\
\Bzero
\end{bmatrix}
\com
\BS
=
\begin{bmatrix}
?? 
& 
\Bzero^\tr\\
\Bzero & \sigma^2\BI_T
\end{bmatrix}
\right)
.
$$

. . .

So by linearity, 

$$
\By
\sim 
\text{N}_{T+1}
\left(
\Bmu=\Bc+\BA\Bm
\com
\BSigma 
=
\BA\BS\BA^\tr
\right)
.
$$

## What's the mean? {.small}

. . .

Recall that 

$$
E\left(
\sum\limits_{i=1}^na_iX_i
\right)
=
\sum\limits_{i=1}^na_iE(X_i).
$$

. . .

So:

$$
\begin{aligned}
E(y_t)
&=
E
\left(
\beta_0\sum\limits_{i=0}^{t-1}\beta_1^i
+
\beta_1^ty_0
+
\sum\limits_{i=0}^{t-1}\beta_1^i\varepsilon_{t-i}
\right)
\\
&=
\beta_0\sum\limits_{i=0}^{t-1}\beta_1^i
+
\beta_1^t
E
\left(
y_0
\right)
+
\sum\limits_{i=0}^{t-1}\beta_1^i
E
\left(
\varepsilon_{t-i}
\right)
\\
&=
\beta_0\sum\limits_{i=0}^{t-1}\beta_1^i
+
\beta_1^t
\mu_0.
\end{aligned}
$$

. . .

We only used $E(\varepsilon_t)=0$. Didn't need independence or normality.

## What's the variance? {.small}

. . .

Recall that for independent random variables, 

$$
\var\left(
\sum\limits_{i=1}^na_iX_i
\right)
=
\sum\limits_{i=1}^na_i^2\var(X_i).
$$

. . .

So: 

$$
\begin{aligned}
\var(y_t)
&=
\var
\left(
\beta_0\sum\limits_{i=0}^{t-1}\beta_1^i
+
\beta_1^ty_0
+
\sum\limits_{i=0}^{t-1}\beta_1^i\varepsilon_{t-i}
\right)
\\
&=
\beta_1^{2t}
\var
\left(
y_0
\right)
+
\sum\limits_{i=0}^{t-1}\beta_1^{2i}
\var
\left(
\varepsilon_{t-i}
\right)
\\
&=
\beta_1^{2t}??
+
\sigma^2
\sum\limits_{i=0}^{t-1}\beta_1^{2i}.
\end{aligned}
$$

. . .

We used time-invariance and independence of $\varepsilon_t$ but not normality.

## What's the covariance?

## Summary {.small}

. . .

Recursive form: 

$$
\begin{aligned}
y_t
&=
\beta_0
+
\beta_1
y_{t-1}
+
\varepsilon_t,
\quad
\varepsilon_t\iid\text{N}(0\com\sigma^2)
\\
y_0
&\sim
\text{N}(\mu_0\com ?).
\end{aligned}
$$

. . .

Joint distribution:

$$
\begin{bmatrix}
y_0 & y_1 & \cdots & y_T
\end{bmatrix}^\tr
\sim\text{N}_{T+1}\left(\Bmu\com \BSigma\right).
$$

. . .

Moments:

$$
\begin{aligned}
E(y_t)
&=
\beta_0\sum\limits_{i=0}^{t-1}\beta_1^i
+
\beta_1^t\mu_0
\\
\var(y_t)
&=
\sigma^2
\sum\limits_{i=0}^{t-1}\beta_1^{2i}+
\beta_1^{2t}??
\\
\cov(y_t\com y_s)
&=
.
\end{aligned}
$$

# How does this behave? 

## Some special cases {.medium}

- [**iid**]{style="color:blue;"}: set $\beta_1=0$ (and $\mu_0=\beta_0$; $??=\sigma^2$), and

. . .

$$
y_t\iid\text{N}(\beta_0\com\sigma^2).
$$

. . .

- [**random walk with drift**]{style="color:blue;"}: set $\beta_1=1$, and

. . .

$$
\begin{aligned}
E(y_t)
&=
\beta_0t+\mu_0
\\
\var(y_t)
&=
\sigma^2t+??.
\end{aligned}
$$

. . .

- [**funky**]{style="color:blue;"}: set $\beta_1=-1$, and 

. . .

$$
fefrf
$$

# Graphical interlude

## Assume $\beta_1\neq 1$ 

$|\beta_1|\neq 1$ 

Then finite geometric sum tells us that 

## Definition of stationarity

consequence: the marginals are all the same (so, time-invariant)

## Chat about stationarity

Hopefully your first thought is ""

Interesting in its own right as pure math 

MCMC

Stationary is like "dependent but not too dependent" you can redo classical iid statistical theory without too much pain

(For a Bayesian, you don't necessarily care about this)


## Stationary AR(1) {.small}

If $-1<\beta_1<1$, $\mu_0=\beta_0/(1-\beta_1)$, and $???=\sigma^2/(1-\beta_1^2)$, then the AR(1) is strictly stationary with the following:

$$
\begin{aligned}
E(y_t)
&=
\frac{\beta_0}{1-\beta_1}
\\
\var(y_t)
&=
\frac{\sigma^2}{1-\beta_1^2}
\\
\cov(y_t\com y_s)
&=
\beta_1^{|t-s|}\var(y_t)
=
\beta_1^{|t-s|}\frac{\sigma^2}{1-\beta_1^2}.
\end{aligned}
$$

The common marginal shared by all $y_t$ is called the stationary distribution:

$$
y_t\sim\text{N}\left(\frac{\beta_0}{1-\beta_1}\com \frac{\sigma^2}{1-\beta_1^2}\right).
$$

So "did: dependent but identically distributed."

## Autocovariance of a stationary process

$$
\gamma(h)=\cov(y_t\com y_{t+h})
$$

For the AR(1)

## Plot pictures of examples

# Let's start thinking about inference

## Plug-in estimators for the moments {.medium}

If your AR(1) is stationary, then given data, expected values like

. . .

$$
\begin{aligned}
\mu
&=
E(y_t)\\
\gamma(h)
&=
E[(y_{t+h}-\mu)(y_t-\mu)],
\end{aligned}
$$

. . .

can be estimated with simple sample averages

. . .

$$
\begin{aligned}
\hat{\mu}_T
&=
\frac{1}{T}\sum\limits_{t=1}^Ty_t
\\
\hat{\gamma}_T(h)
&=
\frac{1}{T}\sum\limits_{t=1}^{T-h}(y_{t+h}-\hat{\mu}_T)(y_t-\hat{\mu}_T).
\end{aligned}
$$

. . .

The last is the **sample autocovariance function**.

## Recall some facts about the stationary AR(1)

Here's what we've got:

. . .

$$
\begin{aligned}
\gamma(1)&=\beta_1\gamma(0) &&\implies\beta_1=\frac{\gamma(1)}{\gamma(0)}\\
\mu&=\frac{\beta_0}{1-\beta_1}&&\implies\beta_0=(1-\beta_1)\mu\\
\gamma(0)&=\frac{\sigma^2}{1-\beta_1^2}&&\implies \sigma^2=(1-\beta_1^2)\gamma(0).
\end{aligned}
$$

. . .

So...any ideas how to estimate these?

## The (baby) Yule-Walker equation(s)

Method of moments estimators for the AR(1) parameters:

$$
\begin{aligned}
\hat{\beta}_1&=\frac{\hat{\gamma}_T(1)}{\hat{\gamma}_T(0)}
\\
\hat{\beta}_0&=(1-\hat{\beta}_1)\hat{\mu}_T\\
\hat{\sigma^2}_T&=(1-\hat{\beta}_1^2)\hat{\gamma}_T(0).
\end{aligned}
$$

## Asymptotics

Don't need normality of the errors for this. You can even relax independence.

## Simulation

Things get weird near the boundary of the stationary region


## A word about assumptions {.scrollable}

::: incremental
- Normality was not essential, but stationarity absolutely was:

    - there is no such thing as $\gamma(h)$ without it;
    - the estimating equations for method-of-moments make no sense if the process isn't stationary;
    
- But interesting "real-world" data probably are not stationary, and so it would be nice to have estimation techniques that don't completely break down without it. 
:::

<div class="vspace"></div>

![](images/bayes.gif){fig-align="center"}