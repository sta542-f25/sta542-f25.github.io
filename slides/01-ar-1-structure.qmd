---
title: "What is the joint distribution of an AR(1)?"
subtitle: "Lecture 1"
format: revealjs
auto-stretch: false
filters:
  - shinylive
---

## Time series {.medium}

- A [**time series**]{style="color:blue;"} is a set of measurements collected over time;

- We model these data as a sequence of [**dependent**]{style="color:blue;"} random variables:

$$
\By_{0:T} = \{\By_0,\,\By_1,\,\By_2,\,...,\,\By_T\}.
$$

- A [**time series model**]{style="color:blue;"} is "just" their joint probability distribution:

$$
p(\By_{0:T}) = p(\By_0)\prod_{t=1}^Tp(\By_t\,|\,\By_{0:t-1}).
$$

## Course themes

We will focus on a small set of themes, but go deep on them: 

::: incremental
1. Inference should be [sequential]{style="color:red;"}. We want recursive estimation techniques to handle data that are *streaming* in real-time;
1. Predictions should be [probabilistic]{style="color:red;"}. We want point, interval, and density forecasts that incorporate many sources of uncertainty;
1. If you can manipulate joint distributions, you can *do* time series analysis. This is obscured in too many TS texts;
:::

. . .

And there is a secret fourth theme:

::: incremental
4. A Bayesian approach is an *excellent* way of achieving the goals of sequential inference and probabilistic prediction.
:::

## The simplest non-trivial time series model {.medium}

The autoregression of order 1, or AR(1):

. . .

$$
\begin{aligned}
y_t
&=
\beta_0
+
\beta_1
y_{t-1}
+
\varepsilon_t,
\quad
\varepsilon_t\iid\text{N}(0\com\sigma^2)
\\
y_0
&\sim
\text{N}(\mu_0\com \initvar),
\end{aligned}
$$

. . .

That's the recursive form. It implies a joint distribution written marginal-conditional style:

. . .

$$
\begin{aligned}
p(y_{0:T})
&=
p(y_0)
\prod_{t=1}^T
p(y_t\given y_{0:t-1})
\\
&=
p(y_0)
\prod_{t=1}^T
p(y_t\given y_{t-1}).
\end{aligned}
$$

## Putting the "regression" in autoregression

In some sense the AR(1) is "just" a simple linear regression

$$
y_t
=
\beta_0
+
\beta_1
x_t
+
\varepsilon_t,
\quad
\varepsilon_t\iid\text{N}(0\com\sigma^2),
$$

where we took the predictor to be $x_t=y_{t-1}$.

This perspective obscures the dependence structure, but will be useful for likelihood-based inference next week.

## The game plan for the next few lectures {.medium}

Do "All of Time Series Analysis" for this simple model:

::: incremental
- what is the joint distribution and what is its structure (marginals, conditionals, moments);
- stationarity;
- classical inference;
- Bayesian inference;
- (emphasizing recursive estimation in both cases);
- probabilistic forecasting from both inferential perspectives;
- evaluating probabilistic forecasts.
:::

. . .

The rest of the course is in some sense theme and variations.

## Last time: substitution fest {.scrollable}

. . .

$t=0$:

$$
y_0=y_0
$$

. . .

$t=1$:

$$
y_1=\beta_0+\beta_1y_0+\varepsilon_1
$$

. . .

$t=2$:

$$
\begin{aligned}
y_2
&=
\beta_0
+
\beta_1
y_1
+
\varepsilon_2
\\
&=
\beta_0
+
\beta_1
(\beta_0+\beta_1y_0+\varepsilon_1)
+
\varepsilon_2
\\
&=
\beta_0+\beta_0\beta_1+\beta_1^2y_0+\beta_1\varepsilon_1+\varepsilon_2.
\end{aligned}
$$

. . .

$t=3$:

$$
\begin{aligned}
y_3
&=
\beta_0
+
\beta_1
y_2
+
\varepsilon_3
\\
&=
\beta_0
+
\beta_1
(\beta_0+\beta_0\beta_1+\beta_1^2y_0+\beta_1\varepsilon_1+\varepsilon_2)
+
\varepsilon_3
\\
&=
\beta_0+\beta_0\beta_1+\beta_0\beta_1^2+\beta_1^3y_0+\beta_1^2\varepsilon_1+\beta_1\varepsilon_2+\varepsilon_3.
\end{aligned}
$$

## Get organized and notice the pattern

$$
\begin{aligned}
y_0
&=
&
y_0
\\
y_1
&=
\beta_0
&+\,
\beta_1y_0
&+\,
{\color{white}\beta_{\color{white}1}}\varepsilon_1
\\
y_2
&=
\beta_0+\beta_0\beta_1
&+\,
\beta_1^2y_0
&+\,
\beta_1\varepsilon_1
+
{\color{white}\beta_{\color{white}1}}\varepsilon_2
\\
y_3
&=
\beta_0+\beta_0\beta_1+\beta_0\beta_1^2
&+\,
\beta_1^3y_0
&+\,
\beta_1^2\varepsilon_1
+
\beta_1\varepsilon_2
+
\varepsilon_3
\\
&\vdots
\\
y_t
&=
\beta_0\sum\limits_{i=0}^{t-1}\beta_1^i
&+\,
\beta_1^ty_0
&+\,
\sum\limits_{i=0}^{t-1}\beta_1^i\varepsilon_{t-i}
\\
&\vdots
\end{aligned}
$$

## The matrix equation {.small .scrollable}

Writing the linear system as a matrix equation, you get:

. . .

$$
\begin{aligned}
\underbrace{
\begin{bmatrix}
y_0
\\
y_1
\\
y_2
\\
y_3
\\
\vdots
\\
y_T
\end{bmatrix}
}_{\By}
&=
\underbrace{
\beta_0
\begin{bmatrix}
0
\\
1
\\
1+\beta_1
\\
1+\beta_1+\beta_1^2
\\
\vdots
\\
\sum\limits_{i=0}^{T-1}\beta_1^i
\end{bmatrix}
}_{\Bc}
+
\underbrace{
\begin{bmatrix}
1 & 0 & 0 & 0 & \cdots & 0 \\
\beta_1 & 1 & 0 & 0 & \cdots & 0 \\
\beta_1^2 & \beta_1 & 1 & 0 & \cdots & 0 \\
\beta_1^3 & \beta_1^2 & \beta_1 & 1 & \cdots & 0 \\
\vdots & \vdots & \vdots & \vdots & \ddots & \vdots \\
\beta_1^T & \beta_1^{T-1} & \beta_1^{T-2} & \beta_1^{T-3} & \cdots & 1 \\
\end{bmatrix}
}_{\BA}
\underbrace{
\begin{bmatrix}
y_0
\\
\varepsilon_1
\\
\varepsilon_2
\\
\varepsilon_3
\\
\vdots
\\
\varepsilon_T
\end{bmatrix}
}_{\Be}
.
\end{aligned}
$$

. . .

By assumption, 

$$
\Be
\sim 
\text{N}_{T+1}
\left(
\Bm=
\begin{bmatrix}
\mu_0
\\
\Bzero
\end{bmatrix}
\com
\BS
=
\begin{bmatrix}
\initvar 
& 
\Bzero^\tr\\
\Bzero & \sigma^2\BI_T
\end{bmatrix}
\right)
.
$$

. . .

So by linearity, 

$$
\By
\sim 
\text{N}_{T+1}
\left(
\Bmu=\Bc+\BA\Bm
\com
\BSigma 
=
\BA\BS\BA^\tr
\right)
.
$$

## What's the mean? {.small}

. . .

Recall that 

$$
E\left(
\sum\limits_{i=1}^na_iX_i
\right)
=
\sum\limits_{i=1}^na_iE(X_i).
$$

. . .

So:

$$
\begin{aligned}
E(y_t)
&=
E
\left(
\beta_0\sum\limits_{i=0}^{t-1}\beta_1^i
+
\beta_1^ty_0
+
\sum\limits_{i=0}^{t-1}\beta_1^i\varepsilon_{t-i}
\right)
\\
&=
\beta_0\sum\limits_{i=0}^{t-1}\beta_1^i
+
\beta_1^t
E
\left(
y_0
\right)
+
\sum\limits_{i=0}^{t-1}\beta_1^i
E
\left(
\varepsilon_{t-i}
\right)
\\
&=
\beta_0\sum\limits_{i=0}^{t-1}\beta_1^i
+
\beta_1^t
\mu_0.
\end{aligned}
$$

. . .

We only used $E(\varepsilon_t)=0$. Didn't need independence or normality.

## What's the variance? {.small}

. . .

Recall that for independent random variables, 

$$
\var\left(
\sum\limits_{i=1}^na_iX_i
\right)
=
\sum\limits_{i=1}^na_i^2\var(X_i).
$$

. . .

So: 

$$
\begin{aligned}
\var(y_t)
&=
\var
\left(
\beta_0\sum\limits_{i=0}^{t-1}\beta_1^i
+
\beta_1^ty_0
+
\sum\limits_{i=0}^{t-1}\beta_1^i\varepsilon_{t-i}
\right)
\\
&=
\beta_1^{2t}
\var
\left(
y_0
\right)
+
\sum\limits_{i=0}^{t-1}\beta_1^{2i}
\var
\left(
\varepsilon_{t-i}
\right)
\\
&=
\beta_1^{2t}\initvar
+
\sigma^2
\sum\limits_{i=0}^{t-1}\beta_1^{2i}.
\end{aligned}
$$

. . .

We used time-invariance and independence of $\varepsilon_t$ but not normality.

## What's the covariance?

Here you go:

$$
\cov(y_t\com y_s)
=
\begin{cases}
\beta_1^{s-t}\var(y_t) & t\leq s\\
\beta_1^{t-s}\var(y_s) & s < t.
\end{cases}
$$

Derivation deferred to Problem Set 1.

## Summary {.small .scrollable}

. . .

Recursive form: 

$$
\begin{aligned}
y_t
&=
\beta_0
+
\beta_1
y_{t-1}
+
\varepsilon_t,
\quad
\varepsilon_t\iid\text{N}(0\com\sigma^2)
\\
y_0
&\sim
\text{N}(\mu_0\com \initvar).
\end{aligned}
$$

. . .

Joint distribution:

$$
\begin{bmatrix}
y_0 & y_1 & \cdots & y_T
\end{bmatrix}^\tr
\sim\text{N}_{T+1}\left(\Bmu\com \BSigma\right).
$$

. . .

Moments:

$$
\begin{aligned}
E(y_t)
&=
\beta_0\sum\limits_{i=0}^{t-1}\beta_1^i
+
\beta_1^t\mu_0
\\
\var(y_t)
&=
\sigma^2
\sum\limits_{i=0}^{t-1}\beta_1^{2i}+
\beta_1^{2t}\initvar
\\
\cov(y_t\com y_s)
&=
\begin{cases}
\beta_1^{s-t}\var(y_t) & t\leq s\\
\beta_1^{t-s}\var(y_s) & s < t.
\end{cases}
.
\end{aligned}
$$

# How does this behave? 

## Some special cases {.small}

- [**iid**]{style="color:blue;"}: set $\beta_1=0$ (and $\mu_0=\beta_0$; $\initvar=\sigma^2$), and

. . .

$$
y_t\iid\text{N}(\beta_0\com\sigma^2).
$$

. . .

- [**random walk with drift**]{style="color:blue;"}: set $\beta_1=1$, and

. . .

$$
\begin{aligned}
E(y_t)
&=
\beta_0t+\mu_0
\\
\var(y_t)
&=
\sigma^2t+\initvar.
\end{aligned}
$$

. . .

- [**funky**]{style="color:blue;"}: set $\beta_1=-1$, and 

. . .

$$
\begin{aligned}
E(y_t)
&=
\beta_0\frac{1 - (-1)^t}{2}+(-1)^t\mu_0
\\
\var(y_t)
&=
\sigma^2t+\initvar.
\end{aligned}
$$

## What does this look like? {.scrollable}



```{shinylive-r}
#| standalone: true
#| viewerHeight: 700

library(shiny)

simulate_ar_1 <- function(T, b0, b1, s, m0, s0){
  y <- numeric(T)
  y[1] <- rnorm(1, m0, s0)
  for(t in 2:T){
    y[t] <- b0 + b1 * y[t - 1] + rnorm(1, 0, s)
  }
  return(y)
}

ar_1_mean <- function(t, b0, b1, m0){
  if(t == 0){
    return(m0)
  }else{
    return(b0 * sum(b1 ^ (0:(t-1))) + m0 * (b1^t)) 
  }
}

ar_1_var <- function(t, b1, s, s0){
  if(t == 0){
    return(s0^2)
  }else{
    return((s0^2) * (b1^(2*t)) + (s^2) * sum(b1 ^ (2*(0:(t-1)))))
  }
}

ar_1_sd <- function(t, b1, s, s0){
  sqrt(ar_1_var(t, b1, s, s0))
}

# Define UI for application that draws a histogram
ui <- fluidPage(
  
  # Application title
  titlePanel("Marginal distributions and sample paths of a Gaussian AR(1)"),
  
  # Sidebar with a slider input for number of bins 
  sidebarLayout(
    sidebarPanel(
      sliderInput("b0",
                  "β₀",
                  min = -5,
                  max = 5,
                  value = 0,
                  step = 0.1),
      sliderInput("b1",
                  "β₁",
                  min = -2,
                  max = 2,
                  value = 0,
                  step = 0.1),
      sliderInput("s",
                  "σ",
                  min = 0,
                  max = 2,
                  value = 1, 
                  step = 0.1),
      sliderInput("m0",
                  "μ₀",
                  min = -5,
                  max = 5,
                  value = 0,
                  step = 0.1),
      sliderInput("T",
                  "T",
                  min = 20,
                  max = 200,
                  step = 20,
                  value = 100),
      actionButton("redo", "New sample path"),
    ),
    
    # Show a plot of the generated distribution
    mainPanel(
      plotOutput("distPlot", height = "600px")
    )
  )
)

# Define server logic required to draw a histogram
server <- function(input, output) {
  
  output$distPlot <- renderPlot({
    input$redo
    b0 <- input$b0
    b1 <- input$b1
    redo <- input$redo
    T <- input$T
    s <- input$s
    m0 <- input$m0
    s0 = 1
    
    range = 0:T
    alpha = c(0.01, seq(0.1, 0.9, by = 0.1))
    
    middle <- sapply(range, ar_1_mean, b0, b1, m0)
    sds <- sapply(range, ar_1_sd, b1, s, s0)
    
    
    plot(range, middle, type = "l",
         xaxt = "n", 
         yaxt = "n",
         xlab = "t",
         ylab = expression(y[t]),
         ylim = c(-20, 20), bty = "n",
         col = "white")
    
    for(a in alpha){
      
      U = qnorm(1 - a / 2, mean = middle, sd = sds)
      L = qnorm(a / 2, mean = middle, sd = sds)
      
      polygon(
        c(range, rev(range)),
        c(U, rev(L)),
        col = rgb(1, 0, 0, 0.15),
        border = NA
      )
    }
    
    inc = 20
    axis(1, pos = 0, at = seq(0, max(range), by = inc), 
         labels = c(NA, seq(inc, max(range), by = inc)))
    axis(2, pos = 0)
    
    lines(range, simulate_ar_1(max(range) + 1, b0, b1, s, m0, s0), col = "black", lwd = 2)
  })
}

# Run the application 
shinyApp(ui = ui, server = server)
```

## Assume $\beta_1\neq 1$ 

Finite geometric sum formula gives:

$$
\begin{aligned}
E(y_t)
&=
\beta_0\frac{1-\beta_1^t}{1-\beta_1}
+
\beta_1^t\mu_0
\\
\var(y_t)
&=
\sigma^2
\frac{1-\beta_1^{2t}}{1-\beta_1^2}+
\beta_1^{2t}\initvar.
\end{aligned}
$$

What happens as $t\to\infty$?


## Stationarity

A joint distribution is **(strictly) stationary** if it is "shift invariant":

$$
\{y_{t_1}\com y_{t_2}\com ...\com y_{t_n}\}\overset{d}{=}\{y_{t_1+h}\com y_{t_2+h}\com ...\com y_{t_n+h}\}.
$$

The Gaussian AR(1) with $|\beta_1|<1$ has this property.

## Stationary AR(1) {.small}

If $-1<\beta_1<1$, $\mu_0=\beta_0/(1-\beta_1)$, and $\initvar=\sigma^2/(1-\beta_1^2)$, then the AR(1) is strictly stationary with the following:

$$
\begin{aligned}
E(y_t)
&=
\frac{\beta_0}{1-\beta_1}
\\
\var(y_t)
&=
\frac{\sigma^2}{1-\beta_1^2}
\\
\cov(y_t\com y_s)
&=
\beta_1^{|t-s|}\var(y_t)
=
\beta_1^{|t-s|}\frac{\sigma^2}{1-\beta_1^2}.
\end{aligned}
$$

The common marginal shared by all $y_t$ is called the stationary distribution:

$$
y_t\sim\text{N}\left(\frac{\beta_0}{1-\beta_1}\com \frac{\sigma^2}{1-\beta_1^2}\right).
$$

So "did: dependent but identically distributed."

## Chat about stationarity

::: incremental
- Hopefully your first thought upon encountering this concept is "real data won't be stationary." True!
- So why care about this?

    - Cute from a pure math point of view;
    - If you simulate distributions with Markov chain Monte Carlo (MCMC), you bow down at the altar of stationarity;
    - Stationarity means "dependent but not too dependent." You can redo classical statistical theory replacing "iid" with "stationary," and not much necessarily changes (convergence rates get worse);
    - (A Bayesian won't necessarily care about that last point.)
:::

## Autocovariance of a stationary process

For a stationary process, the covariance kernel satisfies

$$
\cov(y_t\com y_{s})=\cov(y_{t+h}\com y_{s+h})\quad \forall (t\com s\com h).
$$

So you can define something called the **autocovariance function**:

$$
\gamma(h)=\cov(y_{t+h}\com y_{t}).
$$

For the AR(1), this is 

$$
\begin{aligned}
\gamma(0)&=\sigma^2/(1-\beta_1^2)
\\
\gamma(h)&=\beta_1^h\gamma(0).
\end{aligned}
$$

## What does the autocov function look like?

```{r}
#| echo: false
# Parameters
beta_1 <- 0      # AR(1) coefficient
sigma2 <- 1        # Noise variance
max_lag <- 20      # Number of lags to plot

# Compute theoretical autocovariances
lags <- 0:max_lag
gamma_h <- sigma2 / (1 - beta_1^2) * beta_1^lags

# Plot
plot(lags, gamma_h, type="h", lwd=2,
     xlab="Lag h", ylab="γ(h)",
     main="Theoretical autocovariance of stationary AR(1)")
points(lags, gamma_h, pch=19, col="blue")
abline(h = 0, lty = 2, col = "darkgrey")

# Add legend using Unicode
legend_text <- paste0("β1 = ", beta_1, ", σ² = ", sigma2)
legend("topright", legend=legend_text, bty="n")


```

## What does the autocov function look like?

```{r}
#| echo: false
# Parameters
beta_1 <- 0.5      # AR(1) coefficient
sigma2 <- 1        # Noise variance
max_lag <- 20      # Number of lags to plot

# Compute theoretical autocovariances
lags <- 0:max_lag
gamma_h <- sigma2 / (1 - beta_1^2) * beta_1^lags

# Plot
plot(lags, gamma_h, type="h", lwd=2,
     xlab="Lag h", ylab="γ(h)",
     main="Theoretical autocovariance of stationary AR(1)")
points(lags, gamma_h, pch=19, col="blue")

# Add legend using Unicode
legend_text <- paste0("β1 = ", beta_1, ", σ² = ", sigma2)
legend("topright", legend=legend_text, bty="n")


```

## What does the autocov function look like?

```{r}
#| echo: false
# Parameters
beta_1 <- -0.8      # AR(1) coefficient
sigma2 <- 1        # Noise variance
max_lag <- 20      # Number of lags to plot

# Compute theoretical autocovariances
lags <- 0:max_lag
gamma_h <- sigma2 / (1 - beta_1^2) * beta_1^lags

# Plot
plot(lags, gamma_h, type="h", lwd=2,
     xlab="Lag h", ylab="γ(h)",
     main="Theoretical autocovariance of stationary AR(1)")
points(lags, gamma_h, pch=19, col="blue")
abline(h = 0, lty = 2, col = "darkgrey")

# Add legend using Unicode
legend_text <- paste0("β1 = ", beta_1, ", σ² = ", sigma2)
legend("topright", legend=legend_text, bty="n")


```

## What does the autocov function look like?

```{r}
#| echo: false
# Parameters
beta_1 <- 0.9      # AR(1) coefficient
sigma2 <- 1        # Noise variance
max_lag <- 20      # Number of lags to plot

# Compute theoretical autocovariances
lags <- 0:max_lag
gamma_h <- sigma2 / (1 - beta_1^2) * beta_1^lags

# Plot
plot(lags, gamma_h, type="h", lwd=2,
     xlab="Lag h", ylab="γ(h)",
     main="Theoretical autocovariance of stationary AR(1)")
points(lags, gamma_h, pch=19, col="blue")
abline(h = 0, lty = 2, col = "darkgrey")

# Add legend using Unicode
legend_text <- paste0("β1 = ", beta_1, ", σ² = ", sigma2)
legend("topright", legend=legend_text, bty="n")


```

## What does the autocov function look like?

```{r}
#| echo: false
# Parameters
beta_1 <- 0.99      # AR(1) coefficient
sigma2 <- 1        # Noise variance
max_lag <- 20      # Number of lags to plot

# Compute theoretical autocovariances
lags <- 0:max_lag
gamma_h <- sigma2 / (1 - beta_1^2) * beta_1^lags

# Plot
plot(lags, gamma_h, type="h", lwd=2,
     xlab="Lag h", ylab="γ(h)",
     main="Theoretical autocovariance of stationary AR(1)")
points(lags, gamma_h, pch=19, col="blue")
abline(h = 0, lty = 2, col = "darkgrey")

# Add legend using Unicode
legend_text <- paste0("β1 = ", beta_1, ", σ² = ", sigma2)
legend("topright", legend=legend_text, bty="n")

```

## What does the autocov function look like?

```{r}
#| echo: false
# Parameters
beta_1 <- 0.999      # AR(1) coefficient
sigma2 <- 1        # Noise variance
max_lag <- 20      # Number of lags to plot

# Compute theoretical autocovariances
lags <- 0:max_lag
gamma_h <- sigma2 / (1 - beta_1^2) * beta_1^lags

# Plot
plot(lags, gamma_h, type="h", lwd=2,
     xlab="Lag h", ylab="γ(h)",
     main="Theoretical autocovariance of stationary AR(1)")
points(lags, gamma_h, pch=19, col="blue")
abline(h = 0, lty = 2, col = "darkgrey")

# Add legend using Unicode
legend_text <- paste0("β1 = ", beta_1, ", σ² = ", sigma2)
legend("topright", legend=legend_text, bty="n")


```

# Let's start thinking about inference

## Plug-in estimators for the moments {.medium}

If your AR(1) is stationary, then given data, expected values like

. . .

$$
\begin{aligned}
\mu
&=
E(y_t)\\
\gamma(h)
&=
E[(y_{t+h}-\mu)(y_t-\mu)],
\end{aligned}
$$

. . .

can be estimated with simple sample averages

. . .

$$
\begin{aligned}
\hat{\mu}_T
&=
\frac{1}{T}\sum\limits_{t=1}^Ty_t
\\
\hat{\gamma}_T(h)
&=
\frac{1}{T}\sum\limits_{t=1}^{T-h}(y_{t+h}-\hat{\mu}_T)(y_t-\hat{\mu}_T).
\end{aligned}
$$

. . .

The last is the **sample autocovariance function**.

## Recall some facts about the stationary AR(1)

Here's what we've got:

. . .

$$
\begin{aligned}
\gamma(1)&=\beta_1\gamma(0) &&\implies\beta_1=\frac{\gamma(1)}{\gamma(0)}\\
\mu&=\frac{\beta_0}{1-\beta_1}&&\implies\beta_0=(1-\beta_1)\mu\\
\gamma(0)&=\frac{\sigma^2}{1-\beta_1^2}&&\implies \sigma^2=(1-\beta_1^2)\gamma(0).
\end{aligned}
$$

. . .

So...any ideas how to estimate these?

## The (baby) Yule-Walker equation(s)

Everywhere you see a "population" expected value, plug in the sample version:

. . .

$$
\begin{aligned}
\hat{\beta}_1&=\frac{\hat{\gamma}_T(1)}{\hat{\gamma}_T(0)}
\\
\hat{\beta}_0&=(1-\hat{\beta}_1)\hat{\mu}_T\\
\hat{\sigma^2_T}&=(1-\hat{\beta}_1^2)\hat{\gamma}_T(0).
\end{aligned}
$$

These are *method of moments* estimators for the AR(1) parameters.

## Asymptotics

::: callout-tip
## Shumway and Stoffer (2025) Property 3.7
For a stationary AR(1), the Yuke-Walker estimator has:

$$
\sqrt{T}(\hat{\beta}_1-\beta_1)\cd\N\left(0\com\frac{\sigma^2}{\gamma(0)}\right).
$$
And note in this case that $\sigma^2 / \gamma(0)=1-\beta_1^2$. The result remains true if you plug in the estimator of the asymptotic variance.
:::

. . .

Asymptotically valid $100\times(1-\alpha)\%$ confidence interval:

$$
\hat{\beta}_1\pm z_{1-\alpha/2}\sqrt{\frac{1-\hat{\beta}_1^2}{T}}.
$$



## What happens for $\beta_1$ close to $\pm1$? {.scrollable}

```{shinylive-r}
#| standalone: true
#| viewerHeight: 700

# app.R
library(shiny)

# ---------- Configuration ----------
SAMPLE_SIZES <- seq(10, 200, by = 20)   # smaller default for speed
NSIM <- 500                             # number of simulated AR(1) paths per sample size
Y_LIMITS <- c(-1, 1)
# ------------------------------------

# Yule-Walker AR(1) estimator
yule_walker_ar1 <- function(x) {
  n <- length(x)
  x <- x - mean(x)  # center
  gamma0 <- mean(x^2)
  gamma1 <- mean(x[-1] * x[-n])
  gamma1 / gamma0
}

# Simulate one stationary AR(1) with variance 1
simulate_ar_1 <- function(T, b0, b1, s, m0, s0){
  y <- numeric(T)
  y[1] <- rnorm(1, m0, s0)
  for(t in 2:T){
    y[t] <- b0 + b1 * y[t - 1] + rnorm(1, 0, s)
  }
  return(y)
}

ui <- fluidPage(
  titlePanel("Sampling distribution of Yule–Walker AR(1) estimator"),
  sidebarLayout(
    sidebarPanel(
      sliderInput("b1", "True value of β₁:",
                  min = -0.99, max = 0.99, value = 0.5, step = 0.01)
    ),
    mainPanel(
      plotOutput("boxPlot", height = "600px")
    )
  )
)

server <- function(input, output, session) {
  
  output$boxPlot <- renderPlot({
    b0 <- 0
    b1 <- input$b1
    s <- 1
    m0 <- b0 / (1 - b1)
    s0 <- s / sqrt(1 - b1^2)
    sizes <- SAMPLE_SIZES
    nsim <- NSIM
    
    # Collect estimates in a list, one element per sample size
    est_list <- vector("list", length(sizes))
    
    for (i in seq_along(sizes)) {
      n <- sizes[i]
      phi_hats <- numeric(nsim)
      for (s in 1:nsim) {
        x <- simulate_ar_1(n, b0, b1, s, m0, s0)#simulate_ar1(n, phi)
        phi_hats[s] <- yule_walker_ar1(x)
      }
      est_list[[i]] <- phi_hats
    }
    
    # Draw boxplots side by side
    par()
    boxplot(est_list,
            names = sizes,
            ylim = Y_LIMITS,
            xlab = "Sample size T",
            ylab = "Estimate",
            main = paste("True value: ", b1),
            col = "lightgray", pch = 19)
    
    abline(h = b1, col = "red", lty = 2, lwd = 2)
  })
}

shinyApp(ui, server)

```

## A word about assumptions {.scrollable}

::: incremental
- Normality was not essential, but stationarity absolutely was:

    - there is no such thing as $\gamma(h)$ without it;
    - the estimating equations for method-of-moments make no sense if the process isn't stationary;
    
- But interesting "real-world" data probably are not stationary, and so it would be nice to have estimation techniques that don't completely break down without it. 
:::

<div class="vspace"></div>

![Peekaboo](images/bayes.gif){fig-align="center"}